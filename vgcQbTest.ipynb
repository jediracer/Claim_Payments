{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import win32com.client as wc\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import mysql.connector as mc\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import pyodbc\n",
    "from pdfrw import PdfReader, PdfWriter\n",
    "import pdfrw \n",
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "import img2pdf\n",
    "from PIL import Image\n",
    "\n",
    "# Get sql user and password\n",
    "from configTest import mysql_host, mysql_u, mysql_pw\n",
    "from configTest import vgc_host, vgc_u, vgc_pw\n",
    "from configTest import svr, db, sql_u, sql_pw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all columns in DFs\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batch_id\n",
    "now = datetime.now()\n",
    "batch_id = now.strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Concatenation Function\n",
    "def ConCat_pdf (file_list, outfn):\n",
    "    letter_path = './letters/staging/'\n",
    "    writer = PdfWriter()\n",
    "    for inputfn in file_list:\n",
    "        writer.addpages(PdfReader(letter_path + inputfn).pages)\n",
    "\n",
    "    writer.write(outfn)\n",
    "    return outfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Function\n",
    "def delete_file(del_file_path):\n",
    "    if os.path.exists(del_file_path):\n",
    "        os.remove(del_file_path)\n",
    "    else: print (f\"{del_file_path} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PDF Function\n",
    "def fill_pdf(input_pdf_path, output_pdf_path, data_dict):\n",
    "    ANNOT_KEY = '/Annots'\n",
    "    ANNOT_FIELD_KEY = '/T'\n",
    "    ANNOT_VAL_KEY = '/V'\n",
    "    ANNOT_RECT_KEY = '/Rect'\n",
    "    SUBTYPE_KEY = '/Subtype'\n",
    "    WIDGET_SUBTYPE_KEY = '/Widget'\n",
    "\n",
    "    template_pdf = pdfrw.PdfReader(input_pdf_path)\n",
    "    \n",
    "    for page in template_pdf.pages:\n",
    "        annotations = page[ANNOT_KEY]\n",
    "        for annotation in annotations:\n",
    "            if annotation[SUBTYPE_KEY] == WIDGET_SUBTYPE_KEY:\n",
    "                if annotation[ANNOT_FIELD_KEY]:\n",
    "                    key = annotation[ANNOT_FIELD_KEY][1:-1]\n",
    "                    if key in data_dict.keys():\n",
    "                        if type(data_dict[key]) == bool:\n",
    "                            if data_dict[key] == True:\n",
    "                                annotation.update(pdfrw.PdfDict(\n",
    "                                    AS=pdfrw.PdfName('Yes')))\n",
    "                        else:\n",
    "                            annotation.update(\n",
    "                                pdfrw.PdfDict(V='{}'.format(data_dict[key]))\n",
    "                            )\n",
    "                            annotation.update(pdfrw.PdfDict(AP=''))\n",
    "    template_pdf.Root.AcroForm.update(pdfrw.PdfDict(NeedAppearances=pdfrw.PdfObject('true')))\n",
    "\n",
    "    pdfrw.PdfWriter().write(output_pdf_path, template_pdf)\n",
    "\n",
    "    # return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten PDF function\n",
    "def flatten_pdf(flat_output, img_file):\n",
    "    # Fillable PDF to Image\n",
    "    images = convert_from_path(flat_output, dpi=300, size=(2550,3300))\n",
    "    for i in range(len(images)):\n",
    "   \n",
    "    # Save pages as images in the pdf\n",
    "        images[i].save(img_file + '.png', 'PNG')\n",
    "    \n",
    "    # Delete Fillable PDF\n",
    "    delete_file(img_file + '.pdf')\n",
    "    \n",
    "    # opening image\n",
    "    image_file = Image.open(img_file + '.png')\n",
    "    \n",
    "    # Image to Flat PDF\n",
    "    # define paper size\n",
    "    letter = (img2pdf.in_to_pt(8.5), img2pdf.in_to_pt(11))\n",
    "    layout = img2pdf.get_layout_fun(letter)\n",
    "    # converting into chunks using img2pdf\n",
    "    pdf_bytes = img2pdf.convert(image_file.filename, layout_fun=layout)\n",
    "    \n",
    "    # opening or creating pdf file\n",
    "    flat_pdf = f\"{img_file}.pdf\"\n",
    "    file = open(flat_pdf, \"wb\")\n",
    "    \n",
    "    # writing pdf files with chunks\n",
    "    file.write(pdf_bytes)\n",
    "    \n",
    "    # Add file name to file_name list\n",
    "    # file_list.append(flat_pdf)\n",
    "\n",
    "    # closing image file\n",
    "    image_file.close()\n",
    "    \n",
    "     # Delete Fillable PDF\n",
    "    delete_file(img_file + '.png')\n",
    "\n",
    "    # closing pdf file\n",
    "    file.close()\n",
    "\n",
    "    # return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAP Letter function\n",
    "def gap_letter(template_df, pdf_template, position):\n",
    "    gap_path = './letters/staging/'\n",
    "\n",
    "    template_df['payment_amount'] = template_df['payment_amount'].map('${:,.2f}'.format)\n",
    "    template_df['loss_date'] = pd.to_datetime(template_df['loss_date']).dt.strftime('%B %d, %Y')\n",
    "    template_df['StateDesc'] = template_df['StateDesc'].astype(str).replace({'None':''})\n",
    "    template_df['StateCode'] = template_df['StateCode'].astype(str).replace({'None':''})\n",
    "    template_df['f_lang'] = template_df['f_lang'].astype(str).replace({'None':''})\n",
    "    letter_date = f\"{datetime.now():%B %d, %Y}\"   \n",
    "\n",
    "    for index, row in template_df.iterrows():\n",
    "\n",
    "        # empty dict\n",
    "        data_dict = {}\n",
    "        # store field data in dictionary\n",
    "        data_dict = {\n",
    "            'Date': letter_date,\n",
    "            'Lender': template_df.loc[index]['alt_name'],\n",
    "            'Contact': template_df.loc[index]['contact'],\n",
    "            'Address': template_df.loc[index]['address1'],\n",
    "            'City_St_Zip': f\"{template_df.loc[index]['city']}, {template_df.loc[index]['state']} {template_df.loc[index]['zip']}\",\n",
    "            'Lender2': template_df.loc[index]['alt_name'],\n",
    "            'Borrower': f\"{template_df.loc[index]['first']} {template_df.loc[index]['last']}\",\n",
    "            'Claim_Nbr': template_df.loc[index]['claim_nbr'],\n",
    "            'Acct_Nbr': template_df.loc[index]['acct_number'],\n",
    "            'DOL': template_df.loc[index]['loss_date'],\n",
    "            'GAP_Amt': template_df.loc[index]['payment_amount'],\n",
    "            'State': template_df.loc[index]['StateDesc'],\n",
    "            'St_Code': template_df.loc[index]['StateCode'],\n",
    "            'Fraud': template_df.loc[index]['f_lang'],\n",
    "        }\n",
    "\n",
    "        # store paths as variables\n",
    "        output_file = f\"{template_df.loc[index]['claim_nbr']}-{position}.pdf\"\n",
    "        output_path_fn = f\"{gap_path}{output_file}\"\n",
    "\n",
    "        fill_pdf(pdf_template, output_path_fn, data_dict)\n",
    "\n",
    "        # Set File Paths\n",
    "        flat_output = f\"{os.path.dirname(os.path.abspath(output_file))}\\{gap_path}\\{output_file}\"\n",
    "        img_file = f\"{os.path.dirname(os.path.abspath(output_file))}\\{gap_path}\\{template_df.loc[index]['claim_nbr']}-{position}\"\n",
    "\n",
    "        # Flatten pdf using flatten_pdf function\n",
    "        flatten_pdf(flat_output, img_file) \n",
    "\n",
    "    # return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAP Calculation function\n",
    "def calculations(template_df, pdf_template, position):\n",
    "    gap_path = './letters/staging/'\n",
    "\n",
    "    template_df['loss_date'] = pd.to_datetime(template_df['loss_date']).dt.strftime('%B %d, %Y')\n",
    "    template_df['payoff'] = template_df['payoff'].map('${:,.2f}'.format)\n",
    "    template_df['past_due'] = template_df['past_due'].map('${:,.2f}'.format)\n",
    "    template_df['late_fees'] = template_df['late_fees'].map('${:,.2f}'.format)\n",
    "    template_df['skip_pymts'] = template_df['skip_pymts'].map('${:,.2f}'.format)\n",
    "    template_df['skip_fees'] = template_df['skip_fees'].map('${:,.2f}'.format)\n",
    "    template_df['primary_pymt'] = template_df['primary_pymt'].map('${:,.2f}'.format)\n",
    "    template_df['excess_deductible'] = template_df['excess_deductible'].map('${:,.2f}'.format)\n",
    "    template_df['scr'] = template_df['scr'].map('${:,.2f}'.format)\n",
    "    template_df['clr'] = template_df['clr'].map('${:,.2f}'.format)\n",
    "    template_df['cdr'] = template_df['cdr'].map('${:,.2f}'.format)\n",
    "    template_df['oref'] = template_df['oref'].map('${:,.2f}'.format)\n",
    "    template_df['salvage'] = template_df['salvage'].map('${:,.2f}'.format)\n",
    "    template_df['prior_dmg'] = template_df['prior_dmg'].map('${:,.2f}'.format)\n",
    "    template_df['over_ltv'] = template_df['over_ltv'].map('${:,.2f}'.format)\n",
    "    template_df['other1_amt'] = template_df['other1_amt'].map('${:,.2f}'.format)\n",
    "    template_df['other2_amt'] = template_df['other2_amt'].map('${:,.2f}'.format)\n",
    "    template_df['gap_payable'] = template_df['gap_payable'].map('${:,.2f}'.format)\n",
    "    template_df['last_payment'] = pd.to_datetime(template_df['last_payment']).dt.strftime('%B %d, %Y')\n",
    "    template_df['balance_last_pay'] = template_df['balance_last_pay'].map('${:,.2f}'.format)\n",
    "    template_df['per_day'] = template_df['per_day'].map('${:,.2f}'.format)\n",
    "    template_df['incp_date'] = pd.to_datetime(template_df['incp_date']).dt.strftime('%B %d, %Y')\n",
    "    template_df['deductible'] = template_df['deductible'].map('${:,.2f}'.format)\n",
    "    template_df['subtotal'] = template_df['subtotal'].map('${:,.2f}'.format)\n",
    "    template_df['nbr_of_days'] = template_df['nbr_of_days'].map('{:,.0f}'.format)\n",
    "    template_df['interest_rate'] = template_df['interest_rate'].map('{:,.2f}%'.format)\n",
    "    template_df['ltv'] = template_df['ltv'].map('{:,.2f}%'.format)\n",
    "    template_df['ltv_limit'] = template_df['ltv_limit'].map('{:,.2f}%'.format)\n",
    "    template_df['percent_uncovered'] = template_df['percent_uncovered'].map('{:,.2f}%'.format)\n",
    "    template_df['covered_fin_amount'] = template_df['covered_fin_amount'].map('${:,.2f}'.format)\n",
    "    template_df['Amt_Fin'] = template_df['Amt_Fin'].map('${:,.2f}'.format)\n",
    "    template_df['nada_value'] = template_df['nada_value'].map('${:,.2f}'.format)\n",
    "\n",
    "    for index, row in template_df.iterrows():\n",
    "\n",
    "        # empty dict\n",
    "        data_dict = {}\n",
    "        # store field data in dictionary\n",
    "        data_dict = {\n",
    "            'Claim_Number': template_df.loc[index]['claim_nbr'],\n",
    "            'Status': 'Paid',\n",
    "            'Borrower': f\"{template_df.loc[index]['first']} {template_df.loc[index]['last']}\",\n",
    "            'Vehicle': template_df.loc[index]['vehicle'],\n",
    "            'Date_Of_Loss': template_df.loc[index]['loss_date'],\n",
    "            'Type_Of_Loss': template_df.loc[index]['loss_type'],\n",
    "            'Lender': template_df.loc[index]['alt_name'],         \n",
    "            'Lender_Contact': template_df.loc[index]['contact'],\n",
    "            'Insurance_Carrier': template_df.loc[index]['carrier'],\n",
    "            'Inception_Date': template_df.loc[index]['incp_date'],\n",
    "            'Deductible': template_df.loc[index]['deductible'],\n",
    "            'Payoff': template_df.loc[index]['payoff'],\n",
    "            'Past_Due': template_df.loc[index]['past_due'],\n",
    "            'Late_Fees': template_df.loc[index]['late_fees'],\n",
    "            'Skips': template_df.loc[index]['skip_pymts'],\n",
    "            'Skip_Fees': template_df.loc[index]['skip_fees'],\n",
    "            'Primary': template_df.loc[index]['primary_pymt'],\n",
    "            'Deductible_Excess': template_df.loc[index]['excess_deductible'],\n",
    "            'SCR': template_df.loc[index]['scr'],\n",
    "            'CL_Refund': template_df.loc[index]['clr'],\n",
    "            'CD_Refund': template_df.loc[index]['cdr'],\n",
    "            'O_Refund': template_df.loc[index]['oref'],\n",
    "            'Salvage': template_df.loc[index]['salvage'],\n",
    "            'Prior_Damage': template_df.loc[index]['prior_dmg'],\n",
    "            'Over_LTV': template_df.loc[index]['over_ltv'],\n",
    "            'Other1_Description': template_df.loc[index]['other1_description'],\n",
    "            'Other2_Description': template_df.loc[index]['other2_description'],\n",
    "            'Other1': template_df.loc[index]['other1_amt'],\n",
    "            'Other2': template_df.loc[index]['other2_amt'],\n",
    "            'Deduction_Subtotal': template_df.loc[index]['subtotal'],\n",
    "            'GAP_Amt': template_df.loc[index]['gap_payable'], \n",
    "            'Last_pymt_date': template_df.loc[index]['last_payment'], \n",
    "            'DOL': template_df.loc[index]['loss_date'],\n",
    "            'Number_of_days': template_df.loc[index]['nbr_of_days'], \n",
    "            'Loan_Payoff_As_of_DOL': template_df.loc[index]['payoff'],\n",
    "            'Bal_as_of_last_pymt': template_df.loc[index]['balance_last_pay'],\n",
    "            'Interest_Rate': template_df.loc[index]['interest_rate'],\n",
    "            'Interest_Per_Day': template_df.loc[index]['per_day'],\n",
    "            'Amt_financed': template_df.loc[index]['Amt_Fin'],\n",
    "            'ACV': template_df.loc[index]['nada_value'], \n",
    "            'LTV': template_df.loc[index]['ltv'],\n",
    "            'Max_Amt_Financed': template_df.loc[index]['covered_fin_amount'],\n",
    "            'LTV_limit': template_df.loc[index]['ltv_limit'],\n",
    "            'Percentage_Not_Covered': template_df.loc[index]['percent_uncovered']\n",
    "        }\n",
    "\n",
    "        # store paths as variables\n",
    "        output_file = f\"{template_df.loc[index]['claim_nbr']}-{position}.pdf\"\n",
    "        output_path_fn = f\"{gap_path}{output_file}\"\n",
    "\n",
    "        fill_pdf(pdf_template, output_path_fn, data_dict)\n",
    "\n",
    "        # Set File Paths\n",
    "        flat_output = f\"{os.path.dirname(os.path.abspath(output_file))}\\{gap_path}\\{output_file}\"\n",
    "        img_file = f\"{os.path.dirname(os.path.abspath(output_file))}\\{gap_path}\\{template_df.loc[index]['claim_nbr']}-{position}\"\n",
    "\n",
    "        # Flatten pdf using flatten_pdf function\n",
    "        flatten_pdf(flat_output, img_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_dir (path, ext):\n",
    "    for x in os.listdir(path):\n",
    "        if x.endswith(ext):\n",
    "            os.remove(os.path.join(path, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileList (path, ext):\n",
    "    file_list = []\n",
    "    for x in os.listdir(path):\n",
    "        if x.endswith(ext):\n",
    "            file_list.append(x)\n",
    "\n",
    "    # sort list to collate pages\n",
    "    file_list.sort()\n",
    "    \n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TotalRestart Letter function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TotalRestart Calculation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to DB\n",
    "cnx = mc.connect(user=vgc_u, password=vgc_pw,\n",
    "                host=vgc_host,\n",
    "                database='visualgap_claims')\n",
    "cursor = cnx.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql query for GAP claims that are RTBP\n",
    "sql_file = '''\n",
    "            SELECT c.claim_id, c.claim_nbr, c.carrier_id, cl.alt_name, cl.dealer_securityId, cl.contact, cl.address1,\n",
    "                cl.city, cl.state, cl.zip, cl.payment_method, cb.first, cb.last, \n",
    "                IF(sq.gap_amt_paid > 0, 2,1) AS pymt_type_id, \n",
    "                IF(sq.gap_amt_paid > 0, ROUND(cc.gap_payable - sq.gap_amt_paid,2), cc.gap_payable) AS gap_due,\n",
    "                COALESCE(NULLIF(cb.acct_number,''),'0') AS acct_nbr, c.loss_date\n",
    "            FROM claims c\n",
    "            INNER JOIN claim_lender cl\n",
    "                USING (claim_id)\n",
    "            INNER JOIN claim_borrower cb\n",
    "                USING (claim_id)\n",
    "            INNER JOIN claim_calculations cc\n",
    "                USING (claim_id)\n",
    "            INNER JOIN claim_status cs\n",
    "                ON (c.status_id = cs.status_id)\n",
    "            LEFT JOIN (SELECT cp.claim_id, SUM(cp.payment_amount) AS gap_amt_paid\n",
    "                    FROM claim_payments cp\n",
    "                    INNER JOIN (SELECT c.claim_id\n",
    "                                FROM claims c\n",
    "                                INNER JOIN claim_status cs\n",
    "                                    ON (c.status_id = cs.status_id)\n",
    "                                WHERE cs.status_desc_id = 8) rtbp_sq\n",
    "                        USING (claim_id)\n",
    "                    WHERE payment_category_id = 1\n",
    "                    GROUP BY cp.claim_id) sq\n",
    "                ON (c.claim_id = sq.claim_id)\n",
    "            WHERE cs.status_desc_id = 8;\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute sql\n",
    "cursor.execute(sql_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save query results as DF\n",
    "df = pd.DataFrame(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column names\n",
    "df_cols = ['claim_id', 'claim_nbr', 'carrier_id', 'lender_name', 'dealer_securityId', 'contact', 'address1', 'city', 'state', 'zip', \n",
    "                            'pymt_method', 'first', 'last', 'pymt_type_id', 'amount', 'acct_number', 'loss_date']\n",
    "\n",
    "df.columns = df_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql query for PLUS claims that are RTBP\n",
    "sql_file_plus = '''\n",
    "                SELECT sqp.claim_id, c.claim_nbr, c.carrier_id, cl.alt_name, cl.dealer_securityId, cl.contact, \n",
    "                    cl.address1, cl.city, cl.state, cl.zip, cl.payment_method, cb.first, cb.last, \n",
    "                    1 AS pymt_type_id, \n",
    "                    IF(cl.customer_securityId = 9401, 1500, 1000) AS gap_plus_due, COALESCE(NULLIF(cb.acct_number,''),'0') AS acct_nbr,\n",
    "                    c.loss_date\n",
    "                FROM claims c\n",
    "                INNER JOIN claim_lender cl\n",
    "                    USING (claim_id)\n",
    "                INNER JOIN claim_borrower cb\n",
    "                    USING (claim_id)\n",
    "                INNER JOIN (SELECT pb.claim_id\n",
    "                            FROM claim_plus_benefit pb\n",
    "                            WHERE status_desc_id = 8) sqp\n",
    "                    USING (claim_id)\n",
    "                INNER JOIN (SELECT c.claim_id\n",
    "                            FROM claims c\n",
    "                            INNER JOIN claim_status cs\n",
    "                                ON (c.status_id = cs.status_id)  \n",
    "                            WHERE cs.status_desc_id = 8\n",
    "                                OR cs.status_desc_id = 4) sqg\n",
    "                    USING (claim_id);\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute sql\n",
    "cursor.execute(sql_file_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save query results as DF\n",
    "df2 = pd.DataFrame(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column names\n",
    "df2_cols = ['claim_id', 'claim_nbr', 'carrier_id', 'lender_name', 'dealer_securityId', 'contact', 'address1', 'city', 'state', 'zip', \n",
    "                            'pymt_method', 'first', 'last', 'pymt_type_id', 'amount', 'acct_number', 'loss_date']\n",
    "\n",
    "df2.columns = df2_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql query for TotalRestart claims that are RTBP\n",
    "sql_file_tr = '''\n",
    "            SELECT sqp.claim_id, c.claim_nbr, c.carrier_id, cl.alt_name, cl.dealer_securityId, cl.contact, cl.address1, \n",
    "            cl.city, cl.state, cl.zip, 'Check' AS payment_method, cb.first, cb.last, 1 AS pymt_type_id, \n",
    "            ctr.totalrestart_payable AS tr_due, COALESCE(NULLIF(cb.acct_number,''),'0') AS acct_nbr, c.loss_date\n",
    "            FROM claims c\n",
    "            INNER JOIN claim_lender cl\n",
    "                USING (claim_id)\n",
    "            INNER JOIN claim_borrower cb\n",
    "                USING (claim_id)\n",
    "            INNER JOIN (SELECT pb.claim_id\n",
    "                        FROM claim_totalrestart pb\n",
    "                        WHERE status_desc_id = 8) sqp\n",
    "                USING (claim_id)\n",
    "            INNER JOIN claim_totalrestart ctr\n",
    "                USING (claim_id)\n",
    "             '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute sql\n",
    "cursor.execute(sql_file_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save query results as DF\n",
    "df3 = pd.DataFrame(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column names\n",
    "df3_cols = ['claim_id', 'claim_nbr', 'carrier_id', 'lender_name', 'dealer_securityId', 'contact', 'address1', 'city', 'state', 'zip', \n",
    "                            'pymt_method', 'first', 'last', 'pymt_type_id', 'amount', 'acct_number', 'loss_date']\n",
    "\n",
    "df3.columns = df3_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close mysql connection\n",
    "cursor.close()\n",
    "cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get expense account name\n",
    "cnx = mc.connect(user=mysql_u, password=mysql_pw,\n",
    "                host=mysql_host,\n",
    "                database='claim_qb_payments')\n",
    "cursor = cnx.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAP\n",
    "# convert columns list to string\n",
    "cols = \", \".join(df_cols)\n",
    "\n",
    "# insert DF into the ready_to_be_paid table of the claim_qb_payments database\n",
    "for x,rows in df.iterrows():\n",
    "\n",
    "    sql_file2 = '''INSERT INTO ready_to_be_paid ({columns}, payment_category_id, check_nbr, batch_id, qb_txnid, toVGC) VALUES ({claim_id}, \"{claim_nbr}\", {carrier_id},\"{lender_name}\", \"{lender_id}\",\"{contact}\", \n",
    "                \"{address1}\", \"{city}\", \"{state}\", \"{zip}\", \"{pymt_method}\", \"{first}\", \"{last}\", {pymt_type_id}, {amount}, \"{acct_nbr}\", \"{loss_date}\", 1, 0, {batchId}, 0, 0);'''.format(columns=cols, \n",
    "                claim_id=rows['claim_id'], claim_nbr=rows['claim_nbr'], carrier_id=rows['carrier_id'], lender_name=rows['lender_name'], lender_id=rows['dealer_securityId'], \n",
    "                contact=rows['contact'], address1=rows['address1'], city=rows['city'], state=rows['state'], zip=rows['zip'], \n",
    "                pymt_method=rows['pymt_method'], first = rows['first'], last = rows['last'], pymt_type_id = rows['pymt_type_id'], \n",
    "                amount = rows['amount'], acct_nbr = rows['acct_number'], loss_date = rows['loss_date'], batchId = batch_id)\n",
    "     \n",
    "    # execute and commit sql\n",
    "    cursor.execute(sql_file2)\n",
    "    cnx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLUS\n",
    "# convert columns list to string\n",
    "cols = \", \".join(df2_cols)\n",
    "\n",
    "# insert DF into the ready_to_be_paid table of the claim_qb_payments database\n",
    "for x,rows in df2.iterrows():\n",
    "    sql_file2_plus = '''INSERT INTO ready_to_be_paid ({columns}, payment_category_id, check_nbr, batch_id, qb_txnid, toVGC) VALUES ({claim_id}, \"{claim_nbr}\", {carrier_id},\"{lender_name}\", \"{lender_id}\",\"{contact}\", \n",
    "                \"{address1}\", \"{city}\", \"{state}\", \"{zip}\", \"{pymt_method}\", \"{first}\", \"{last}\", {pymt_type_id}, {amount}, \"{acct_nbr}\", \"{loss_date}\", 2, 0, {batchId}, 0, 0);'''.format(columns=cols, \n",
    "                claim_id=rows['claim_id'], claim_nbr=rows['claim_nbr'], carrier_id=rows['carrier_id'], lender_name=rows['lender_name'], lender_id=rows['dealer_securityId'], \n",
    "                contact=rows['contact'], address1=rows['address1'], city=rows['city'], state=rows['state'], zip=rows['zip'], \n",
    "                pymt_method=rows['pymt_method'], first = rows['first'], last = rows['last'], pymt_type_id = rows['pymt_type_id'], \n",
    "                amount = rows['amount'], acct_nbr = rows['acct_number'], loss_date = rows['loss_date'], batchId = batch_id)\n",
    "          \n",
    "    # execute and commit sql\n",
    "    cursor.execute(sql_file2_plus)\n",
    "    cnx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOTALRESTART\n",
    "# convert columns list to string\n",
    "cols = \", \".join(df3_cols)\n",
    "\n",
    "# insert DF into the ready_to_be_paid table of the claim_qb_payments database\n",
    "for x,rows in df3.iterrows():\n",
    "    sql_file2_tr = '''INSERT INTO ready_to_be_paid ({columns}, payment_category_id, check_nbr, batch_id, qb_txnid, toVGC) VALUES ({claim_id}, \"{claim_nbr}\", {carrier_id},\"{lender_name}\", \"{lender_id}\",\"{contact}\", \n",
    "                \"{address1}\", \"{city}\", \"{state}\", \"{zip}\", \"{pymt_method}\", \"{first}\", \"{last}\", {pymt_type_id}, {amount}, \"{acct_nbr}\", \"{loss_date}\", 3, 0, {batchId}, 0, 0);'''.format(columns=cols, \n",
    "                claim_id=rows['claim_id'], claim_nbr=rows['claim_nbr'], carrier_id=rows['carrier_id'], lender_name=rows['lender_name'], lender_id=rows['dealer_securityId'], \n",
    "                contact=rows['contact'], address1=rows['address1'], city=rows['city'], state=rows['state'], zip=rows['zip'], \n",
    "                pymt_method=rows['pymt_method'], first = rows['first'], last = rows['last'], pymt_type_id = rows['pymt_type_id'], \n",
    "                amount = rows['amount'], acct_nbr = rows['acct_number'], loss_date = rows['loss_date'], batchId = batch_id)\n",
    "          \n",
    "    # execute and commit sql\n",
    "    cursor.execute(sql_file2_tr)\n",
    "    cnx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create select query to pull current batch with ID\n",
    "sql_file3 = '''SELECT rtbp_id, claim_id, claim_nbr, carrier_id, lender_name, dealer_securityId, contact, address1, city, \n",
    "                     state, zip, pymt_method, first, last, pymt_type_id, amount, payment_category_id, check_nbr, batch_id, \n",
    "                     qb_txnid, acct_number, loss_date\n",
    "              FROM ready_to_be_paid\n",
    "              WHERE batch_id = {batchId}\n",
    "              \n",
    "              ORDER BY payment_category_id, claim_id;'''.format(batchId = batch_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute sql\n",
    "cursor.execute(sql_file3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save query results as DF\n",
    "pymts_df = pd.DataFrame(cursor.fetchall())\n",
    "\n",
    "# add column names\n",
    "pymts_df_cols = ['rtbp_id', 'claim_id', 'claim_nbr', 'carrier_id', 'lender_name', 'dealer_securityId', 'contact', 'address1', 'city', 'state', 'zip', \n",
    "                    'pymt_method', 'first', 'last', 'pymt_type_id', 'amount','payment_category_id', 'check_nbr', 'batch_id', 'qb_txnid', \n",
    "                    'acct_number', 'loss_date']\n",
    "\n",
    "pymts_df.columns = pymts_df_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql query for expense accounts in DB\n",
    "sql_file4 = '''\n",
    "    SELECT carrier_id, qb_fullname\n",
    "    FROM qb_accounts\n",
    "    WHERE account_type = 'Expense';\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute sql\n",
    "cursor.execute(sql_file4)\n",
    "# save query results as DF\n",
    "expense_df = pd.DataFrame(cursor.fetchall())\n",
    "# add column names to DF\n",
    "col_names = ['carrier_id', 'expense']\n",
    "expense_df.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql query for checking accounts in DB\n",
    "sql_file5 = '''\n",
    "    SELECT carrier_id, qb_fullname\n",
    "    FROM qb_accounts\n",
    "    WHERE account_type = 'Checking';\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute sql\n",
    "cursor.execute(sql_file5)\n",
    "# save query results as DF\n",
    "checking_df = pd.DataFrame(cursor.fetchall())\n",
    "# add column names to DF\n",
    "col_names = ['carrier_id', 'checking']\n",
    "checking_df.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge expense account name into df\n",
    "pymts_df = pymts_df.merge(expense_df, left_on='carrier_id', right_on='carrier_id').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge checking account name into df\n",
    "pymts_df = pymts_df.merge(checking_df, left_on='carrier_id', right_on='carrier_id').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sql server connection\n",
    "cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+svr+';DATABASE='+db+';UID='+sql_u+';PWD='+ sql_pw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create query\n",
    "sql_svr_file = '''SELECT VGSecurityId, QB_ListID\n",
    "                  FROM business_entity\n",
    "                  WHERE QB_ListID IS NOT NULL;\n",
    "               '''\n",
    "# execute query\n",
    "listid_df = pd.read_sql(sql_svr_file, cnxn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY ########################################################################################\n",
    "# Convert test dealer_securityId to Production dealer_securityId\n",
    "pymts_df['dealer_securityId'].replace({22260:46724,21945:52715}, inplace=True)\n",
    "# TEMPORARY ########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOTALRESTART #####################################################################################\n",
    "# Update TR claims with the correct 'checking' & 'expense' names and possibly QB_ListID\n",
    "# maybe change carrier to pull correct 'checking' & 'expense' names and possibly QB_ListID??\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge QB_ListID into df\n",
    "pymts_df = pymts_df.merge(listid_df, left_on='dealer_securityId', right_on='VGSecurityId').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add carrier name\n",
    "cnx_c = mc.connect(user=vgc_u, password=vgc_pw,\n",
    "                host=vgc_host,\n",
    "                database='visualgap_claims')\n",
    "cursor_c = cnx_c.cursor()\n",
    "\n",
    "sql_file6 = '''\n",
    "    SELECT carrier_id, description\n",
    "    FROM carriers;\n",
    "    '''\n",
    "cursor_c.execute(sql_file6)\n",
    "\n",
    "carrier_df = pd.DataFrame(cursor_c.fetchall())\n",
    "\n",
    "col_names = ['carrier_id', 'carrier']\n",
    "carrier_df.columns = col_names\n",
    "\n",
    "cursor_c.close()\n",
    "cnx_c.close()\n",
    "\n",
    "# Merge QB_ListID into df\n",
    "pymts_df = pymts_df.merge(carrier_df, left_on='carrier_id', right_on='carrier_id').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# payments greater than 0 df\n",
    "qb_pymts_df = pymts_df.loc[pymts_df['amount'] > 0]\n",
    "qb_pymts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Quickbooks\n",
    "sessionManager = wc.Dispatch(\"QBXMLRP2.RequestProcessor\")    \n",
    "sessionManager.OpenConnection('', 'Test qbXML Request')\n",
    "ticket = sessionManager.BeginSession(\"\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create qbxml query to add payments to QB\n",
    "pay_date = f\"{dt.date.today():%Y-%m-%d}\"\n",
    "\n",
    "for index, row in pymts_df.iterrows():\n",
    "    if row['payment_category_id'] == 1:\n",
    "        if row['pymt_type_id'] == 2:\n",
    "            pymt_type = 'Additional GAP Claim Pymt'\n",
    "        else: \n",
    "            pymt_type = 'GAP Claim'\n",
    "\n",
    "    elif row['payment_category_id'] == 2:\n",
    "        if row['pymt_type_id'] == 2:\n",
    "            pymt_type = 'Additional GAP Plus Pymt'\n",
    "        else: \n",
    "            pymt_type = 'GAP Plus'\n",
    "            \n",
    "    elif row['payment_category_id'] == 3:\n",
    "        if row['pymt_type_id'] == 2:\n",
    "            pymt_type = 'Additional TotalRestart Pymt'\n",
    "        else: \n",
    "            pymt_type = 'TotalRestart'\n",
    "\n",
    "    pymtAmt = \"{:.2f}\".format(row['amount'])\n",
    "\n",
    "    if row['pymt_method'] == 'Check':\n",
    "        qbxmlQuery = '''\n",
    "        <?qbxml version=\"14.0\"?>\n",
    "        <QBXML>\n",
    "            <QBXMLMsgsRq onError=\"stopOnError\">\n",
    "                <CheckAddRq>\n",
    "                    <CheckAdd>\n",
    "                        <AccountRef>\n",
    "                            <FullName>{checking}</FullName>\n",
    "                        </AccountRef>\n",
    "                        <PayeeEntityRef>\n",
    "                            <ListID>{lender_qbid}</ListID>\n",
    "                        </PayeeEntityRef>\n",
    "                        <TxnDate>{date}</TxnDate>\n",
    "                        <Memo>{memo}</Memo>\n",
    "                        <Address>\n",
    "                            <Addr1>{lender}</Addr1>\n",
    "                            <Addr2>{contact}</Addr2>\n",
    "                            <Addr3>{address}</Addr3>\n",
    "                            <City>{city}</City>\n",
    "                            <State>{state}</State>\n",
    "                            <PostalCode>{zip}</PostalCode>\n",
    "                        </Address>\n",
    "                        <IsToBePrinted>true</IsToBePrinted>\n",
    "                        <ExpenseLineAdd>\n",
    "                            <AccountRef>\n",
    "                                <FullName>{expense}</FullName>\n",
    "                            </AccountRef>\n",
    "                            <Amount>{amount}</Amount>\n",
    "                            <Memo>{memo}</Memo>\n",
    "                        </ExpenseLineAdd>\n",
    "                    </CheckAdd>\n",
    "                </CheckAddRq>\n",
    "            </QBXMLMsgsRq>\n",
    "        </QBXML>'''.format(checking=row['checking'], lender_qbid=row['QB_ListID'], lender=row['lender_name'], date=pay_date, \n",
    "                memo=f\"{row['last']}/{row['first']} {pymt_type}\", contact=row['contact'], address=row['address1'], city=row['city'], state=row['state'],\n",
    "                zip=row['zip'], expense=row['expense'], amount = pymtAmt)\n",
    "\n",
    "    elif 'ACH' in row['pymt_method']:\n",
    "        qbxmlQuery = '''\n",
    "        <?qbxml version=\"14.0\"?>\n",
    "        <QBXML>\n",
    "            <QBXMLMsgsRq onError=\"stopOnError\">\n",
    "                <CheckAddRq>\n",
    "                    <CheckAdd>\n",
    "                        <AccountRef>\n",
    "                            <FullName>{checking}</FullName>\n",
    "                        </AccountRef>\n",
    "                        <PayeeEntityRef>\n",
    "                            <ListID>{lender_qbid}</ListID>\n",
    "                        </PayeeEntityRef>\n",
    "                        <RefNumber>ACH</RefNumber>\n",
    "                        <TxnDate>{date}</TxnDate>\n",
    "                        <Memo>{memo}</Memo>\n",
    "                        <Address>\n",
    "                            <Addr1>{lender}</Addr1>\n",
    "                            <Addr2>{contact}</Addr2>\n",
    "                            <Addr3>{address}</Addr3>\n",
    "                            <City>{city}</City>\n",
    "                            <State>{state}</State>\n",
    "                            <PostalCode>{zip}</PostalCode>\n",
    "                        </Address>\n",
    "                        <IsToBePrinted>false</IsToBePrinted>\n",
    "                        <ExpenseLineAdd>\n",
    "                            <AccountRef>\n",
    "                                <FullName>{expense}</FullName>\n",
    "                            </AccountRef>\n",
    "                            <Amount>{amount}</Amount>\n",
    "                            <Memo>{memo}</Memo>\n",
    "                        </ExpenseLineAdd>\n",
    "                    </CheckAdd>\n",
    "                </CheckAddRq>            \n",
    "            </QBXMLMsgsRq>\n",
    "        </QBXML>'''.format(checking=row['checking'], lender_qbid=row['QB_ListID'], lender=row['lender_name'], date=pay_date, \n",
    "                memo=f\"{row['last']}/{row['first']} {pymt_type}\", contact=row['contact'], address=row['address1'], city=row['city'], state=row['state'],\n",
    "                zip=row['zip'], expense=row['expense'], amount = pymtAmt)\n",
    "        \n",
    "    # Send query and receive response\n",
    "    responseString = sessionManager.ProcessRequest(ticket, qbxmlQuery)\n",
    "\n",
    "    # output TxnID\n",
    "    QBXML = ET.fromstring(responseString)\n",
    "    QBXMLMsgsRs = QBXML.find('QBXMLMsgsRs')\n",
    "    checkResults = QBXMLMsgsRs.iter(\"CheckRet\")\n",
    "    txnId = 0\n",
    "    for checkResult in checkResults:\n",
    "        txnId = checkResult.find('TxnID').text\n",
    "\n",
    "    # Add TxnID to ready_to_be_paid table\n",
    "    sql_file6 = '''UPDATE ready_to_be_paid\n",
    "                   SET qb_txnid = '{TxnID}'\n",
    "                   WHERE rtbp_id = {rowID};'''.format(TxnID=txnId, rowID=row['rtbp_id'])\n",
    "    \n",
    "    # execute and commit sql\n",
    "    cursor.execute(sql_file6)\n",
    "    cnx.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disconnect from Quickbooks\n",
    "sessionManager.EndSession(ticket)\n",
    "sessionManager.CloseConnection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close mysql connection\n",
    "cursor.close()\n",
    "cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and print claim letter and calculation\n",
    "# Import functions to create letters [payment_category_id = GAP (1), PLUS(2), TOTALRESTART (3)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Fraud Language fields to pymts_df\n",
    "cnx_f = mc.connect(user=vgc_u, password=vgc_pw,\n",
    "                host=vgc_host,\n",
    "                database='visualgap_claims')\n",
    "cursor_f = cnx_f.cursor()\n",
    "\n",
    "sql_file7 = '''\n",
    "    SELECT StateId, \n",
    "    StateDesc,\n",
    "    StateCode,\n",
    "    CAST(Language AS CHAR(1000) CHARACTER SET utf8)\n",
    "    FROM FraudLang;\n",
    "    '''\n",
    "cursor_f.execute(sql_file7)\n",
    "\n",
    "fraud_lang_df = pd.DataFrame(cursor_f.fetchall())\n",
    "\n",
    "col_names = ['StateId', 'StateDesc', 'StateCode', 'f_lang']\n",
    "fraud_lang_df.columns = col_names\n",
    "\n",
    "cursor_f.close()\n",
    "cnx_f.close()\n",
    "\n",
    "# Merge QB_ListID into df\n",
    "pymts_df = pymts_df.merge(fraud_lang_df, left_on='state', right_on='StateId').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Column names\n",
    "pymts_df.rename(columns = {'amount':'payment_amount', 'lender_name':'alt_name'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create GAP Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect GAP payments\n",
    "gap_pymts_df = pymts_df.loc[pymts_df['payment_category_id'] == 1].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_letters_df = gap_pymts_df.loc[gap_pymts_df['payment_amount'] > 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove files from staging directory\n",
    "file_staging_dir = './letters/staging/'\n",
    "file_ext = \".pdf\"\n",
    "\n",
    "clear_dir(file_staging_dir, file_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create letters for amounts greater than $0 paid via check\n",
    "\n",
    "# Filter gap_letters_df for pymt_method 'Check'\n",
    "checks_df = gap_letters_df.loc[gap_letters_df['pymt_method'] == 'Check'].copy()\n",
    "\n",
    "if len(checks_df.index) > 0:\n",
    "    # Create list of Carriers on checks_df\n",
    "    carriers = []\n",
    "    carriers = checks_df.carrier.unique()\n",
    "    letter_cols = ['claim_nbr', 'loss_date', 'alt_name', 'contact', 'address1', 'city', 'state', 'zip', 'first', 'last', 'acct_number', 'payment_amount', 'StateDesc', 'StateCode', 'f_lang' ]\n",
    "\n",
    "    # Loop through carriers list\n",
    "    for carrier in carriers:\n",
    "\n",
    "        # Variable Defaults\n",
    "        sql_where_cal = ''\n",
    "        g_letters = True\n",
    "        s_letters = True\n",
    "        cals = []\n",
    "\n",
    "        # check for payment_type_id for GAP letters by carrier\n",
    "        g_letters_df = checks_df.loc[(checks_df['pymt_type_id'] == 1) & (checks_df['carrier'] == carrier)].copy()\n",
    "\n",
    "        # # GAP Letter - create WHERE statement\n",
    "        if len(g_letters_df.index) > 0:\n",
    "            g_letters = True\n",
    "        else:\n",
    "            # No Letters\n",
    "            g_letters = False\n",
    "\n",
    "        # check for payment_type_id for Supplemental letters\n",
    "        s_letters_df = checks_df.loc[(checks_df['pymt_type_id'] == 2) & (checks_df['carrier'] == carrier)].copy()\n",
    "\n",
    "        # Supplemental Letter\n",
    "        if len(s_letters_df.index) > 0:\n",
    "            s_letters = True\n",
    "        else:\n",
    "            # No Letters\n",
    "            s_letters = False\n",
    "\n",
    "        # Calculations\n",
    "        # check for calculations for carrier\n",
    "        calculations_df = checks_df.loc[checks_df['carrier'] == carrier].copy()\n",
    "\n",
    "        if len(calculations_df.index) > 0:\n",
    "            # Multiple Calculation\n",
    "            for index, row in calculations_df.iterrows():\n",
    "                cals.append(row['claim_id'])\n",
    "\n",
    "        # Connect to SQL DB\n",
    "        cnx = mc.connect(user=vgc_u, password=vgc_pw,\n",
    "                        host=vgc_host,\n",
    "                        database='visualgap_claims')\n",
    "        cursor = cnx.cursor()\n",
    "\n",
    "        # GAP - Create letters\n",
    "        if g_letters == True:\n",
    "            # create df for template\n",
    "            g_template_df = g_letters_df[letter_cols]\n",
    "\n",
    "            # Create GAP Letters\n",
    "            g_pdf_template = \"letters/pdf_templates/GAP_letter_template.pdf\"\n",
    "            position = 1\n",
    "            gap_letter(g_template_df, g_pdf_template, position)\n",
    "        \n",
    "        # Supplemental - Create letters\n",
    "        if s_letters == True:\n",
    "            # create df for template\n",
    "            s_template_df = s_letters_df[letter_cols]\n",
    "            \n",
    "            # Create Supplement Letters\n",
    "            s_pdf_template = \"letters/pdf_templates/Supp_letter_template.pdf\"\n",
    "            position = 2\n",
    "            gap_letter(s_template_df, s_pdf_template, position)\n",
    "\n",
    "        # Calculations - Create SQL query, run query, create calculation sheets\n",
    "        if g_letters == True or s_letters == True:\n",
    "\n",
    "            # create calculation dataframe\n",
    "            temp_cols = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]                             \n",
    "            c_consolidate_df = pd.DataFrame(columns = temp_cols)\n",
    "\n",
    "            for cal in cals:\n",
    "                # create sql for calculation\n",
    "                c_sql_query = '''\n",
    "                        SELECT c.claim_nbr, c.loss_date, l.alt_name, l.contact, b.first, b.last, cc.gap_payable,\n",
    "                            cl.incp_date, cl.last_payment, cl.interest_rate, cl.amount AS Amt_Fin, cc.balance_last_pay,\n",
    "                            cc.nbr_of_days, cc.per_day, ROUND(cc.payoff,2) AS payoff, (cc.ltv * 100) AS ltv, cc.covered_fin_amount,\n",
    "                            (cc.percent_uncovered * 100) AS percent_uncovered, (cc.ltv_limit * 100) AS ltv_limit, v.nada_value, CONCAT(v.year, ' ', v.make, ' ', v.model) AS vehicle,\n",
    "                            v.deductible, cls.description AS loss_type, cc26.description AS primary_carrier,\n",
    "                            cc14.amount AS past_due, cc15.amount AS late_fees, cc16.amount AS skip_pymts,\n",
    "                            cc17.amount AS skip_fees, cc5.amount AS primary_pymt, cc7.amount AS excess_deductible,\n",
    "                            cc8.amount AS scr, cc9.amount AS clr, cc10.amount AS cdr, cc11.amount AS oref,\n",
    "                            cc18.amount AS salvage, cc19.amount AS prior_dmg, cc20.amount AS over_ltv,\n",
    "                            cc21.amount AS other1_amt, cc21.description AS other1_description,\n",
    "                            cc22.amount AS other2_amt, cc22.description AS other2_description, ca.description AS carrier,\n",
    "                            (cc22.amount + cc21.amount + cc20.amount + cc19.amount + cc18.amount + cc11.amount + cc10.amount + cc9.amount + \n",
    "                            cc8.amount + cc7.amount + cc5.amount + cc17.amount + cc16.amount + cc15.amount + cc14.amount) as subtotal\n",
    "                        FROM claims c\n",
    "                        INNER JOIN claim_lender l\n",
    "                            USING (claim_id)\n",
    "                        INNER JOIN claim_borrower b\n",
    "                            USING (claim_id)\n",
    "                        INNER JOIN claim_loan cl\n",
    "                            USING (claim_id)\n",
    "                        INNER JOIN claim_calculations cc\n",
    "                            USING (claim_id)\n",
    "                        INNER JOIN claim_vehicle v\n",
    "                            USING (claim_id)\n",
    "                        INNER JOIN claims_loss_type cls\n",
    "                            USING (loss_type_id)\n",
    "                        INNER JOIN carriers ca\n",
    "                            ON c.carrier_id = ca.carrier_id                        \n",
    "                        INNER JOIN claim_checklist AS cc26\n",
    "                            ON c.claim_id = cc26.claim_id\n",
    "                            AND cc26.checklist_item_id = 26    \n",
    "                        INNER JOIN claim_checklist AS cc14\n",
    "                            ON c.claim_id = cc14.claim_id\n",
    "                            AND cc14.checklist_item_id = 14    \n",
    "                        INNER JOIN claim_checklist AS cc15\n",
    "                            ON c.claim_id = cc15.claim_id\n",
    "                            AND cc15.checklist_item_id = 15    \n",
    "                        INNER JOIN claim_checklist AS cc16\n",
    "                            ON c.claim_id = cc16.claim_id\n",
    "                            AND cc16.checklist_item_id = 16       \n",
    "                        INNER JOIN claim_checklist AS cc17\n",
    "                            ON c.claim_id = cc17.claim_id\n",
    "                            AND cc17.checklist_item_id = 17       \n",
    "                        INNER JOIN claim_checklist AS cc5\n",
    "                            ON c.claim_id = cc5.claim_id\n",
    "                            AND cc5.checklist_item_id = 5\n",
    "                        INNER JOIN claim_checklist AS cc7\n",
    "                            ON c.claim_id = cc7.claim_id\n",
    "                            AND cc7.checklist_item_id = 7\n",
    "                        INNER JOIN claim_checklist AS cc8\n",
    "                            ON c.claim_id = cc8.claim_id\n",
    "                            AND cc8.checklist_item_id = 8\n",
    "                        INNER JOIN claim_checklist AS cc9\n",
    "                            ON c.claim_id = cc9.claim_id\n",
    "                            AND cc9.checklist_item_id = 9\n",
    "                        INNER JOIN claim_checklist AS cc10\n",
    "                            ON c.claim_id = cc10.claim_id\n",
    "                            AND cc10.checklist_item_id = 10\n",
    "                        INNER JOIN claim_checklist AS cc11\n",
    "                            ON c.claim_id = cc11.claim_id\n",
    "                            AND cc11.checklist_item_id = 11      \n",
    "                        INNER JOIN claim_checklist AS cc18\n",
    "                            ON c.claim_id = cc18.claim_id\n",
    "                            AND cc18.checklist_item_id = 18\n",
    "                        INNER JOIN claim_checklist AS cc19\n",
    "                            ON c.claim_id = cc19.claim_id\n",
    "                            AND cc19.checklist_item_id = 19\n",
    "                        INNER JOIN claim_checklist AS cc20\n",
    "                            ON c.claim_id = cc20.claim_id\n",
    "                            AND cc20.checklist_item_id = 20     \n",
    "                        INNER JOIN claim_checklist AS cc21\n",
    "                            ON c.claim_id = cc21.claim_id\n",
    "                            AND cc21.checklist_item_id = 21\n",
    "                        INNER JOIN claim_checklist AS cc22\n",
    "                            ON c.claim_id = cc22.claim_id\n",
    "                            AND cc22.checklist_item_id = 22  \n",
    "                        WHERE c.claim_id = {claimID};'''.format(claimID = cal)\n",
    "\n",
    "                cursor.execute(c_sql_query)\n",
    "                # save query results as DF\n",
    "                c_temp_df = pd.DataFrame(cursor.fetchall())\n",
    "\n",
    "                # append query result to c_template_df\n",
    "                c_consolidate_df = c_consolidate_df.append(c_temp_df)\n",
    "\n",
    "            cal_cols = {0:'claim_nbr', 1:'loss_date', 2:'alt_name', 3:'contact', 4:'first', 5:'last', 6:'gap_payable', 7:'incp_date', 8:'last_payment', 9:'interest_rate', 10:'Amt_Fin',\n",
    "                11:'balance_last_pay', 12:'nbr_of_days', 13:'per_day', 14:'payoff', 15:'ltv', 16:'covered_fin_amount', 17:'percent_uncovered', 18:'ltv_limit', 19:'nada_value',\n",
    "                20:'vehicle', 21:'deductible', 22:'loss_type', 23:'primary_carrier', 24:'past_due', 25:'late_fees', 26:'skip_pymts', 27:'skip_fees', 28:'primary_pymt',\n",
    "                29:'excess_deductible', 30:'scr', 31:'clr', 32:'cdr', 33:'oref', 34:'salvage', 35:'prior_dmg', 36:'over_ltv', 37:'other1_amt', 38:'other1_description',\n",
    "                39:'other2_amt', 40:'other2_description', 41:'carrier', 42:'subtotal'}\n",
    "\n",
    "            c_template_df = c_consolidate_df.rename(columns = cal_cols)\n",
    "            c_template_df = c_template_df.reset_index(drop=True)\n",
    "\n",
    "            # Create Calculations\n",
    "            c_pdf_template = \"letters/pdf_templates/GAP_calculation_template.pdf\"\n",
    "            position = 3\n",
    "            calculations(c_template_df, c_pdf_template, position)\n",
    "                \n",
    "        # Close SQL Connection\n",
    "        cursor.close()\n",
    "        cnx.close()\n",
    "\n",
    "        # create \n",
    "        file_list = fileList(file_staging_dir, file_ext)\n",
    "\n",
    "        # Concatenated output file\n",
    "        outfn = f'S:/claims/letters/{carrier}_{now.strftime(\"%Y-%m-%d\")}_GAP.pdf'\n",
    "\n",
    "        ConCat_pdf(file_list, outfn)\n",
    "\n",
    "        # Remove files from staging directory\n",
    "        clear_dir(file_staging_dir, file_ext)\n",
    "\n",
    "else: print('No amount greater then 0.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Plus Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect GAP Plus\n",
    "plus_pymts_df = pymts_df.loc[pymts_df['payment_category_id'] == 2].copy()\n",
    "# plus_pymts_df.rename(columns = {'amount':'payment_amount', 'lender_name':'alt_name'}, inplace = True)\n",
    "plus_letters_df = plus_pymts_df.loc[plus_pymts_df['payment_amount'] > 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAP plus Letters\n",
    "\n",
    "# Filter plus_letters_df for pymt_method 'Check'\n",
    "plus_df = plus_letters_df.loc[plus_letters_df['pymt_method'] == 'Check'].copy()\n",
    "\n",
    "if len(plus_df.index) > 0:\n",
    "\n",
    "    # Create list of Carriers on checks_df\n",
    "    p_carriers = []\n",
    "    p_carriers = plus_df.carrier.unique()\n",
    "\n",
    "    # Loop through carriers list\n",
    "    for p_carrier in p_carriers:\n",
    "\n",
    "        # Variable Defaults\n",
    "        p_letters = True\n",
    "\n",
    "        # check for payment_type_id for GAP letters by carrier\n",
    "        p_letters_df = plus_df.loc[plus_df['carrier'] == p_carrier].copy()\n",
    "\n",
    "        # check for records\n",
    "        if len(p_letters_df.index) > 0:\n",
    "            p_letters = True\n",
    "        else:\n",
    "            # No Letters\n",
    "            p_letters = False\n",
    "\n",
    "        # PLUS - Create letters\n",
    "        if p_letters == True:\n",
    "            # create df for template\n",
    "            p_template_df = p_letters_df[letter_cols]\n",
    "\n",
    "            # Create GAP Letters\n",
    "            p_pdf_template = \"letters/pdf_templates/PLUS_letter_template.pdf\"\n",
    "            position = 1\n",
    "            gap_letter(p_template_df, p_pdf_template, position)\n",
    "        \n",
    "            file_list = fileList(file_staging_dir, file_ext)\n",
    "\n",
    "            # Concatenated output file\n",
    "            outfn = f'S:/claims/letters/{p_carrier}_{now.strftime(\"%Y-%m-%d\")}_PLUS.pdf'\n",
    "\n",
    "            ConCat_pdf(file_list, outfn)\n",
    "\n",
    "        # Remove files from staging directory\n",
    "        clear_dir(file_staging_dir, file_ext)\n",
    "\n",
    "else: print('No Plus.')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25bbdcee2c7b2635f2af9203b4c23ac3a2ece7544aac2f4db010efbeb60ffe95"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 32-bit ('pyfrost32-dev': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
