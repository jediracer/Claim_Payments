{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import win32com.client as wc\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import mysql.connector as mc\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import pyodbc\n",
    "from pdfrw import PdfReader, PdfWriter\n",
    "import pdfrw \n",
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "import img2pdf\n",
    "from PIL import Image\n",
    "\n",
    "# Get sql user and password\n",
    "from configTest import mysql_host, mysql_u, mysql_pw\n",
    "from configTest import vgc_host, vgc_u, vgc_pw\n",
    "from configTest import svr, db, sql_u, sql_pw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all columns in DFs\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batch_id\n",
    "now = datetime.now()\n",
    "batch_id = now.strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Concatenation Function\n",
    "def ConCat_pdf (file_list, outfn):\n",
    "    letter_path = './letters/staging/'\n",
    "    writer = PdfWriter()\n",
    "    for inputfn in file_list:\n",
    "        writer.addpages(PdfReader(letter_path + inputfn).pages)\n",
    "\n",
    "    writer.write(outfn)\n",
    "    return outfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Function\n",
    "def delete_file(del_file_path):\n",
    "    if os.path.exists(del_file_path):\n",
    "        os.remove(del_file_path)\n",
    "    else: print (f\"{del_file_path} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PDF Function\n",
    "def fill_pdf(input_pdf_path, output_pdf_path, data_dict):\n",
    "    ANNOT_KEY = '/Annots'\n",
    "    ANNOT_FIELD_KEY = '/T'\n",
    "    ANNOT_VAL_KEY = '/V'\n",
    "    ANNOT_RECT_KEY = '/Rect'\n",
    "    SUBTYPE_KEY = '/Subtype'\n",
    "    WIDGET_SUBTYPE_KEY = '/Widget'\n",
    "\n",
    "    template_pdf = pdfrw.PdfReader(input_pdf_path)\n",
    "    \n",
    "    for page in template_pdf.pages:\n",
    "        annotations = page[ANNOT_KEY]\n",
    "        for annotation in annotations:\n",
    "            if annotation[SUBTYPE_KEY] == WIDGET_SUBTYPE_KEY:\n",
    "                if annotation[ANNOT_FIELD_KEY]:\n",
    "                    key = annotation[ANNOT_FIELD_KEY][1:-1]\n",
    "                    if key in data_dict.keys():\n",
    "                        if type(data_dict[key]) == bool:\n",
    "                            if data_dict[key] == True:\n",
    "                                annotation.update(pdfrw.PdfDict(\n",
    "                                    AS=pdfrw.PdfName('Yes')))\n",
    "                        else:\n",
    "                            annotation.update(\n",
    "                                pdfrw.PdfDict(V='{}'.format(data_dict[key]))\n",
    "                            )\n",
    "                            annotation.update(pdfrw.PdfDict(AP=''))\n",
    "    template_pdf.Root.AcroForm.update(pdfrw.PdfDict(NeedAppearances=pdfrw.PdfObject('true')))\n",
    "\n",
    "    pdfrw.PdfWriter().write(output_pdf_path, template_pdf)\n",
    "\n",
    "    # return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten PDF function\n",
    "def flatten_pdf(flat_output, img_file):\n",
    "    # Fillable PDF to Image\n",
    "    images = convert_from_path(flat_output, dpi=300, size=(2550,3300))\n",
    "    for i in range(len(images)):\n",
    "   \n",
    "    # Save pages as images in the pdf\n",
    "        images[i].save(img_file + '.png', 'PNG')\n",
    "    \n",
    "    # Delete Fillable PDF\n",
    "    delete_file(img_file + '.pdf')\n",
    "    \n",
    "    # opening image\n",
    "    image_file = Image.open(img_file + '.png')\n",
    "    \n",
    "    # Image to Flat PDF\n",
    "    # define paper size\n",
    "    letter = (img2pdf.in_to_pt(8.5), img2pdf.in_to_pt(11))\n",
    "    layout = img2pdf.get_layout_fun(letter)\n",
    "    # converting into chunks using img2pdf\n",
    "    pdf_bytes = img2pdf.convert(image_file.filename, layout_fun=layout)\n",
    "    \n",
    "    # opening or creating pdf file\n",
    "    flat_pdf = f\"{img_file}.pdf\"\n",
    "    file = open(flat_pdf, \"wb\")\n",
    "    \n",
    "    # writing pdf files with chunks\n",
    "    file.write(pdf_bytes)\n",
    "    \n",
    "    # Add file name to file_name list\n",
    "    # file_list.append(flat_pdf)\n",
    "\n",
    "    # closing image file\n",
    "    image_file.close()\n",
    "    \n",
    "     # Delete Fillable PDF\n",
    "    delete_file(img_file + '.png')\n",
    "\n",
    "    # closing pdf file\n",
    "    file.close()\n",
    "\n",
    "    # return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAP Letter function\n",
    "def gap_letter(template_df, pdf_template, position):\n",
    "    gap_path = 'letters/staging/'\n",
    "\n",
    "    template_df['payment_amount'] = template_df['payment_amount'].map('${:,.2f}'.format)\n",
    "    template_df['loss_date'] = pd.to_datetime(template_df['loss_date']).dt.strftime('%B %d, %Y')\n",
    "    template_df['StateDesc'] = template_df['StateDesc'].astype(str).replace({'None':''})\n",
    "    template_df['StateCode'] = template_df['StateCode'].astype(str).replace({'None':''})\n",
    "    template_df['f_lang'] = template_df['f_lang'].astype(str).replace({'None':''})\n",
    "    letter_date = f\"{datetime.now():%B %d, %Y}\"   \n",
    "\n",
    "    for index, row in template_df.iterrows():\n",
    "\n",
    "        # empty dict\n",
    "        data_dict = {}\n",
    "        # store field data in dictionary\n",
    "        data_dict = {\n",
    "            'Date': letter_date,\n",
    "            'Lender': template_df.loc[index]['alt_name'],\n",
    "            'Contact': template_df.loc[index]['contact'],\n",
    "            'Address': template_df.loc[index]['address1'],\n",
    "            'City_St_Zip': f\"{template_df.loc[index]['city']}, {template_df.loc[index]['state']} {template_df.loc[index]['zip']}\",\n",
    "            'Lender2': template_df.loc[index]['alt_name'],\n",
    "            'Borrower': f\"{template_df.loc[index]['first']} {template_df.loc[index]['last']}\",\n",
    "            'Claim_Nbr': template_df.loc[index]['claim_nbr'],\n",
    "            'Acct_Nbr': template_df.loc[index]['acct_number'],\n",
    "            'DOL': template_df.loc[index]['loss_date'],\n",
    "            'GAP_Amt': template_df.loc[index]['payment_amount'],\n",
    "            'State': template_df.loc[index]['StateDesc'],\n",
    "            'St_Code': template_df.loc[index]['StateCode'],\n",
    "            'Fraud': template_df.loc[index]['f_lang'],\n",
    "        }\n",
    "\n",
    "        # store paths as variables\n",
    "        output_file = f\"{template_df.loc[index]['claim_nbr']}-{position}.pdf\"\n",
    "        output_path_fn = f\"{gap_path}{output_file}\"\n",
    "\n",
    "        fill_pdf(pdf_template, output_path_fn, data_dict)\n",
    "\n",
    "        # Set File Paths\n",
    "        flat_output = f\"{os.path.dirname(os.path.abspath(output_file))}\\{gap_path}\\{output_file}\"\n",
    "        img_file = f\"{os.path.dirname(os.path.abspath(output_file))}\\{gap_path}\\{template_df.loc[index]['claim_nbr']}-{position}\"\n",
    "\n",
    "        # Flatten pdf using flatten_pdf function\n",
    "        flatten_pdf(flat_output, img_file) \n",
    "\n",
    "    # return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAP Calculation function\n",
    "def calculations(template_df, pdf_template, position):\n",
    "    gap_path = 'letters/staging/'\n",
    "\n",
    "    template_df['loss_date'] = pd.to_datetime(template_df['loss_date']).dt.strftime('%B %d, %Y')\n",
    "    template_df['payoff'] = template_df['payoff'].map('${:,.2f}'.format)\n",
    "    template_df['past_due'] = template_df['past_due'].map('${:,.2f}'.format)\n",
    "    template_df['late_fees'] = template_df['late_fees'].map('${:,.2f}'.format)\n",
    "    template_df['skip_pymts'] = template_df['skip_pymts'].map('${:,.2f}'.format)\n",
    "    template_df['skip_fees'] = template_df['skip_fees'].map('${:,.2f}'.format)\n",
    "    template_df['primary_pymt'] = template_df['primary_pymt'].map('${:,.2f}'.format)\n",
    "    template_df['excess_deductible'] = template_df['excess_deductible'].map('${:,.2f}'.format)\n",
    "    template_df['scr'] = template_df['scr'].map('${:,.2f}'.format)\n",
    "    template_df['clr'] = template_df['clr'].map('${:,.2f}'.format)\n",
    "    template_df['cdr'] = template_df['cdr'].map('${:,.2f}'.format)\n",
    "    template_df['oref'] = template_df['oref'].map('${:,.2f}'.format)\n",
    "    template_df['salvage'] = template_df['salvage'].map('${:,.2f}'.format)\n",
    "    template_df['prior_dmg'] = template_df['prior_dmg'].map('${:,.2f}'.format)\n",
    "    template_df['over_ltv'] = template_df['over_ltv'].map('${:,.2f}'.format)\n",
    "    template_df['other1_amt'] = template_df['other1_amt'].map('${:,.2f}'.format)\n",
    "    template_df['other2_amt'] = template_df['other2_amt'].map('${:,.2f}'.format)\n",
    "    template_df['gap_payable'] = template_df['gap_payable'].map('${:,.2f}'.format)\n",
    "    template_df['last_payment'] = pd.to_datetime(template_df['last_payment']).dt.strftime('%B %d, %Y')\n",
    "    template_df['balance_last_pay'] = template_df['balance_last_pay'].map('${:,.2f}'.format)\n",
    "    template_df['per_day'] = template_df['per_day'].map('${:,.2f}'.format)\n",
    "\n",
    "    for index, row in template_df.iterrows():\n",
    "\n",
    "        # empty dict\n",
    "        data_dict = {}\n",
    "        # store field data in dictionary\n",
    "        data_dict = {\n",
    "            'Claim_Nbr': template_df.loc[index]['claim_nbr'],\n",
    "            'Status': 'Paid',\n",
    "            'Borrower': f\"{template_df.loc[index]['first']} {template_df.loc[index]['last']}\",\n",
    "            'Vehicle': template_df.loc[index]['vehicle'],\n",
    "            'Date_Of_Loss': template_df.loc[index]['loss_date'],\n",
    "            'Type_Of_Loss': template_df.loc[index]['loss_type'],\n",
    "            'Lender': template_df.loc[index]['alt_name'],         \n",
    "            'Lender_Contact': template_df.loc[index]['contact'],\n",
    "            'Insurance_Carrier': template_df.loc[index]['carrier'],\n",
    "            'Inception_Date': template_df.loc[index]['incp_date'],\n",
    "            'Deductible': template_df.loc[index]['deductible'],\n",
    "            'Payoff': template_df.loc[index]['payoff'],\n",
    "            'Past_Due': template_df.loc[index]['past_due'],\n",
    "            'Late_Fees': template_df.loc[index]['late_fees'],\n",
    "            'Skips': template_df.loc[index]['skip_pymts'],\n",
    "            'Skip_Fees': template_df.loc[index]['skip_fees'],\n",
    "            'Primary': template_df.loc[index]['primary_pymt'],\n",
    "            'Deductible_Excess': template_df.loc[index]['excess_deductible'],\n",
    "            'SCR': template_df.loc[index]['scr'],\n",
    "            'CL_Refund': template_df.loc[index]['clr'],\n",
    "            'CD_Refund': template_df.loc[index]['cdr'],\n",
    "            'O_Refund': template_df.loc[index]['oref'],\n",
    "            'Salvage': template_df.loc[index]['salvage'],\n",
    "            'Prior_Damage': template_df.loc[index]['prior_dmg'],\n",
    "            'Over_LTV': template_df.loc[index]['over_ltv'],\n",
    "            'Other1_Description': template_df.loc[index]['other1_description'],\n",
    "            'Other2_Description': template_df.loc[index]['other2_description'],\n",
    "            'Other1': template_df.loc[index]['other1_amt'],\n",
    "            'Other2': template_df.loc[index]['other2_amt'],\n",
    "            'Deduction_Subtotal': (template_df.loc[index]['other2_amt'] + template_df.loc[index]['other1_amt'] + template_df.loc[index]['over_ltv'] + template_df.loc[index]['prior_dmg']\n",
    "                + template_df.loc[index]['salvage'] + template_df.loc[index]['oref'] + template_df.loc[index]['cdr'] + template_df.loc[index]['clr'] + template_df.loc[index]['scr']\n",
    "                + template_df.loc[index]['excess_deductible'] + template_df.loc[index]['primary_pymt'] + template_df.loc[index]['skip_fees'] + template_df.loc[index]['skip_pymts']\n",
    "                + template_df.loc[index]['late_fees'] + template_df.loc[index]['past_due']),\n",
    "            'GAP_Amt': template_df.loc[index]['gap_payable'], \n",
    "            'Last_pymt_date': template_df.loc[index]['last_payment'], \n",
    "            'DOL': template_df.loc[index]['loss_date'],\n",
    "            'Number_of_days': template_df.loc[index]['nbr_of_days'], \n",
    "            'Loan_Payoff_As_of_DOL': template_df.loc[index]['payoff'],\n",
    "            'Bal_as_of_last_pymt': template_df.loc[index]['balance_last_pay'],\n",
    "            'Interest_Rate': template_df.loc[index]['interest_rate'],\n",
    "            'Interest_Per_Day': template_df.loc[index]['per_day']\n",
    "        }\n",
    "\n",
    "        # store paths as variables\n",
    "        output_file = f\"{template_df.loc[index]['claim_nbr']}-{position}.pdf\"\n",
    "        output_path_fn = f\"{gap_path}{output_file}\"\n",
    "\n",
    "        fill_pdf(pdf_template, output_path_fn, data_dict)\n",
    "\n",
    "        # Set File Paths\n",
    "        flat_output = f\"{os.path.dirname(os.path.abspath(output_file))}\\{gap_path}\\{output_file}\"\n",
    "        img_file = f\"{os.path.dirname(os.path.abspath(output_file))}\\{gap_path}\\{template_df.loc[index]['claim_nbr']}-{position}\"\n",
    "\n",
    "        # Flatten pdf using flatten_pdf function\n",
    "        flatten_pdf(flat_output, img_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAP Plus function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TotalRestart Letter function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TotalRestart Calculation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to DB\n",
    "cnx = mc.connect(user=vgc_u, password=vgc_pw,\n",
    "                host=vgc_host,\n",
    "                database='visualgap_claims')\n",
    "cursor = cnx.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql query for GAP claims that are RTBP\n",
    "sql_file = '''\n",
    "            SELECT c.claim_id, c.claim_nbr, c.carrier_id, cl.alt_name, cl.dealer_securityId, cl.contact, cl.address1,\n",
    "                cl.city, cl.state, cl.zip, cl.payment_method, cb.first, cb.last, \n",
    "                IF(sq.gap_amt_paid > 0, 2,1) AS pymt_type_id, \n",
    "                IF(sq.gap_amt_paid > 0, ROUND(cc.gap_payable - sq.gap_amt_paid,2), cc.gap_payable) AS gap_due,\n",
    "                COALESCE(NULLIF(cb.acct_number,''),'0') AS acct_nbr, c.loss_date\n",
    "            FROM claims c\n",
    "            INNER JOIN claim_lender cl\n",
    "                USING (claim_id)\n",
    "            INNER JOIN claim_borrower cb\n",
    "                USING (claim_id)\n",
    "            INNER JOIN claim_calculations cc\n",
    "                USING (claim_id)\n",
    "            INNER JOIN claim_status cs\n",
    "                ON (c.status_id = cs.status_id)\n",
    "            LEFT JOIN (SELECT cp.claim_id, SUM(cp.payment_amount) AS gap_amt_paid\n",
    "                    FROM claim_payments cp\n",
    "                    INNER JOIN (SELECT c.claim_id\n",
    "                                FROM claims c\n",
    "                                INNER JOIN claim_status cs\n",
    "                                    ON (c.status_id = cs.status_id)\n",
    "                                WHERE cs.status_desc_id = 8) rtbp_sq\n",
    "                        USING (claim_id)\n",
    "                    WHERE payment_category_id = 1\n",
    "                    GROUP BY cp.claim_id) sq\n",
    "                ON (c.claim_id = sq.claim_id)\n",
    "            WHERE cs.status_desc_id = 8;\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute sql\n",
    "cursor.execute(sql_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save query results as DF\n",
    "df = pd.DataFrame(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column names\n",
    "df_cols = ['claim_id', 'claim_nbr', 'carrier_id', 'lender_name', 'dealer_securityId', 'contact', 'address1', 'city', 'state', 'zip', \n",
    "                            'pymt_method', 'first', 'last', 'pymt_type_id', 'amount', 'acct_number', 'loss_date']\n",
    "\n",
    "df.columns = df_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql query for PLUS claims that are RTBP\n",
    "sql_file_plus = '''\n",
    "                SELECT sqp.claim_id, c.claim_nbr, c.carrier_id, cl.alt_name, cl.dealer_securityId, cl.contact, \n",
    "                    cl.address1, cl.city, cl.state, cl.zip, cl.payment_method, cb.first, cb.last, \n",
    "                    1 AS pymt_type_id, \n",
    "                    IF(cl.customer_securityId = 9401, 1500, 1000) AS gap_plus_due, COALESCE(NULLIF(cb.acct_number,''),'0') AS acct_nbr,\n",
    "                    c.loss_date\n",
    "                FROM claims c\n",
    "                INNER JOIN claim_lender cl\n",
    "                    USING (claim_id)\n",
    "                INNER JOIN claim_borrower cb\n",
    "                    USING (claim_id)\n",
    "                INNER JOIN (SELECT pb.claim_id\n",
    "                            FROM claim_plus_benefit pb\n",
    "                            WHERE status_desc_id = 8) sqp\n",
    "                    USING (claim_id)\n",
    "                INNER JOIN (SELECT c.claim_id\n",
    "                            FROM claims c\n",
    "                            INNER JOIN claim_status cs\n",
    "                                ON (c.status_id = cs.status_id)  \n",
    "                            WHERE cs.status_desc_id = 8\n",
    "                                OR cs.status_desc_id = 4) sqg\n",
    "                    USING (claim_id);\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute sql\n",
    "cursor.execute(sql_file_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save query results as DF\n",
    "df2 = pd.DataFrame(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column names\n",
    "df2_cols = ['claim_id', 'claim_nbr', 'carrier_id', 'lender_name', 'dealer_securityId', 'contact', 'address1', 'city', 'state', 'zip', \n",
    "                            'pymt_method', 'first', 'last', 'pymt_type_id', 'amount', 'acct_number', 'loss_date']\n",
    "\n",
    "df2.columns = df2_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql query for TotalRestart claims that are RTBP\n",
    "sql_file_tr = '''\n",
    "            SELECT sqp.claim_id, c.claim_nbr, c.carrier_id, cl.alt_name, cl.dealer_securityId, cl.contact, cl.address1, \n",
    "            cl.city, cl.state, cl.zip, 'Check' AS payment_method, cb.first, cb.last, 1 AS pymt_type_id, \n",
    "            ctr.totalrestart_payable AS tr_due, COALESCE(NULLIF(cb.acct_number,''),'0') AS acct_nbr, c.loss_date\n",
    "            FROM claims c\n",
    "            INNER JOIN claim_lender cl\n",
    "                USING (claim_id)\n",
    "            INNER JOIN claim_borrower cb\n",
    "                USING (claim_id)\n",
    "            INNER JOIN (SELECT pb.claim_id\n",
    "                        FROM claim_totalrestart pb\n",
    "                        WHERE status_desc_id = 8) sqp\n",
    "                USING (claim_id)\n",
    "            INNER JOIN claim_totalrestart ctr\n",
    "                USING (claim_id)\n",
    "             '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute sql\n",
    "cursor.execute(sql_file_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save query results as DF\n",
    "df3 = pd.DataFrame(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column names\n",
    "df3_cols = ['claim_id', 'claim_nbr', 'carrier_id', 'lender_name', 'dealer_securityId', 'contact', 'address1', 'city', 'state', 'zip', \n",
    "                            'pymt_method', 'first', 'last', 'pymt_type_id', 'amount', 'acct_number', 'loss_date']\n",
    "\n",
    "df3.columns = df3_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close mysql connection\n",
    "cursor.close()\n",
    "cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get expense account name\n",
    "cnx = mc.connect(user=mysql_u, password=mysql_pw,\n",
    "                host=mysql_host,\n",
    "                database='claim_qb_payments')\n",
    "cursor = cnx.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAP\n",
    "# convert columns list to string\n",
    "cols = \", \".join(df_cols)\n",
    "\n",
    "# insert DF into the ready_to_be_paid table of the claim_qb_payments database\n",
    "for x,rows in df.iterrows():\n",
    "\n",
    "    sql_file2 = '''INSERT INTO ready_to_be_paid ({columns}, payment_category_id, check_nbr, batch_id, qb_txnid, toVGC) VALUES ({claim_id}, \"{claim_nbr}\", {carrier_id},\"{lender_name}\", \"{lender_id}\",\"{contact}\", \n",
    "                \"{address1}\", \"{city}\", \"{state}\", \"{zip}\", \"{pymt_method}\", \"{first}\", \"{last}\", {pymt_type_id}, {amount}, \"{acct_nbr}\", \"{loss_date}\", 1, 0, {batchId}, 0, 0);'''.format(columns=cols, \n",
    "                claim_id=rows['claim_id'], claim_nbr=rows['claim_nbr'], carrier_id=rows['carrier_id'], lender_name=rows['lender_name'], lender_id=rows['dealer_securityId'], \n",
    "                contact=rows['contact'], address1=rows['address1'], city=rows['city'], state=rows['state'], zip=rows['zip'], \n",
    "                pymt_method=rows['pymt_method'], first = rows['first'], last = rows['last'], pymt_type_id = rows['pymt_type_id'], \n",
    "                amount = rows['amount'], acct_nbr = rows['acct_number'], loss_date = rows['loss_date'], batchId = batch_id)\n",
    "     \n",
    "    # execute and commit sql\n",
    "    cursor.execute(sql_file2)\n",
    "    cnx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLUS\n",
    "# convert columns list to string\n",
    "cols = \", \".join(df2_cols)\n",
    "\n",
    "# insert DF into the ready_to_be_paid table of the claim_qb_payments database\n",
    "for x,rows in df2.iterrows():\n",
    "    sql_file2_plus = '''INSERT INTO ready_to_be_paid ({columns}, payment_category_id, check_nbr, batch_id, qb_txnid, toVGC) VALUES ({claim_id}, \"{claim_nbr}\", {carrier_id},\"{lender_name}\", \"{lender_id}\",\"{contact}\", \n",
    "                \"{address1}\", \"{city}\", \"{state}\", \"{zip}\", \"{pymt_method}\", \"{first}\", \"{last}\", {pymt_type_id}, {amount}, \"{acct_nbr}\", \"{loss_date}\", 2, 0, {batchId}, 0, 0);'''.format(columns=cols, \n",
    "                claim_id=rows['claim_id'], claim_nbr=rows['claim_nbr'], carrier_id=rows['carrier_id'], lender_name=rows['lender_name'], lender_id=rows['dealer_securityId'], \n",
    "                contact=rows['contact'], address1=rows['address1'], city=rows['city'], state=rows['state'], zip=rows['zip'], \n",
    "                pymt_method=rows['pymt_method'], first = rows['first'], last = rows['last'], pymt_type_id = rows['pymt_type_id'], \n",
    "                amount = rows['amount'], acct_nbr = rows['acct_number'], loss_date = rows['loss_date'], batchId = batch_id)\n",
    "          \n",
    "    # execute and commit sql\n",
    "    cursor.execute(sql_file2_plus)\n",
    "    cnx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOTALRESTART\n",
    "# convert columns list to string\n",
    "cols = \", \".join(df3_cols)\n",
    "\n",
    "# insert DF into the ready_to_be_paid table of the claim_qb_payments database\n",
    "for x,rows in df3.iterrows():\n",
    "    sql_file2_tr = '''INSERT INTO ready_to_be_paid ({columns}, payment_category_id, check_nbr, batch_id, qb_txnid, toVGC) VALUES ({claim_id}, \"{claim_nbr}\", {carrier_id},\"{lender_name}\", \"{lender_id}\",\"{contact}\", \n",
    "                \"{address1}\", \"{city}\", \"{state}\", \"{zip}\", \"{pymt_method}\", \"{first}\", \"{last}\", {pymt_type_id}, {amount}, \"{acct_nbr}\", \"{loss_date}\", 3, 0, {batchId}, 0, 0);'''.format(columns=cols, \n",
    "                claim_id=rows['claim_id'], claim_nbr=rows['claim_nbr'], carrier_id=rows['carrier_id'], lender_name=rows['lender_name'], lender_id=rows['dealer_securityId'], \n",
    "                contact=rows['contact'], address1=rows['address1'], city=rows['city'], state=rows['state'], zip=rows['zip'], \n",
    "                pymt_method=rows['pymt_method'], first = rows['first'], last = rows['last'], pymt_type_id = rows['pymt_type_id'], \n",
    "                amount = rows['amount'], acct_nbr = rows['acct_number'], loss_date = rows['loss_date'], batchId = batch_id)\n",
    "          \n",
    "    # execute and commit sql\n",
    "    cursor.execute(sql_file2_tr)\n",
    "    cnx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create select query to pull current batch with ID\n",
    "sql_file3 = '''SELECT rtbp_id, claim_id, claim_nbr, carrier_id, lender_name, dealer_securityId, contact, address1, city, \n",
    "                     state, zip, pymt_method, first, last, pymt_type_id, amount, payment_category_id, check_nbr, batch_id, \n",
    "                     qb_txnid, acct_number, loss_date\n",
    "              FROM ready_to_be_paid\n",
    "              WHERE batch_id = {batchId}\n",
    "              \n",
    "              ORDER BY payment_category_id, claim_id;'''.format(batchId = batch_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute sql\n",
    "cursor.execute(sql_file3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save query results as DF\n",
    "pymts_df = pd.DataFrame(cursor.fetchall())\n",
    "\n",
    "# add column names\n",
    "pymts_df_cols = ['rtbp_id', 'claim_id', 'claim_nbr', 'carrier_id', 'lender_name', 'dealer_securityId', 'contact', 'address1', 'city', 'state', 'zip', \n",
    "                    'pymt_method', 'first', 'last', 'pymt_type_id', 'amount','payment_category_id', 'check_nbr', 'batch_id', 'qb_txnid', \n",
    "                    'acct_number', 'loss_date']\n",
    "\n",
    "pymts_df.columns = pymts_df_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql query for expense accounts in DB\n",
    "sql_file4 = '''\n",
    "    SELECT carrier_id, qb_fullname\n",
    "    FROM qb_accounts\n",
    "    WHERE account_type = 'Expense';\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute sql\n",
    "cursor.execute(sql_file4)\n",
    "# save query results as DF\n",
    "expense_df = pd.DataFrame(cursor.fetchall())\n",
    "# add column names to DF\n",
    "col_names = ['carrier_id', 'expense']\n",
    "expense_df.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql query for checking accounts in DB\n",
    "sql_file5 = '''\n",
    "    SELECT carrier_id, qb_fullname\n",
    "    FROM qb_accounts\n",
    "    WHERE account_type = 'Checking';\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute sql\n",
    "cursor.execute(sql_file5)\n",
    "# save query results as DF\n",
    "checking_df = pd.DataFrame(cursor.fetchall())\n",
    "# add column names to DF\n",
    "col_names = ['carrier_id', 'checking']\n",
    "checking_df.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge expense account name into df\n",
    "pymts_df = pymts_df.merge(expense_df, left_on='carrier_id', right_on='carrier_id').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge checking account name into df\n",
    "pymts_df = pymts_df.merge(checking_df, left_on='carrier_id', right_on='carrier_id').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sql server connection\n",
    "cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+svr+';DATABASE='+db+';UID='+sql_u+';PWD='+ sql_pw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create query\n",
    "sql_svr_file = '''SELECT VGSecurityId, QB_ListID\n",
    "                  FROM business_entity\n",
    "                  WHERE QB_ListID IS NOT NULL;\n",
    "               '''\n",
    "# execute query\n",
    "listid_df = pd.read_sql(sql_svr_file, cnxn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY ########################################################################################\n",
    "# Convert test dealer_securityId to Production dealer_securityId\n",
    "pymts_df['dealer_securityId'].replace({22260:46724,21945:52715}, inplace=True)\n",
    "# TEMPORARY ########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOTALRESTART #####################################################################################\n",
    "# Update TR claims with the correct 'checking' & 'expense' names and possibly QB_ListID\n",
    "# maybe change carrier to pull correct 'checking' & 'expense' names and possibly QB_ListID??\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge QB_ListID into df\n",
    "pymts_df = pymts_df.merge(listid_df, left_on='dealer_securityId', right_on='VGSecurityId').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add carrier name\n",
    "cnx_c = mc.connect(user=vgc_u, password=vgc_pw,\n",
    "                host=vgc_host,\n",
    "                database='visualgap_claims')\n",
    "cursor_c = cnx_c.cursor()\n",
    "\n",
    "sql_file6 = '''\n",
    "    SELECT carrier_id, description\n",
    "    FROM carriers;\n",
    "    '''\n",
    "cursor_c.execute(sql_file6)\n",
    "\n",
    "carrier_df = pd.DataFrame(cursor_c.fetchall())\n",
    "\n",
    "col_names = ['carrier_id', 'carrier']\n",
    "carrier_df.columns = col_names\n",
    "\n",
    "cursor_c.close()\n",
    "cnx_c.close()\n",
    "\n",
    "# Merge QB_ListID into df\n",
    "pymts_df = pymts_df.merge(carrier_df, left_on='carrier_id', right_on='carrier_id').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY ########################################################################################\n",
    "# temp Check order (claim_id, payment_category_id)\n",
    "# pymts_df\n",
    "# TEMPORARY ########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # payments greater than 0 df\n",
    "# qb_pymts_df = pymts_df.loc[pymts_df['amount'] > 0]\n",
    "# qb_pymts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Connect to Quickbooks\n",
    "# sessionManager = wc.Dispatch(\"QBXMLRP2.RequestProcessor\")    \n",
    "# sessionManager.OpenConnection('', 'Test qbXML Request')\n",
    "# ticket = sessionManager.BeginSession(\"\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create qbxml query to add payments to QB\n",
    "# pay_date = f\"{dt.date.today():%Y-%m-%d}\"\n",
    "\n",
    "# for index, row in pymts_df.iterrows():\n",
    "#     if row['payment_category_id'] == 1:\n",
    "#         if row['pymt_type_id'] == 2:\n",
    "#             pymt_type = 'Additional GAP Claim Pymt'\n",
    "#         else: \n",
    "#             pymt_type = 'GAP Claim'\n",
    "\n",
    "#     elif row['payment_category_id'] == 2:\n",
    "#         if row['pymt_type_id'] == 2:\n",
    "#             pymt_type = 'Additional GAP Plus Pymt'\n",
    "#         else: \n",
    "#             pymt_type = 'GAP Plus'\n",
    "            \n",
    "#     elif row['payment_category_id'] == 3:\n",
    "#         if row['pymt_type_id'] == 2:\n",
    "#             pymt_type = 'Additional TotalRestart Pymt'\n",
    "#         else: \n",
    "#             pymt_type = 'TotalRestart'\n",
    "\n",
    "#     pymtAmt = \"{:.2f}\".format(row['amount'])\n",
    "\n",
    "#     if row['pymt_method'] == 'Check':\n",
    "#         qbxmlQuery = '''\n",
    "#         <?qbxml version=\"14.0\"?>\n",
    "#         <QBXML>\n",
    "#             <QBXMLMsgsRq onError=\"stopOnError\">\n",
    "#                 <CheckAddRq>\n",
    "#                     <CheckAdd>\n",
    "#                         <AccountRef>\n",
    "#                             <FullName>{checking}</FullName>\n",
    "#                         </AccountRef>\n",
    "#                         <PayeeEntityRef>\n",
    "#                             <ListID>{lender_qbid}</ListID>\n",
    "#                         </PayeeEntityRef>\n",
    "#                         <TxnDate>{date}</TxnDate>\n",
    "#                         <Memo>{memo}</Memo>\n",
    "#                         <Address>\n",
    "#                             <Addr1>{lender}</Addr1>\n",
    "#                             <Addr2>{contact}</Addr2>\n",
    "#                             <Addr3>{address}</Addr3>\n",
    "#                             <City>{city}</City>\n",
    "#                             <State>{state}</State>\n",
    "#                             <PostalCode>{zip}</PostalCode>\n",
    "#                         </Address>\n",
    "#                         <IsToBePrinted>true</IsToBePrinted>\n",
    "#                         <ExpenseLineAdd>\n",
    "#                             <AccountRef>\n",
    "#                                 <FullName>{expense}</FullName>\n",
    "#                             </AccountRef>\n",
    "#                             <Amount>{amount}</Amount>\n",
    "#                             <Memo>{memo}</Memo>\n",
    "#                         </ExpenseLineAdd>\n",
    "#                     </CheckAdd>\n",
    "#                 </CheckAddRq>\n",
    "#             </QBXMLMsgsRq>\n",
    "#         </QBXML>'''.format(checking=row['checking'], lender_qbid=row['QB_ListID'], lender=row['lender_name'], date=pay_date, \n",
    "#                 memo=f\"{row['last']}/{row['first']} {pymt_type}\", contact=row['contact'], address=row['address1'], city=row['city'], state=row['state'],\n",
    "#                 zip=row['zip'], expense=row['expense'], amount = pymtAmt)\n",
    "\n",
    "#     elif 'ACH' in row['pymt_method']:\n",
    "#         qbxmlQuery = '''\n",
    "#         <?qbxml version=\"14.0\"?>\n",
    "#         <QBXML>\n",
    "#             <QBXMLMsgsRq onError=\"stopOnError\">\n",
    "#                 <CheckAddRq>\n",
    "#                     <CheckAdd>\n",
    "#                         <AccountRef>\n",
    "#                             <FullName>{checking}</FullName>\n",
    "#                         </AccountRef>\n",
    "#                         <PayeeEntityRef>\n",
    "#                             <ListID>{lender_qbid}</ListID>\n",
    "#                         </PayeeEntityRef>\n",
    "#                         <RefNumber>ACH</RefNumber>\n",
    "#                         <TxnDate>{date}</TxnDate>\n",
    "#                         <Memo>{memo}</Memo>\n",
    "#                         <Address>\n",
    "#                             <Addr1>{lender}</Addr1>\n",
    "#                             <Addr2>{contact}</Addr2>\n",
    "#                             <Addr3>{address}</Addr3>\n",
    "#                             <City>{city}</City>\n",
    "#                             <State>{state}</State>\n",
    "#                             <PostalCode>{zip}</PostalCode>\n",
    "#                         </Address>\n",
    "#                         <IsToBePrinted>false</IsToBePrinted>\n",
    "#                         <ExpenseLineAdd>\n",
    "#                             <AccountRef>\n",
    "#                                 <FullName>{expense}</FullName>\n",
    "#                             </AccountRef>\n",
    "#                             <Amount>{amount}</Amount>\n",
    "#                             <Memo>{memo}</Memo>\n",
    "#                         </ExpenseLineAdd>\n",
    "#                     </CheckAdd>\n",
    "#                 </CheckAddRq>            \n",
    "#             </QBXMLMsgsRq>\n",
    "#         </QBXML>'''.format(checking=row['checking'], lender_qbid=row['QB_ListID'], lender=row['lender_name'], date=pay_date, \n",
    "#                 memo=f\"{row['last']}/{row['first']} {pymt_type}\", contact=row['contact'], address=row['address1'], city=row['city'], state=row['state'],\n",
    "#                 zip=row['zip'], expense=row['expense'], amount = pymtAmt)\n",
    "        \n",
    "#     # Send query and receive response\n",
    "#     responseString = sessionManager.ProcessRequest(ticket, qbxmlQuery)\n",
    "\n",
    "#     # output TxnID\n",
    "#     QBXML = ET.fromstring(responseString)\n",
    "#     QBXMLMsgsRs = QBXML.find('QBXMLMsgsRs')\n",
    "#     checkResults = QBXMLMsgsRs.iter(\"CheckRet\")\n",
    "#     txnId = 0\n",
    "#     for checkResult in checkResults:\n",
    "#         txnId = checkResult.find('TxnID').text\n",
    "\n",
    "#     # Add TxnID to ready_to_be_paid table\n",
    "#     sql_file6 = '''UPDATE ready_to_be_paid\n",
    "#                    SET qb_txnid = '{TxnID}'\n",
    "#                    WHERE rtbp_id = {rowID};'''.format(TxnID=txnId, rowID=row['rtbp_id'])\n",
    "    \n",
    "#     # execute and commit sql\n",
    "#     cursor.execute(sql_file6)\n",
    "#     cnx.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Disconnect from Quickbooks\n",
    "# sessionManager.EndSession(ticket)\n",
    "# sessionManager.CloseConnection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close mysql connection\n",
    "cursor.close()\n",
    "cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and print claim letter and calculation\n",
    "# Import functions to create letters [payment_category_id = GAP (1), PLUS(2), TOTALRESTART (3)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Fraud Language fields to pymts_df\n",
    "cnx_f = mc.connect(user=vgc_u, password=vgc_pw,\n",
    "                host=vgc_host,\n",
    "                database='visualgap_claims')\n",
    "cursor_f = cnx_f.cursor()\n",
    "\n",
    "sql_file7 = '''\n",
    "    SELECT StateId, \n",
    "    StateDesc,\n",
    "    StateCode,\n",
    "    CAST(Language AS CHAR(1000) CHARACTER SET utf8)\n",
    "    FROM FraudLang;\n",
    "    '''\n",
    "cursor_f.execute(sql_file7)\n",
    "\n",
    "fraud_lang_df = pd.DataFrame(cursor_f.fetchall())\n",
    "\n",
    "col_names = ['StateId', 'StateDesc', 'StateCode', 'f_lang']\n",
    "fraud_lang_df.columns = col_names\n",
    "\n",
    "cursor_f.close()\n",
    "cnx_f.close()\n",
    "\n",
    "# Merge QB_ListID into df\n",
    "pymts_df = pymts_df.merge(fraud_lang_df, left_on='state', right_on='StateId').copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create GAP Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbehler\\Anaconda3\\envs\\pyfrost32-dev\\lib\\site-packages\\pandas\\core\\frame.py:5039: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n"
     ]
    }
   ],
   "source": [
    "# Collect GAP payments\n",
    "gap_pymts_df = pymts_df.loc[pymts_df['payment_category_id'] == 1]\n",
    "gap_pymts_df.rename(columns = {'amount':'payment_amount', 'lender_name':'alt_name'}, inplace = True)\n",
    "# gap_pymts_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_letters_df = gap_pymts_df.loc[gap_pymts_df['payment_amount'] > 0]\n",
    "# gap_letters_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove files from staging directory\n",
    "file_staging_dir = './letters/staging/'\n",
    "for x in os.listdir(file_staging_dir):\n",
    "    if x.endswith(\".pdf\"):\n",
    "        os.remove(os.path.join(file_staging_dir, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rtbp_id</th>\n",
       "      <th>claim_id</th>\n",
       "      <th>claim_nbr</th>\n",
       "      <th>carrier_id</th>\n",
       "      <th>alt_name</th>\n",
       "      <th>dealer_securityId</th>\n",
       "      <th>contact</th>\n",
       "      <th>address1</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>pymt_method</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>pymt_type_id</th>\n",
       "      <th>payment_amount</th>\n",
       "      <th>payment_category_id</th>\n",
       "      <th>check_nbr</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>qb_txnid</th>\n",
       "      <th>acct_number</th>\n",
       "      <th>loss_date</th>\n",
       "      <th>expense</th>\n",
       "      <th>checking</th>\n",
       "      <th>VGSecurityId</th>\n",
       "      <th>QB_ListID</th>\n",
       "      <th>carrier</th>\n",
       "      <th>StateId</th>\n",
       "      <th>StateDesc</th>\n",
       "      <th>StateCode</th>\n",
       "      <th>f_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>746</td>\n",
       "      <td>1477</td>\n",
       "      <td>202110192499</td>\n",
       "      <td>9</td>\n",
       "      <td>City Credit Union</td>\n",
       "      <td>52715</td>\n",
       "      <td>Text Contact</td>\n",
       "      <td>7474 FERGUSON RD</td>\n",
       "      <td>DALLAS</td>\n",
       "      <td>TX</td>\n",
       "      <td>75228</td>\n",
       "      <td>Check</td>\n",
       "      <td>BRIAN</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>2</td>\n",
       "      <td>191.17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20211229164606</td>\n",
       "      <td>0</td>\n",
       "      <td>250505961</td>\n",
       "      <td>2021-08-20</td>\n",
       "      <td>GAP Claims Advance-ANICO</td>\n",
       "      <td>Claims ANICO (Fifth Third)</td>\n",
       "      <td>52715</td>\n",
       "      <td>80000002-1628088752</td>\n",
       "      <td>ANICO</td>\n",
       "      <td>TX</td>\n",
       "      <td>Texas</td>\n",
       "      <td>V.T.C.A. Ins. Code s. 704.00</td>\n",
       "      <td>Any person who knowingly presents a false or f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rtbp_id  claim_id     claim_nbr  carrier_id           alt_name  \\\n",
       "2      746      1477  202110192499           9  City Credit Union   \n",
       "\n",
       "   dealer_securityId       contact          address1    city state    zip  \\\n",
       "2              52715  Text Contact  7474 FERGUSON RD  DALLAS    TX  75228   \n",
       "\n",
       "  pymt_method   first      last  pymt_type_id  payment_amount  \\\n",
       "2       Check  BRIAN   CHANDLER             2          191.17   \n",
       "\n",
       "   payment_category_id check_nbr        batch_id qb_txnid acct_number  \\\n",
       "2                    1         0  20211229164606        0   250505961   \n",
       "\n",
       "    loss_date                   expense                    checking  \\\n",
       "2  2021-08-20  GAP Claims Advance-ANICO  Claims ANICO (Fifth Third)   \n",
       "\n",
       "   VGSecurityId            QB_ListID carrier StateId StateDesc  \\\n",
       "2         52715  80000002-1628088752   ANICO      TX     Texas   \n",
       "\n",
       "                      StateCode  \\\n",
       "2  V.T.C.A. Ins. Code s. 704.00   \n",
       "\n",
       "                                              f_lang  \n",
       "2  Any person who knowingly presents a false or f...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_checks_df = gap_letters_df[gap_letters_df['pymt_method'] == 'Check']\n",
    "temp_df = temp_checks_df[(temp_checks_df['pymt_type_id'] == 2) & (temp_checks_df['carrier'] == 'ANICO')]\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(FUTURE) email letters if 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_nbr</th>\n",
       "      <th>loss_date</th>\n",
       "      <th>alt_name</th>\n",
       "      <th>contact</th>\n",
       "      <th>address1</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>acct_number</th>\n",
       "      <th>payment_amount</th>\n",
       "      <th>StateDesc</th>\n",
       "      <th>StateCode</th>\n",
       "      <th>f_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202110192499</td>\n",
       "      <td>2021-08-20</td>\n",
       "      <td>City Credit Union</td>\n",
       "      <td>Text Contact</td>\n",
       "      <td>7474 FERGUSON RD</td>\n",
       "      <td>DALLAS</td>\n",
       "      <td>TX</td>\n",
       "      <td>75228</td>\n",
       "      <td>BRIAN</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>250505961</td>\n",
       "      <td>191.17</td>\n",
       "      <td>Texas</td>\n",
       "      <td>V.T.C.A. Ins. Code s. 704.00</td>\n",
       "      <td>Any person who knowingly presents a false or f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      claim_nbr   loss_date           alt_name       contact  \\\n",
       "2  202110192499  2021-08-20  City Credit Union  Text Contact   \n",
       "\n",
       "           address1    city state    zip   first      last acct_number  \\\n",
       "2  7474 FERGUSON RD  DALLAS    TX  75228  BRIAN   CHANDLER   250505961   \n",
       "\n",
       "   payment_amount StateDesc                     StateCode  \\\n",
       "2          191.17     Texas  V.T.C.A. Ins. Code s. 704.00   \n",
       "\n",
       "                                              f_lang  \n",
       "2  Any person who knowingly presents a false or f...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_cols = ['claim_nbr', 'loss_date', 'alt_name', 'contact', 'address1', 'city', 'state', 'zip', 'first', 'last', 'acct_number', 'payment_amount', 'StateDesc', 'StateCode', 'f_lang' ]\n",
    "temp_gap_df = temp_df[letter_cols]\n",
    "temp_gap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_16996/1693605427.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['payment_amount'] = template_df['payment_amount'].map('${:,.2f}'.format)\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_16996/1693605427.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['loss_date'] = pd.to_datetime(template_df['loss_date']).dt.strftime('%B %d, %Y')\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_16996/1693605427.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['StateDesc'] = template_df['StateDesc'].astype(str).replace({'None':''})\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_16996/1693605427.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['StateCode'] = template_df['StateCode'].astype(str).replace({'None':''})\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_16996/1693605427.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['f_lang'] = template_df['f_lang'].astype(str).replace({'None':''})\n"
     ]
    }
   ],
   "source": [
    "# create letters for amounts greater than $0 paid via check\n",
    "\n",
    "# Filter gap_letters_df for pymt_method 'Check'\n",
    "checks_df = gap_letters_df.loc[gap_letters_df['pymt_method'] == 'Check']\n",
    "\n",
    "if len(checks_df.index) > 0:\n",
    "    # Create list of Carriers on checks_df\n",
    "    carriers = []\n",
    "    carriers = checks_df.carrier.unique()\n",
    "    letter_cols = ['claim_nbr', 'loss_date', 'alt_name', 'contact', 'address1', 'city', 'state', 'zip', 'first', 'last', 'acct_number', 'payment_amount', 'StateDesc', 'StateCode', 'f_lang' ]\n",
    "\n",
    "    # Loop through carriers list\n",
    "    for carrier in carriers:\n",
    "\n",
    "        # Variable Defaults\n",
    "        sql_where_cal = ''\n",
    "        g_letters = True\n",
    "        s_letters = True\n",
    "\n",
    "        # check for payment_type_id for GAP letters by carrier\n",
    "        g_letters_df = checks_df.loc[(checks_df['pymt_type_id'] == 1) & (checks_df['carrier'] == carrier)]\n",
    "\n",
    "        # # GAP Letter - create WHERE statement\n",
    "        if len(g_letters_df.index) > 0:\n",
    "            g_letters = True\n",
    "        else:\n",
    "            # No Letters\n",
    "            g_letters = False\n",
    "\n",
    "        # check for payment_type_id for Supplemental letters\n",
    "        s_letters_df = checks_df.loc[(checks_df['pymt_type_id'] == 2) & (checks_df['carrier'] == carrier)]\n",
    "\n",
    "        # Supplemental Letter\n",
    "        if len(s_letters_df.index) > 0:\n",
    "            s_letters = True\n",
    "        else:\n",
    "            # No Letters\n",
    "            s_letters = False\n",
    "\n",
    "        # Calculations\n",
    "        if len(checks_df.index) > 1:\n",
    "            # Multiple Calculation\n",
    "            for index, row in checks_df.iterrows():\n",
    "                claimID  = row['claim_id']  \n",
    "                if sql_where_cal == '':\n",
    "                    sql_where_cal = f\"c.claim_id = {claimID}\"\n",
    "                    # sql_where_cal = f\"{pymt_type}c.claim_id = {claimID}\"\n",
    "                else:\n",
    "                    sql_where_cal = sql_where_cal + f\" OR c.claim_id = {claimID}\"\n",
    "            else:\n",
    "                # Single Calculation\n",
    "                for index, row in checks_df.iterrows():\n",
    "                    claimID  = row['claim_id']    \n",
    "                    sql_where_cal = f\"c.claim_id = {claimID}\"\n",
    "                    # sql_where_cal = f\"{pymt_type}c.claim_id = {claimID}\"\n",
    "\n",
    "        # Connect to SQL DB\n",
    "        cnx = mc.connect(user=vgc_u, password=vgc_pw,\n",
    "                        host=vgc_host,\n",
    "                        database='visualgap_claims')\n",
    "        cursor = cnx.cursor()\n",
    "\n",
    "        # GAP - Create letters\n",
    "        if g_letters == True:\n",
    "            # create df for template\n",
    "            g_template_df = g_letters_df[letter_cols]\n",
    "\n",
    "            # Create GAP Letters\n",
    "            g_pdf_template = \"letters/pdf_templates/GAP_letter_template.pdf\"\n",
    "            position = 1\n",
    "            gap_letter(g_template_df, g_pdf_template, position)\n",
    "        \n",
    "        # Supplemental - Create letters\n",
    "        if s_letters == True:\n",
    "            # create df for template\n",
    "            s_template_df = s_letters_df[letter_cols]\n",
    "            \n",
    "            # Create Supplement Letters\n",
    "            s_pdf_template = \"letters/pdf_templates/Supp_letter_template.pdf\"\n",
    "            position = 2\n",
    "            gap_letter(s_template_df, s_pdf_template, position)\n",
    "\n",
    "        # Calculations - Create SQL query, run query, create calculation sheets\n",
    "        if g_letters == True or s_letters == True:\n",
    "\n",
    "            # create sql for calculation\n",
    "            c_sql_query = '''\n",
    "                    SELECT c.claim_nbr, c.loss_date, l.alt_name, l.contact, b.first, b.last, cc.gap_payable,\n",
    "                        cl.incp_date, cl.last_payment, cl.interest_rate, cl.amount AS Amt_Fin, cc.balance_last_pay,\n",
    "                        cc.nbr_of_days, cc.per_day, ROUND(cc.payoff,2) AS payoff, cc.ltv, cc.covered_fin_amount,\n",
    "                        cc.percent_uncovered, cc.ltv_limit, v.nada_value, CONCAT(v.year, ' ', v.make, ' ', v.model) AS vehicle,\n",
    "                        v.deductible, cls.description AS loss_type, cc26.description AS primary_carrier,\n",
    "                        cc14.amount AS past_due, cc15.amount AS late_fees, cc16.amount AS skip_pymts,\n",
    "                        cc17.amount AS skip_fees, cc5.amount AS primary_pymt, cc7.amount AS excess_deductible,\n",
    "                        cc8.amount AS scr, cc9.amount AS clr, cc10.amount AS cdr, cc11.amount AS oref,\n",
    "                        cc18.amount AS salvage, cc19.amount AS prior_dmg, cc20.amount AS over_ltv,\n",
    "                        cc21.amount AS other1_amt, cc21.description AS other1_description,\n",
    "                        cc22.amount AS other2_amt, cc22.description AS other2_description, ca.description AS carrier\n",
    "                    FROM claims c\n",
    "                    INNER JOIN claim_lender l\n",
    "                        USING (claim_id)\n",
    "                    INNER JOIN claim_borrower b\n",
    "                        USING (claim_id)\n",
    "                    INNER JOIN claim_loan cl\n",
    "                        USING (claim_id)\n",
    "                    INNER JOIN claim_calculations cc\n",
    "                        USING (claim_id)\n",
    "                    INNER JOIN claim_vehicle v\n",
    "                        USING (claim_id)\n",
    "                    INNER JOIN claims_loss_type cls\n",
    "                        USING (loss_type_id)\n",
    "                    INNER JOIN carriers ca\n",
    "                        ON c.carrier_id = ca.carrier_id                        \n",
    "                    INNER JOIN claim_checklist AS cc26\n",
    "                        ON c.claim_id = cc26.claim_id\n",
    "                        AND cc26.checklist_item_id = 26    \n",
    "                    INNER JOIN claim_checklist AS cc14\n",
    "                        ON c.claim_id = cc14.claim_id\n",
    "                        AND cc14.checklist_item_id = 14    \n",
    "                    INNER JOIN claim_checklist AS cc15\n",
    "                        ON c.claim_id = cc15.claim_id\n",
    "                        AND cc15.checklist_item_id = 15    \n",
    "                    INNER JOIN claim_checklist AS cc16\n",
    "                        ON c.claim_id = cc16.claim_id\n",
    "                        AND cc16.checklist_item_id = 16       \n",
    "                    INNER JOIN claim_checklist AS cc17\n",
    "                        ON c.claim_id = cc17.claim_id\n",
    "                        AND cc17.checklist_item_id = 17       \n",
    "                    INNER JOIN claim_checklist AS cc5\n",
    "                        ON c.claim_id = cc5.claim_id\n",
    "                        AND cc5.checklist_item_id = 5\n",
    "                    INNER JOIN claim_checklist AS cc7\n",
    "                        ON c.claim_id = cc7.claim_id\n",
    "                        AND cc7.checklist_item_id = 7\n",
    "                    INNER JOIN claim_checklist AS cc8\n",
    "                        ON c.claim_id = cc8.claim_id\n",
    "                        AND cc8.checklist_item_id = 8\n",
    "                    INNER JOIN claim_checklist AS cc9\n",
    "                        ON c.claim_id = cc9.claim_id\n",
    "                        AND cc9.checklist_item_id = 9\n",
    "                    INNER JOIN claim_checklist AS cc10\n",
    "                        ON c.claim_id = cc10.claim_id\n",
    "                        AND cc10.checklist_item_id = 10\n",
    "                    INNER JOIN claim_checklist AS cc11\n",
    "                        ON c.claim_id = cc11.claim_id\n",
    "                        AND cc11.checklist_item_id = 11      \n",
    "                    INNER JOIN claim_checklist AS cc18\n",
    "                        ON c.claim_id = cc18.claim_id\n",
    "                        AND cc18.checklist_item_id = 18\n",
    "                    INNER JOIN claim_checklist AS cc19\n",
    "                        ON c.claim_id = cc19.claim_id\n",
    "                        AND cc19.checklist_item_id = 19\n",
    "                    INNER JOIN claim_checklist AS cc20\n",
    "                        ON c.claim_id = cc20.claim_id\n",
    "                        AND cc20.checklist_item_id = 20     \n",
    "                    INNER JOIN claim_checklist AS cc21\n",
    "                        ON c.claim_id = cc21.claim_id\n",
    "                        AND cc21.checklist_item_id = 21\n",
    "                    INNER JOIN claim_checklist AS cc22\n",
    "                        ON c.claim_id = cc22.claim_id\n",
    "                        AND cc22.checklist_item_id = 22  \n",
    "                    WHERE {sqlWhere};'''.format(sqlWhere = sql_where_cal)\n",
    "\n",
    "            cursor.execute(c_sql_query)\n",
    "            # save query results as DF\n",
    "            c_template_df = pd.DataFrame(cursor.fetchall())\n",
    "            # add column names to DF\n",
    "            num_cols = len(cursor.description)\n",
    "            col_names = [i[0] for i in cursor.description]\n",
    "            c_template_df.columns = col_names\n",
    "\n",
    "            # Create Calculations\n",
    "            c_pdf_template = \"letters/pdf_templates/GAP_calculation_template.pdf\"\n",
    "            position = 3\n",
    "            calculations(c_template_df, c_pdf_template, position)\n",
    "                \n",
    "        # Close SQL Connection\n",
    "        cursor.close()\n",
    "        cnx.close()\n",
    "\n",
    "        # Pull file list from directory\n",
    "        file_list = []\n",
    "        for x in os.listdir(file_staging_dir):\n",
    "            if x.endswith(\".pdf\"):\n",
    "                file_list.append(x)\n",
    "\n",
    "        # sort list to collate pages\n",
    "        file_list.sort()\n",
    "\n",
    "        # Concatenated output file\n",
    "        outfn = f'S:/claims/letters/{carrier}_{now.strftime(\"%Y-%m-%d\")}_GAP.pdf'\n",
    "\n",
    "        ConCat_pdf(file_list, outfn)\n",
    "\n",
    "        # Remove files from staging directory\n",
    "        file_staging_dir = './letters/staging/'\n",
    "        for x in os.listdir(file_staging_dir):\n",
    "            if x.endswith(\".pdf\"):\n",
    "                os.remove(os.path.join(file_staging_dir, x))\n",
    "\n",
    "else: print('No amount greater then 0.')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25bbdcee2c7b2635f2af9203b4c23ac3a2ece7544aac2f4db010efbeb60ffe95"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 32-bit ('pyfrost32-dev': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
