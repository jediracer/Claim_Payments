{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import win32com.client as wc\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import mysql.connector as mc\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import pyodbc\n",
    "from pdfrw import PdfReader, PdfWriter\n",
    "import pdfrw \n",
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "import img2pdf\n",
    "from PIL import Image\n",
    "import smtplib\n",
    "import ssl\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.application import MIMEApplication\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "# Get db credentials\n",
    "from configTest import mysql_host, mysql_u, mysql_pw, vgc_host, vgc_u, vgc_pw, svr, db, sql_u, sql_pw, smtp_host, e_user, e_pw, port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all columns in DFs\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batch_id\n",
    "now = datetime.now()\n",
    "batch_id = now.strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySQL\n",
    "def mysql_q (u, p, h, db, sql, cols, commit):\n",
    "    # cols (0 = no, 1 = yes)\n",
    "    # commit (select = 0, insert/update = 1)\n",
    "\n",
    "    # connect to claim_qb_payments db\n",
    "    cnx = mc.connect(user=u, password=p,\n",
    "                    host=h,\n",
    "                    database=db)\n",
    "    cursor = cnx.cursor()\n",
    "\n",
    "    # commit?\n",
    "    if commit == 1:\n",
    "        cursor.execute(sql)\n",
    "        cnx.commit()\n",
    "        sql_result = 0     \n",
    "    else:\n",
    "        cursor.execute(sql)\n",
    "        sql_result = cursor.fetchall()\n",
    "\n",
    "    # columns ?\n",
    "    if cols == 1:\n",
    "        columns=list([x[0] for x in cursor.description])\n",
    "        # close connection\n",
    "        cursor.close()\n",
    "        cnx.close()\n",
    "        # return query result [0] and columns [1]\n",
    "        return sql_result\n",
    "    else:\n",
    "        # close connection\n",
    "        cursor.close()\n",
    "        cnx.close()\n",
    "        # return query result\n",
    "        return sql_result      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Concatenation Function\n",
    "def ConCat_pdf (file_list, outfn):\n",
    "    letter_path = './letters/staging/'\n",
    "    writer = PdfWriter()\n",
    "    for inputfn in file_list:\n",
    "        writer.addpages(PdfReader(letter_path + inputfn).pages)\n",
    "\n",
    "    outfile = outfn + '.pdf'\n",
    "    fnNum = 0\n",
    "\n",
    "    while (os.path.isfile(outfile) == True):\n",
    "        fnNum += 1\n",
    "        outfile = outfn + '-' + str(fnNum) + '.pdf'\n",
    "\n",
    "    writer.write(outfile)\n",
    "    return outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Function\n",
    "def delete_file(del_file_path):\n",
    "    if os.path.exists(del_file_path):\n",
    "        os.remove(del_file_path)\n",
    "    else: print (f\"{del_file_path} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PDF Function\n",
    "def fill_pdf(input_pdf_path, output_pdf_path, data_dict):\n",
    "    ANNOT_KEY = '/Annots'\n",
    "    ANNOT_FIELD_KEY = '/T'\n",
    "    ANNOT_VAL_KEY = '/V'\n",
    "    ANNOT_RECT_KEY = '/Rect'\n",
    "    SUBTYPE_KEY = '/Subtype'\n",
    "    WIDGET_SUBTYPE_KEY = '/Widget'\n",
    "\n",
    "    template_pdf = pdfrw.PdfReader(input_pdf_path)\n",
    "    \n",
    "    for page in template_pdf.pages:\n",
    "        annotations = page[ANNOT_KEY]\n",
    "        for annotation in annotations:\n",
    "            if annotation[SUBTYPE_KEY] == WIDGET_SUBTYPE_KEY:\n",
    "                if annotation[ANNOT_FIELD_KEY]:\n",
    "                    key = annotation[ANNOT_FIELD_KEY][1:-1]\n",
    "                    if key in data_dict.keys():\n",
    "                        if type(data_dict[key]) == bool:\n",
    "                            if data_dict[key] == True:\n",
    "                                annotation.update(pdfrw.PdfDict(\n",
    "                                    AS=pdfrw.PdfName('Yes')))\n",
    "                        else:\n",
    "                            annotation.update(\n",
    "                                pdfrw.PdfDict(V='{}'.format(data_dict[key]))\n",
    "                            )\n",
    "                            annotation.update(pdfrw.PdfDict(AP=''))\n",
    "    template_pdf.Root.AcroForm.update(pdfrw.PdfDict(NeedAppearances=pdfrw.PdfObject('true')))\n",
    "\n",
    "    pdfrw.PdfWriter().write(output_pdf_path, template_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten PDF function\n",
    "def flatten_pdf(flat_output, img_file):\n",
    "    # Fillable PDF to Image\n",
    "    images = convert_from_path(flat_output, dpi=300, size=(2550,3300))\n",
    "    for i in range(len(images)):\n",
    "   \n",
    "    # Save pages as images in the pdf\n",
    "        images[i].save(img_file + '.png', 'PNG')\n",
    "    \n",
    "    # Delete Fillable PDF\n",
    "    delete_file(img_file + '.pdf')\n",
    "    \n",
    "    # opening image\n",
    "    image_file = Image.open(img_file + '.png')\n",
    "    \n",
    "    # Image to Flat PDF\n",
    "    # define paper size\n",
    "    letter = (img2pdf.in_to_pt(8.5), img2pdf.in_to_pt(11))\n",
    "    layout = img2pdf.get_layout_fun(letter)\n",
    "    # converting into chunks using img2pdf\n",
    "    pdf_bytes = img2pdf.convert(image_file.filename, layout_fun=layout)\n",
    "    \n",
    "    # opening or creating pdf file\n",
    "    flat_pdf = f\"{img_file}.pdf\"\n",
    "    file = open(flat_pdf, \"wb\")\n",
    "    \n",
    "    # writing pdf files with chunks\n",
    "    file.write(pdf_bytes)\n",
    "\n",
    "    # closing image file\n",
    "    image_file.close()\n",
    "    \n",
    "     # Delete Fillable PDF\n",
    "    delete_file(img_file + '.png')\n",
    "\n",
    "    # closing pdf file\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAP Letter function\n",
    "def gap_letter(template_df, pdf_template, position):\n",
    "    gap_path = './letters/staging/'\n",
    "\n",
    "    template_df['payment_amount'] = template_df['payment_amount'].map('${:,.2f}'.format)\n",
    "    template_df['loss_date'] = pd.to_datetime(template_df['loss_date']).dt.strftime('%B %d, %Y')\n",
    "    template_df['StateDesc'] = template_df['StateDesc'].astype(str).replace({'None':''})\n",
    "    template_df['StateCode'] = template_df['StateCode'].astype(str).replace({'None':''})\n",
    "    template_df['f_lang'] = template_df['f_lang'].astype(str).replace({'None':''})\n",
    "    letter_date = f\"{datetime.now():%B %d, %Y}\"   \n",
    "\n",
    "    for index, row in template_df.iterrows():\n",
    "\n",
    "        # empty dict\n",
    "        data_dict = {}\n",
    "        # store field data in dictionary\n",
    "        data_dict = {\n",
    "            'Date': letter_date,\n",
    "            'Lender': template_df.loc[index]['alt_name'],\n",
    "            'Contact': template_df.loc[index]['contact'],\n",
    "            'Address': template_df.loc[index]['address1'],\n",
    "            'City_St_Zip': f\"{template_df.loc[index]['city']}, {template_df.loc[index]['state']} {template_df.loc[index]['zip']}\",\n",
    "            'Lender2': template_df.loc[index]['alt_name'],\n",
    "            'Borrower': f\"{template_df.loc[index]['first']} {template_df.loc[index]['last']}\",\n",
    "            'Claim_Nbr': template_df.loc[index]['claim_nbr'],\n",
    "            'Acct_Nbr': template_df.loc[index]['acct_number'],\n",
    "            'DOL': template_df.loc[index]['loss_date'],\n",
    "            'GAP_Amt': template_df.loc[index]['payment_amount'],\n",
    "            'State': template_df.loc[index]['StateDesc'],\n",
    "            'St_Code': template_df.loc[index]['StateCode'],\n",
    "            'Fraud': template_df.loc[index]['f_lang'],\n",
    "        }\n",
    "\n",
    "        # store paths as variables\n",
    "        output_file = f\"{template_df.loc[index]['claim_nbr']}-{position}.pdf\"\n",
    "        output_path_fn = f\"{gap_path}{output_file}\"\n",
    "\n",
    "        fill_pdf(pdf_template, output_path_fn, data_dict)\n",
    "\n",
    "        # Set File Paths\n",
    "        flat_output = f\"{os.path.dirname(os.path.abspath(output_file))}\\{gap_path}\\{output_file}\"\n",
    "        img_file = f\"{os.path.dirname(os.path.abspath(output_file))}\\{gap_path}\\{template_df.loc[index]['claim_nbr']}-{position}\"\n",
    "\n",
    "        # Flatten pdf using flatten_pdf function\n",
    "        flatten_pdf(flat_output, img_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAP Calculation function\n",
    "def calculations(template_df, pdf_template, position):\n",
    "    gap_path = './letters/staging/'\n",
    "\n",
    "    template_df['loss_date'] = pd.to_datetime(template_df['loss_date']).dt.strftime('%B %d, %Y')\n",
    "    template_df['last_payment'] = pd.to_datetime(template_df['last_payment']).dt.strftime('%B %d, %Y')  \n",
    "    template_df['incp_date'] = pd.to_datetime(template_df['incp_date']).dt.strftime('%B %d, %Y')    \n",
    "    template_df['payoff'] = template_df['payoff'].map('${:,.2f}'.format)\n",
    "    template_df['past_due'] = template_df['past_due'].map('${:,.2f}'.format)\n",
    "    template_df['late_fees'] = template_df['late_fees'].map('${:,.2f}'.format)\n",
    "    template_df['skip_pymts'] = template_df['skip_pymts'].map('${:,.2f}'.format)\n",
    "    template_df['skip_fees'] = template_df['skip_fees'].map('${:,.2f}'.format)\n",
    "    template_df['primary_pymt'] = template_df['primary_pymt'].map('${:,.2f}'.format)\n",
    "    template_df['excess_deductible'] = template_df['excess_deductible'].map('${:,.2f}'.format)\n",
    "    template_df['scr'] = template_df['scr'].map('${:,.2f}'.format)\n",
    "    template_df['clr'] = template_df['clr'].map('${:,.2f}'.format)\n",
    "    template_df['cdr'] = template_df['cdr'].map('${:,.2f}'.format)\n",
    "    template_df['oref'] = template_df['oref'].map('${:,.2f}'.format)\n",
    "    template_df['salvage'] = template_df['salvage'].map('${:,.2f}'.format)\n",
    "    template_df['prior_dmg'] = template_df['prior_dmg'].map('${:,.2f}'.format)\n",
    "    template_df['over_ltv'] = template_df['over_ltv'].map('${:,.2f}'.format)\n",
    "    template_df['other1_amt'] = template_df['other1_amt'].map('${:,.2f}'.format)\n",
    "    template_df['other2_amt'] = template_df['other2_amt'].map('${:,.2f}'.format)\n",
    "    template_df['gap_payable'] = template_df['gap_payable'].map('${:,.2f}'.format)\n",
    "    template_df['balance_last_pay'] = template_df['balance_last_pay'].map('${:,.2f}'.format)\n",
    "    template_df['per_day'] = template_df['per_day'].map('${:,.2f}'.format)\n",
    "    template_df['deductible'] = template_df['deductible'].map('${:,.2f}'.format)\n",
    "    template_df['subtotal'] = template_df['subtotal'].map('${:,.2f}'.format)\n",
    "    template_df['nbr_of_days'] = template_df['nbr_of_days'].map('{:,.0f}'.format)\n",
    "    template_df['interest_rate'] = template_df['interest_rate'].map('{:,.2f}%'.format)\n",
    "    template_df['ltv'] = template_df['ltv'].map('{:,.2f}%'.format)\n",
    "    template_df['ltv_limit'] = template_df['ltv_limit'].map('{:,.2f}%'.format)\n",
    "    template_df['percent_uncovered'] = template_df['percent_uncovered'].map('{:,.2f}%'.format)\n",
    "    template_df['covered_fin_amount'] = template_df['covered_fin_amount'].map('${:,.2f}'.format)\n",
    "    template_df['Amt_Fin'] = template_df['Amt_Fin'].map('${:,.2f}'.format)\n",
    "    template_df['nada_value'] = template_df['nada_value'].map('${:,.2f}'.format)\n",
    "\n",
    "    for index, row in template_df.iterrows():\n",
    "\n",
    "        # empty dict\n",
    "        data_dict = {}\n",
    "        # store field data in dictionary\n",
    "        data_dict = {\n",
    "            'Claim_Number': template_df.loc[index]['claim_nbr'],\n",
    "            'Status': 'Paid',\n",
    "            'Borrower': f\"{template_df.loc[index]['first']} {template_df.loc[index]['last']}\",\n",
    "            'Vehicle': template_df.loc[index]['vehicle'],\n",
    "            'Date_Of_Loss': template_df.loc[index]['loss_date'],\n",
    "            'Type_Of_Loss': template_df.loc[index]['loss_type'],\n",
    "            'Lender': template_df.loc[index]['alt_name'],         \n",
    "            'Lender_Contact': template_df.loc[index]['contact'],\n",
    "            'Insurance_Carrier': template_df.loc[index]['carrier'],\n",
    "            'Inception_Date': template_df.loc[index]['incp_date'],\n",
    "            'Deductible': template_df.loc[index]['deductible'],\n",
    "            'Payoff': template_df.loc[index]['payoff'],\n",
    "            'Past_Due': template_df.loc[index]['past_due'],\n",
    "            'Late_Fees': template_df.loc[index]['late_fees'],\n",
    "            'Skips': template_df.loc[index]['skip_pymts'],\n",
    "            'Skip_Fees': template_df.loc[index]['skip_fees'],\n",
    "            'Primary': template_df.loc[index]['primary_pymt'],\n",
    "            'Deductible_Excess': template_df.loc[index]['excess_deductible'],\n",
    "            'SCR': template_df.loc[index]['scr'],\n",
    "            'CL_Refund': template_df.loc[index]['clr'],\n",
    "            'CD_Refund': template_df.loc[index]['cdr'],\n",
    "            'O_Refund': template_df.loc[index]['oref'],\n",
    "            'Salvage': template_df.loc[index]['salvage'],\n",
    "            'Prior_Damage': template_df.loc[index]['prior_dmg'],\n",
    "            'Over_LTV': template_df.loc[index]['over_ltv'],\n",
    "            'Other1_Description': template_df.loc[index]['other1_description'],\n",
    "            'Other2_Description': template_df.loc[index]['other2_description'],\n",
    "            'Other1': template_df.loc[index]['other1_amt'],\n",
    "            'Other2': template_df.loc[index]['other2_amt'],\n",
    "            'Deduction_Subtotal': template_df.loc[index]['subtotal'],\n",
    "            'GAP_Amt': template_df.loc[index]['gap_payable'], \n",
    "            'Last_pymt_date': template_df.loc[index]['last_payment'], \n",
    "            'DOL': template_df.loc[index]['loss_date'],\n",
    "            'Number_of_days': template_df.loc[index]['nbr_of_days'], \n",
    "            'Loan_Payoff_As_of_DOL': template_df.loc[index]['payoff'],\n",
    "            'Bal_as_of_last_pymt': template_df.loc[index]['balance_last_pay'],\n",
    "            'Interest_Rate': template_df.loc[index]['interest_rate'],\n",
    "            'Interest_Per_Day': template_df.loc[index]['per_day'],\n",
    "            'Amt_financed': template_df.loc[index]['Amt_Fin'],\n",
    "            'ACV': template_df.loc[index]['nada_value'], \n",
    "            'LTV': template_df.loc[index]['ltv'],\n",
    "            'Max_Amt_Financed': template_df.loc[index]['covered_fin_amount'],\n",
    "            'LTV_limit': template_df.loc[index]['ltv_limit'],\n",
    "            'Percentage_Not_Covered': template_df.loc[index]['percent_uncovered']\n",
    "        }\n",
    "\n",
    "        # store paths as variables\n",
    "        output_file = f\"{template_df.loc[index]['claim_nbr']}-{position}.pdf\"\n",
    "        output_path_fn = f\"{gap_path}{output_file}\"\n",
    "\n",
    "        fill_pdf(pdf_template, output_path_fn, data_dict)\n",
    "\n",
    "        # Set File Paths\n",
    "        flat_output = f\"{os.path.dirname(os.path.abspath(output_file))}\\{gap_path}\\{output_file}\"\n",
    "        img_file = f\"{os.path.dirname(os.path.abspath(output_file))}\\{gap_path}\\{template_df.loc[index]['claim_nbr']}-{position}\"\n",
    "\n",
    "        # Flatten pdf using flatten_pdf function\n",
    "        flatten_pdf(flat_output, img_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_dir (path, ext):\n",
    "    for x in os.listdir(path):\n",
    "        if x.endswith(ext):\n",
    "            os.remove(os.path.join(path, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileList (path, ext):\n",
    "    file_list = []\n",
    "    for x in os.listdir(path):\n",
    "        if x.endswith(ext):\n",
    "            file_list.append(x)\n",
    "\n",
    "    # sort list to collate pages\n",
    "    file_list.sort()\n",
    "    \n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TotalRestart Calculation function\n",
    "def tr_calculations(template_df, pdf_template, position):\n",
    "    gap_path = './letters/staging/'\n",
    "\n",
    "    template_df['loss_date'] = pd.to_datetime(template_df['loss_date']).dt.strftime('%B %d, %Y')\n",
    "    template_df['incp_date'] = pd.to_datetime(template_df['incp_date']).dt.strftime('%B %d, %Y')\n",
    "    template_df['primary_pymt'] = template_df['primary_pymt'].map('${:,.2f}'.format)\n",
    "    template_df['excess_deductible'] = template_df['excess_deductible'].map('${:,.2f}'.format)\n",
    "    template_df['scr'] = template_df['scr'].map('${:,.2f}'.format)\n",
    "    template_df['clr'] = template_df['clr'].map('${:,.2f}'.format)\n",
    "    template_df['cdr'] = template_df['cdr'].map('${:,.2f}'.format)\n",
    "    template_df['oref'] = template_df['oref'].map('${:,.2f}'.format)\n",
    "    template_df['salvage'] = template_df['salvage'].map('${:,.2f}'.format)\n",
    "    template_df['prior_dmg'] = template_df['prior_dmg'].map('${:,.2f}'.format)\n",
    "    template_df['other1_amt'] = template_df['other1_amt'].map('${:,.2f}'.format)\n",
    "    template_df['other2_amt'] = template_df['other2_amt'].map('${:,.2f}'.format)\n",
    "    template_df['other3_amt'] = template_df['other3_amt'].map('${:,.2f}'.format)    \n",
    "    template_df['gap_payable'] = template_df['gap_payable'].map('${:,.2f}'.format)\n",
    "    template_df['subtotal'] = template_df['subtotal'].map('${:,.2f}'.format)\n",
    "    template_df['nada_value'] = template_df['nada_value'].map('${:,.2f}'.format)\n",
    "    template_df['max_benefit'] = template_df['max_benefit'].map('${:,.2f}'.format)    \n",
    "    template_df['totalrestart_payable'] = template_df['totalrestart_payable'].map('${:,.2f}'.format)\n",
    "\n",
    "    for index, row in template_df.iterrows():\n",
    "\n",
    "        # empty dict\n",
    "        data_dict = {}\n",
    "        # store field data in dictionary\n",
    "        data_dict = {\n",
    "            'Claim_Number': template_df.loc[index]['claim_nbr'],\n",
    "            'Status': 'Paid',\n",
    "            'Borrower': f\"{template_df.loc[index]['first']} {template_df.loc[index]['last']}\",\n",
    "            'Vehicle': template_df.loc[index]['vehicle'],\n",
    "            'Date_Of_Loss': template_df.loc[index]['loss_date'],\n",
    "            'Type_Of_Loss': template_df.loc[index]['loss_type'],\n",
    "            'Lender': template_df.loc[index]['alt_name'],         \n",
    "            'Lender_Contact': template_df.loc[index]['contact'],\n",
    "            'Inception_Date': template_df.loc[index]['incp_date'],\n",
    "            'Max_Potential_Benefit': template_df.loc[index]['max_benefit'],\n",
    "            'Membership_Term': template_df.loc[index]['term'],\n",
    "            'Primary': template_df.loc[index]['primary_pymt'],\n",
    "            'Deductible_Excess': template_df.loc[index]['excess_deductible'],\n",
    "            'SCR': template_df.loc[index]['scr'],\n",
    "            'CL_Refund': template_df.loc[index]['clr'],\n",
    "            'CD_Refund': template_df.loc[index]['cdr'],\n",
    "            'O_Refund': template_df.loc[index]['oref'],\n",
    "            'Salvage': template_df.loc[index]['salvage'],\n",
    "            'Prior_Damage': template_df.loc[index]['prior_dmg'],\n",
    "            'Other1_Description': template_df.loc[index]['other1_description'],\n",
    "            'Other2_Description': template_df.loc[index]['other2_description'],\n",
    "            'Other3_Description': template_df.loc[index]['other3_description'],\n",
    "            'Other1': template_df.loc[index]['other1_amt'],\n",
    "            'Other2': template_df.loc[index]['other2_amt'],\n",
    "            'Other3': template_df.loc[index]['other3_amt'],\n",
    "            'TR_Deduction_Subtotal': template_df.loc[index]['subtotal'],\n",
    "            'GAP_Amt': template_df.loc[index]['gap_payable'], \n",
    "            'ACV': template_df.loc[index]['nada_value'], \n",
    "            'TR_Amt': template_df.loc[index]['totalrestart_payable']\n",
    "        }\n",
    "\n",
    "        # store paths as variables\n",
    "        output_file = f\"{template_df.loc[index]['claim_nbr']}-{position}.pdf\"\n",
    "        output_path_fn = f\"{gap_path}{output_file}\"\n",
    "\n",
    "        fill_pdf(pdf_template, output_path_fn, data_dict)\n",
    "\n",
    "        # Set File Paths\n",
    "        flat_output = f\"{os.path.dirname(os.path.abspath(output_file))}\\{gap_path}\\{output_file}\"\n",
    "        img_file = f\"{os.path.dirname(os.path.abspath(output_file))}\\{gap_path}\\{template_df.loc[index]['claim_nbr']}-{position}\"\n",
    "\n",
    "        # Flatten pdf using flatten_pdf function\n",
    "        flatten_pdf(flat_output, img_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Email\n",
    "def send_email(toEmail, subject, msg_html, attachPath, *args):\n",
    "    fromEmail = 'claims@visualgap.com'\n",
    "\n",
    "    # address message\n",
    "    msg = MIMEMultipart()\n",
    "    msg['Subject'] = subject\n",
    "    msg['From'] = fromEmail\n",
    "    msg['To'] = ','.join(toEmail)\n",
    "\n",
    "    # create body\n",
    "    body_html = MIMEText(msg_html, 'html')\n",
    "    msg.attach(body_html) \n",
    "\n",
    "    for arg in args:\n",
    "        fName = attachPath + arg\n",
    "        filename = os.path.abspath(fName)\n",
    "\n",
    "        with open(filename, 'rb') as fn:\n",
    "            attachment = MIMEApplication(fn.read())\n",
    "            attachment.add_header('Content-Disposition', 'attachment', filename=arg)\n",
    "            msg.attach(attachment)\n",
    "\n",
    "    context = ssl.create_default_context()\n",
    "    try:\n",
    "        server = smtplib.SMTP(smtp_host, port)\n",
    "        # check connection\n",
    "        server.ehlo()  \n",
    "        # Secure the connection\n",
    "        server.starttls(context=context)  \n",
    "        # check connection\n",
    "        server.ehlo()\n",
    "        server.login(e_user, e_pw)\n",
    "        # Send email\n",
    "        server.sendmail(fromEmail, toEmail, msg.as_string())\n",
    "\n",
    "    except Exception as e:\n",
    "        # Print any error messages\n",
    "        print(e)\n",
    "    finally:\n",
    "        server.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tovgc_1(df):\n",
    "    # Update toVGC to 1\n",
    "\n",
    "    if len(df) > 0:\n",
    "        for index, row in df.iterrows():\n",
    "            err_sql = '''\n",
    "                    UPDATE ready_to_be_paid\n",
    "                    SET toVGC = 1\n",
    "                    WHERE rtbp_id = {rtbp_id};\n",
    "                    '''.format(rtbp_id=row['rtbp_id'])\n",
    "\n",
    "            # connect to claim_qb_payments db\n",
    "            mysql_q(mysql_u, mysql_pw, mysql_host, 'claim_qb_payments', err_sql, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql query for GAP claims that are RTBP\n",
    "sql_file = '''\n",
    "            SELECT c.claim_id, c.claim_nbr, c.carrier_id, cl.alt_name, cl.dealer_securityId, cl.contact, cl.address1,\n",
    "                cl.city, cl.state, cl.zip, cl.payment_method, cb.first, cb.last, \n",
    "                IF(sq.gap_amt_paid > 0, 2,1) AS pymt_type_id, \n",
    "                IF(sq.gap_amt_paid > 0, ROUND(cc.gap_payable - sq.gap_amt_paid,2), cc.gap_payable) AS gap_due,\n",
    "                COALESCE(NULLIF(cb.acct_number,''),'0') AS acct_nbr, c.loss_date, cl.title, cl.email2\n",
    "            FROM claims c\n",
    "            INNER JOIN claim_lender cl\n",
    "                USING (claim_id)\n",
    "            INNER JOIN claim_borrower cb\n",
    "                USING (claim_id)\n",
    "            INNER JOIN claim_calculations cc\n",
    "                USING (claim_id)\n",
    "            INNER JOIN claim_status cs\n",
    "                ON (c.status_id = cs.status_id)\n",
    "            LEFT JOIN (SELECT cp.claim_id, SUM(cp.payment_amount) AS gap_amt_paid\n",
    "                    FROM claim_payments cp\n",
    "                    INNER JOIN (SELECT c.claim_id\n",
    "                                FROM claims c\n",
    "                                INNER JOIN claim_status cs\n",
    "                                    ON (c.status_id = cs.status_id)\n",
    "                                WHERE cs.status_desc_id = 8) rtbp_sq\n",
    "                        USING (claim_id)\n",
    "                    WHERE payment_category_id = 1\n",
    "                    GROUP BY cp.claim_id) sq\n",
    "                ON (c.claim_id = sq.claim_id)\n",
    "            WHERE cs.status_desc_id = 8;\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save query results as DF\n",
    "df = pd.DataFrame(mysql_q(vgc_u, vgc_pw, vgc_host, 'visualgap_claims', sql_file, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column names\n",
    "df_cols = ['claim_id', 'claim_nbr', 'carrier_id', 'lender_name', 'dealer_securityId', 'contact', 'address1', 'city', 'state', 'zip', \n",
    "                            'pymt_method', 'first', 'last', 'pymt_type_id', 'amount', 'acct_number', 'loss_date', 'email', 'email2']\n",
    "\n",
    "df.columns = df_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql query for PLUS claims that are RTBP\n",
    "sql_file_plus = '''\n",
    "                SELECT sqp.claim_id, c.claim_nbr, c.carrier_id, cl.alt_name, cl.dealer_securityId, cl.contact, \n",
    "                    cl.address1, cl.city, cl.state, cl.zip, cl.payment_method, cb.first, cb.last, \n",
    "                    1 AS pymt_type_id, \n",
    "                    IF(cl.customer_securityId = 9401, 1500, 1000) AS gap_plus_due, COALESCE(NULLIF(cb.acct_number,''),'0') AS acct_nbr,\n",
    "                    c.loss_date, cl.title, cl.email2\n",
    "                FROM claims c\n",
    "                INNER JOIN claim_lender cl\n",
    "                    USING (claim_id)\n",
    "                INNER JOIN claim_borrower cb\n",
    "                    USING (claim_id)\n",
    "                INNER JOIN (SELECT pb.claim_id\n",
    "                            FROM claim_plus_benefit pb\n",
    "                            WHERE status_desc_id = 8) sqp\n",
    "                    USING (claim_id)\n",
    "                INNER JOIN (SELECT c.claim_id\n",
    "                            FROM claims c\n",
    "                            INNER JOIN claim_status cs\n",
    "                                ON (c.status_id = cs.status_id)  \n",
    "                            WHERE cs.status_desc_id = 8\n",
    "                                OR cs.status_desc_id = 4) sqg\n",
    "                    USING (claim_id);\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(mysql_q(vgc_u, vgc_pw, vgc_host, 'visualgap_claims', sql_file_plus, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column names\n",
    "df2_cols = ['claim_id', 'claim_nbr', 'carrier_id', 'lender_name', 'dealer_securityId', 'contact', 'address1', 'city', 'state', 'zip', \n",
    "                            'pymt_method', 'first', 'last', 'pymt_type_id', 'amount', 'acct_number', 'loss_date', 'email', 'email2']\n",
    "\n",
    "df2.columns = df2_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql query for TotalRestart claims that are RTBP\n",
    "# manually entered carrier_id 12\n",
    "sql_file_tr = '''\n",
    "            SELECT sqp.claim_id, c.claim_nbr, 12 AS carrier_id, cl.alt_name, cl.dealer_securityId, cl.contact, cl.address1, \n",
    "            cl.city, cl.state, cl.zip, 'Check' AS payment_method, cb.first, cb.last, 1 AS pymt_type_id, \n",
    "            ctr.totalrestart_payable AS tr_due, COALESCE(NULLIF(cb.acct_number,''),'0') AS acct_nbr, c.loss_date, cl.title, cl.email2\n",
    "            FROM claims c\n",
    "            INNER JOIN claim_lender cl\n",
    "                USING (claim_id)\n",
    "            INNER JOIN claim_borrower cb\n",
    "                USING (claim_id)\n",
    "            INNER JOIN (SELECT pb.claim_id\n",
    "                        FROM claim_totalrestart pb\n",
    "                        WHERE status_desc_id = 8) sqp\n",
    "                USING (claim_id)\n",
    "            INNER JOIN claim_totalrestart ctr\n",
    "                USING (claim_id)\n",
    "             '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(mysql_q(vgc_u, vgc_pw, vgc_host, 'visualgap_claims', sql_file_tr, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column names\n",
    "df3_cols = ['claim_id', 'claim_nbr', 'carrier_id', 'lender_name', 'dealer_securityId', 'contact', 'address1', 'city', 'state', 'zip', \n",
    "                            'pymt_method', 'first', 'last', 'pymt_type_id', 'amount', 'acct_number', 'loss_date', 'email', 'email2']\n",
    "\n",
    "df3.columns = df3_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAP\n",
    "# convert columns list to string\n",
    "cols = \", \".join(df_cols)\n",
    "\n",
    "# insert DF into the ready_to_be_paid table of the claim_qb_payments database\n",
    "for x,rows in df.iterrows():\n",
    "\n",
    "    sql_file2 = '''INSERT INTO ready_to_be_paid ({columns}, payment_category_id, check_nbr, batch_id, qb_txnid, toVGC) VALUES ({claim_id}, \"{claim_nbr}\", {carrier_id},\"{lender_name}\", \"{lender_id}\",\"{contact}\", \n",
    "                \"{address1}\", \"{city}\", \"{state}\", \"{zip}\", \"{pymt_method}\", \"{first}\", \"{last}\", {pymt_type_id}, {amount}, \"{acct_nbr}\", \"{loss_date}\", \"{email}\", \"{email2}\", 1, 0, {batchId}, 0, 0);'''.format(columns=cols, \n",
    "                claim_id=rows['claim_id'], claim_nbr=rows['claim_nbr'], carrier_id=rows['carrier_id'], lender_name=rows['lender_name'], lender_id=rows['dealer_securityId'], \n",
    "                contact=rows['contact'], address1=rows['address1'], city=rows['city'], state=rows['state'], zip=rows['zip'], \n",
    "                pymt_method=rows['pymt_method'], first = rows['first'], last = rows['last'], pymt_type_id = rows['pymt_type_id'], \n",
    "                amount = rows['amount'], acct_nbr = rows['acct_number'], loss_date = rows['loss_date'], email= rows['email'], email2= rows['email2'], batchId = batch_id)\n",
    "     \n",
    "    # # execute and commit sql\n",
    "    mysql_q(mysql_u, mysql_pw, mysql_host, 'claim_qb_payments', sql_file2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLUS\n",
    "# convert columns list to string\n",
    "cols = \", \".join(df2_cols)\n",
    "\n",
    "# insert DF into the ready_to_be_paid table of the claim_qb_payments database\n",
    "for x,rows in df2.iterrows():\n",
    "    sql_file2_plus = '''INSERT INTO ready_to_be_paid ({columns}, payment_category_id, check_nbr, batch_id, qb_txnid, toVGC) VALUES ({claim_id}, \"{claim_nbr}\", {carrier_id},\"{lender_name}\", \"{lender_id}\",\"{contact}\", \n",
    "                \"{address1}\", \"{city}\", \"{state}\", \"{zip}\", \"{pymt_method}\", \"{first}\", \"{last}\", {pymt_type_id}, {amount}, \"{acct_nbr}\", \"{loss_date}\", \"{email}\", \"{email2}\", 2, 0, {batchId}, 0, 0);'''.format(columns=cols, \n",
    "                claim_id=rows['claim_id'], claim_nbr=rows['claim_nbr'], carrier_id=rows['carrier_id'], lender_name=rows['lender_name'], lender_id=rows['dealer_securityId'], \n",
    "                contact=rows['contact'], address1=rows['address1'], city=rows['city'], state=rows['state'], zip=rows['zip'], \n",
    "                pymt_method=rows['pymt_method'], first = rows['first'], last = rows['last'], pymt_type_id = rows['pymt_type_id'], \n",
    "                amount = rows['amount'], acct_nbr = rows['acct_number'], loss_date = rows['loss_date'], email= rows['email'], email2= rows['email2'], batchId = batch_id)\n",
    "          \n",
    "    # execute and commit sql\n",
    "    mysql_q(mysql_u, mysql_pw, mysql_host, 'claim_qb_payments', sql_file2_plus, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOTALRESTART\n",
    "# convert columns list to string\n",
    "cols = \", \".join(df3_cols)\n",
    "\n",
    "# insert DF into the ready_to_be_paid table of the claim_qb_payments database\n",
    "for x,rows in df3.iterrows():\n",
    "    sql_file2_tr = '''INSERT INTO ready_to_be_paid ({columns}, payment_category_id, check_nbr, batch_id, qb_txnid, toVGC) VALUES ({claim_id}, \"{claim_nbr}\", {carrier_id},\"{lender_name}\", \"{lender_id}\",\"{contact}\", \n",
    "                \"{address1}\", \"{city}\", \"{state}\", \"{zip}\", \"{pymt_method}\", \"{first}\", \"{last}\", {pymt_type_id}, {amount}, \"{acct_nbr}\", \"{loss_date}\", \"{email}\", \"{email2}\", 3, 0, {batchId}, 0, 0);'''.format(columns=cols, \n",
    "                claim_id=rows['claim_id'], claim_nbr=rows['claim_nbr'], carrier_id=rows['carrier_id'], lender_name=rows['lender_name'], lender_id=rows['dealer_securityId'], \n",
    "                contact=rows['contact'], address1=rows['address1'], city=rows['city'], state=rows['state'], zip=rows['zip'], \n",
    "                pymt_method=rows['pymt_method'], first = rows['first'], last = rows['last'], pymt_type_id = rows['pymt_type_id'], \n",
    "                amount = rows['amount'], acct_nbr = rows['acct_number'], loss_date = rows['loss_date'], email= rows['email'], email2= rows['email2'], batchId = batch_id)\n",
    "          \n",
    "    # execute and commit sql\n",
    "    mysql_q(mysql_u, mysql_pw, mysql_host, 'claim_qb_payments', sql_file2_tr, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create select query to pull current batch with ID\n",
    "sql_file3 = '''SELECT rtbp_id, claim_id, claim_nbr, carrier_id, lender_name, dealer_securityId, contact, address1, city, \n",
    "                     state, zip, pymt_method, first, last, pymt_type_id, amount, payment_category_id, check_nbr, batch_id, \n",
    "                     qb_txnid, acct_number, loss_date, email, email2\n",
    "              FROM ready_to_be_paid\n",
    "              WHERE batch_id = {batchId}\n",
    "              ORDER BY payment_category_id, claim_id;'''.format(batchId = batch_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save query results as DF\n",
    "pymts_df = pd.DataFrame(mysql_q(mysql_u, mysql_pw, mysql_host, 'claim_qb_payments', sql_file3, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save query results as DF\n",
    "\n",
    "# add column names\n",
    "pymts_df_cols = ['rtbp_id', 'claim_id', 'claim_nbr', 'carrier_id', 'lender_name', 'dealer_securityId', 'contact', 'address1', 'city', 'state', 'zip', \n",
    "                    'pymt_method', 'first', 'last', 'pymt_type_id', 'amount','payment_category_id', 'check_nbr', 'batch_id', 'qb_txnid', \n",
    "                    'acct_number', 'loss_date', 'email', 'email2']\n",
    "\n",
    "pymts_df.columns = pymts_df_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql query for expense accounts in DB\n",
    "sql_file4 = '''\n",
    "    SELECT carrier_id, qb_fullname\n",
    "    FROM qb_accounts\n",
    "    WHERE account_type = 'Expense';\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save query results as DF\n",
    "expense_df = pd.DataFrame(mysql_q(mysql_u, mysql_pw, mysql_host, 'claim_qb_payments', sql_file4, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column names to DF\n",
    "col_names = ['carrier_id', 'expense']\n",
    "expense_df.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql query for checking accounts in DB\n",
    "sql_file5 = '''\n",
    "    SELECT carrier_id, qb_fullname\n",
    "    FROM qb_accounts\n",
    "    WHERE account_type = 'Checking';\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save query results as DF\n",
    "checking_df = pd.DataFrame(mysql_q(mysql_u, mysql_pw, mysql_host, 'claim_qb_payments', sql_file5, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column names to DF\n",
    "col_names = ['carrier_id', 'checking']\n",
    "checking_df.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge expense account name into df\n",
    "pymts_df = pymts_df.merge(expense_df, how='left', left_on='carrier_id', right_on='carrier_id').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge checking account name into df\n",
    "pymts_df = pymts_df.merge(checking_df, how='left', left_on='carrier_id', right_on='carrier_id').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Error DF\n",
    "error_df = pd.DataFrame(columns = ['rtbp_id', 'err_msg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing expense and/or checking names\n",
    "chk_exp_error_df = pymts_df.loc[(pymts_df['expense'].isnull()) | (pymts_df['checking'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with error\n",
    "pymts_df.drop(pymts_df[(pymts_df['expense'].isnull()) | (pymts_df['checking'].isnull())].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to Error DF\n",
    "if len(chk_exp_error_df) > 0:\n",
    "    for index, row in chk_exp_error_df.iterrows():\n",
    "        error_df.loc[error_df.shape[0]] = [row['rtbp_id'], 'No matching CHECKING and-or EXPENSE accounts in QuickBooks']       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sql server connection\n",
    "cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+svr+';DATABASE='+db+';UID='+sql_u+';PWD='+ sql_pw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create query\n",
    "sql_svr_file = '''SELECT VGSecurityId, QB_ListID\n",
    "                  FROM business_entity\n",
    "                  WHERE QB_ListID IS NOT NULL;\n",
    "               '''\n",
    "# execute query\n",
    "listid_df = pd.read_sql(sql_svr_file, cnxn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY ########################################################################################\n",
    "# Convert test dealer_securityId to Production dealer_securityId\n",
    "pymts_df['dealer_securityId'].replace({22260:46724,21945:52715}, inplace=True)\n",
    "# TEMPORARY ########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge QB_ListID into df\n",
    "pymts_df = pymts_df.merge(listid_df, how='left', left_on='dealer_securityId', right_on='VGSecurityId').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing QBlistID\n",
    "qbListId_error_df = pymts_df.loc[(pymts_df['QB_ListID'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to Error DF\n",
    "if len(qbListId_error_df) > 0:\n",
    "    for index, row in qbListId_error_df.iterrows():\n",
    "        error_df.loc[error_df.shape[0]] = [row['rtbp_id'], 'Missing QB_LIST_ID in the Claim_Payments database.  Run GET CUSTOMER to update the database.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using error_df, send error messages to ready_to_be_paid DB and update toVGC = 2\n",
    "if len(error_df) > 0:\n",
    "    for index, row in error_df.iterrows():\n",
    "        err_sql = '''\n",
    "                  UPDATE ready_to_be_paid\n",
    "                  SET toVGC = 2,\n",
    "                      err_msg = '{err_msg}'\n",
    "                  WHERE rtbp_id = {rtbp_id};\n",
    "                  '''.format(err_msg=row['err_msg'], rtbp_id=row['rtbp_id'])\n",
    "\n",
    "mysql_q(mysql_u, mysql_pw, mysql_host, 'claim_qb_payments', err_sql, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with error\n",
    "pymts_df.drop(pymts_df[pymts_df['QB_ListID'].isnull()].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add carrier name\n",
    "sql_file6 = '''\n",
    "    SELECT carrier_id, description\n",
    "    FROM carriers;\n",
    "    '''\n",
    "\n",
    "# save query results as DF\n",
    "carrier_df = pd.DataFrame(mysql_q(vgc_u, vgc_pw, vgc_host, 'visualgap_claims', sql_file6, 0, 0))\n",
    "\n",
    "col_names = ['carrier_id', 'carrier']\n",
    "carrier_df.columns = col_names\n",
    "\n",
    "# Merge QB_ListID into df\n",
    "pymts_df = pymts_df.merge(carrier_df, left_on='carrier_id', right_on='carrier_id').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# payments greater than 0 df\n",
    "qb_pymts_df = pymts_df.loc[pymts_df['amount'] > 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Quickbooks\n",
    "try:\n",
    "    sessionManager = wc.Dispatch(\"QBXMLRP2.RequestProcessor\")    \n",
    "    sessionManager.OpenConnection('', 'Claim Payments')\n",
    "    ticket = sessionManager.BeginSession(\"\", 2)\n",
    "except Exception as e:\n",
    "    print('''\n",
    "    Make sure QuickBooks is running and you are logged into the Company File.\n",
    "    ERROR: {}'''.format(e))\n",
    "    sys.exit(\"Error with communicating with QuickBooks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create qbxml query to add payments to QB\n",
    "pay_date = f\"{dt.date.today():%Y-%m-%d}\"\n",
    "\n",
    "for index, row in qb_pymts_df.iterrows():\n",
    "    if row['payment_category_id'] == 1:\n",
    "        if row['pymt_type_id'] == 2:\n",
    "            pymt_type = 'Additional GAP Claim Pymt'\n",
    "        else: \n",
    "            pymt_type = 'GAP Claim'\n",
    "\n",
    "    elif row['payment_category_id'] == 2:\n",
    "        if row['pymt_type_id'] == 2:\n",
    "            pymt_type = 'Additional GAP Plus Pymt'\n",
    "        else: \n",
    "            pymt_type = 'GAP Plus'\n",
    "            \n",
    "    elif row['payment_category_id'] == 3:\n",
    "        if row['pymt_type_id'] == 2:\n",
    "            pymt_type = 'Additional TotalRestart Pymt'\n",
    "        else: \n",
    "            pymt_type = 'TotalRestart'\n",
    "\n",
    "    pymtAmt = \"{:.2f}\".format(row['amount'])\n",
    "\n",
    "    if row['pymt_method'] == 'Check':\n",
    "        qbxmlQuery = '''\n",
    "        <?qbxml version=\"14.0\"?>\n",
    "        <QBXML>\n",
    "            <QBXMLMsgsRq onError=\"stopOnError\">\n",
    "                <CheckAddRq>\n",
    "                    <CheckAdd>\n",
    "                        <AccountRef>\n",
    "                            <FullName>{checking}</FullName>\n",
    "                        </AccountRef>\n",
    "                        <PayeeEntityRef>\n",
    "                            <ListID>{lender_qbid}</ListID>\n",
    "                        </PayeeEntityRef>\n",
    "                        <TxnDate>{date}</TxnDate>\n",
    "                        <Memo>{memo}</Memo>\n",
    "                        <Address>\n",
    "                            <Addr1>{lender}</Addr1>\n",
    "                            <Addr2>{contact}</Addr2>\n",
    "                            <Addr3>{address}</Addr3>\n",
    "                            <City>{city}</City>\n",
    "                            <State>{state}</State>\n",
    "                            <PostalCode>{zip}</PostalCode>\n",
    "                        </Address>\n",
    "                        <IsToBePrinted>true</IsToBePrinted>\n",
    "                        <ExpenseLineAdd>\n",
    "                            <AccountRef>\n",
    "                                <FullName>{expense}</FullName>\n",
    "                            </AccountRef>\n",
    "                            <Amount>{amount}</Amount>\n",
    "                            <Memo>{memo}</Memo>\n",
    "                        </ExpenseLineAdd>\n",
    "                    </CheckAdd>\n",
    "                </CheckAddRq>\n",
    "            </QBXMLMsgsRq>\n",
    "        </QBXML>'''.format(checking=row['checking'], lender_qbid=row['QB_ListID'], lender=row['lender_name'], date=pay_date, \n",
    "                memo=f\"{row['last']}/{row['first']} {pymt_type}\", contact=row['contact'], address=row['address1'], city=row['city'], state=row['state'],\n",
    "                zip=row['zip'], expense=row['expense'], amount = pymtAmt)\n",
    "\n",
    "    elif 'ACH' in row['pymt_method']:\n",
    "        qbxmlQuery = '''\n",
    "        <?qbxml version=\"14.0\"?>\n",
    "        <QBXML>\n",
    "            <QBXMLMsgsRq onError=\"stopOnError\">\n",
    "                <CheckAddRq>\n",
    "                    <CheckAdd>\n",
    "                        <AccountRef>\n",
    "                            <FullName>{checking}</FullName>\n",
    "                        </AccountRef>\n",
    "                        <PayeeEntityRef>\n",
    "                            <ListID>{lender_qbid}</ListID>\n",
    "                        </PayeeEntityRef>\n",
    "                        <RefNumber>ACH</RefNumber>\n",
    "                        <TxnDate>{date}</TxnDate>\n",
    "                        <Memo>{memo}</Memo>\n",
    "                        <Address>\n",
    "                            <Addr1>{lender}</Addr1>\n",
    "                            <Addr2>{contact}</Addr2>\n",
    "                            <Addr3>{address}</Addr3>\n",
    "                            <City>{city}</City>\n",
    "                            <State>{state}</State>\n",
    "                            <PostalCode>{zip}</PostalCode>\n",
    "                        </Address>\n",
    "                        <IsToBePrinted>false</IsToBePrinted>\n",
    "                        <ExpenseLineAdd>\n",
    "                            <AccountRef>\n",
    "                                <FullName>{expense}</FullName>\n",
    "                            </AccountRef>\n",
    "                            <Amount>{amount}</Amount>\n",
    "                            <Memo>{memo}</Memo>\n",
    "                        </ExpenseLineAdd>\n",
    "                    </CheckAdd>\n",
    "                </CheckAddRq>            \n",
    "            </QBXMLMsgsRq>\n",
    "        </QBXML>'''.format(checking=row['checking'], lender_qbid=row['QB_ListID'], lender=row['lender_name'], date=pay_date, \n",
    "                memo=f\"{row['last']}/{row['first']} {pymt_type}\", contact=row['contact'], address=row['address1'], city=row['city'], state=row['state'],\n",
    "                zip=row['zip'], expense=row['expense'], amount = pymtAmt)\n",
    "        \n",
    "    # Send query and receive response\n",
    "    responseString = sessionManager.ProcessRequest(ticket, qbxmlQuery)\n",
    "\n",
    "    # output TxnID\n",
    "    QBXML = ET.fromstring(responseString)\n",
    "    QBXMLMsgsRs = QBXML.find('QBXMLMsgsRs')\n",
    "    checkResults = QBXMLMsgsRs.iter(\"CheckRet\")\n",
    "    txnId = 0\n",
    "    for checkResult in checkResults:\n",
    "        txnId = checkResult.find('TxnID').text\n",
    "\n",
    "    # Add TxnID to ready_to_be_paid table\n",
    "    qb_sql_file = '''UPDATE ready_to_be_paid\n",
    "                   SET qb_txnid = '{TxnID}',\n",
    "                       pymt_date = '{paydate}'\n",
    "                   WHERE rtbp_id = {rowID};'''.format(TxnID=txnId, paydate=pay_date, rowID=row['rtbp_id'])\n",
    "    \n",
    "    # execute and commit sql\n",
    "    mysql_q(mysql_u, mysql_pw, mysql_host, 'claim_qb_payments', qb_sql_file, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disconnect from Quickbooks\n",
    "sessionManager.EndSession(ticket)\n",
    "sessionManager.CloseConnection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Paid Date to $0 claims\n",
    "zero_pymt_df = pymts_df.loc[pymts_df['amount'] == 0].copy()\n",
    "for index, row in zero_pymt_df.iterrows():\n",
    "    #sql query\n",
    "    zero_sql_file = '''UPDATE ready_to_be_paid\n",
    "                   SET pymt_date = '{paydate}'\n",
    "                   WHERE rtbp_id = {rowID};'''.format(paydate=pay_date, rowID=row['rtbp_id'])\n",
    "    \n",
    "    # execute and commit sql\n",
    "    mysql_q(mysql_u, mysql_pw, mysql_host, 'claim_qb_payments', zero_sql_file, 0, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Fraud Language fields to pymts_df\n",
    "sql_file7 = '''\n",
    "    SELECT StateId, \n",
    "    StateDesc,\n",
    "    StateCode,\n",
    "    CAST(Language AS CHAR(1000) CHARACTER SET utf8)\n",
    "    FROM FraudLang;\n",
    "    '''\n",
    "\n",
    "# run sql query\n",
    "fraud_lang_df = pd.DataFrame(mysql_q(vgc_u, vgc_pw, vgc_host, 'visualgap_claims', sql_file7, 0, 0))\n",
    "\n",
    "col_names = ['StateId', 'StateDesc', 'StateCode', 'f_lang']\n",
    "fraud_lang_df.columns = col_names\n",
    "\n",
    "# Merge QB_ListID into df\n",
    "pymts_df = pymts_df.merge(fraud_lang_df, how='left', left_on='state', right_on='StateId').copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN with ''\n",
    "pymts_df.fillna('', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Column names\n",
    "pymts_df.rename(columns = {'amount':'payment_amount', 'lender_name':'alt_name'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create GAP Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect GAP payments\n",
    "gap_pymts_df = pymts_df.loc[pymts_df['payment_category_id'] == 1].copy()\n",
    "gap_letters_df = gap_pymts_df.loc[gap_pymts_df['payment_amount'] > 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove files from staging directory\n",
    "file_staging_dir = './letters/staging/'\n",
    "file_ext = \".pdf\"\n",
    "\n",
    "clear_dir(file_staging_dir, file_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create letters for amounts greater than $0 paid via check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['payment_amount'] = template_df['payment_amount'].map('${:,.2f}'.format)\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['loss_date'] = pd.to_datetime(template_df['loss_date']).dt.strftime('%B %d, %Y')\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['StateDesc'] = template_df['StateDesc'].astype(str).replace({'None':''})\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['StateCode'] = template_df['StateCode'].astype(str).replace({'None':''})\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['f_lang'] = template_df['f_lang'].astype(str).replace({'None':''})\n"
     ]
    }
   ],
   "source": [
    "# Filter gap_letters_df for pymt_method 'Check'\n",
    "checks_df = gap_letters_df.loc[gap_letters_df['pymt_method'] == 'Check'].copy()\n",
    "\n",
    "if len(checks_df.index) > 0:\n",
    "    # Create list of Carriers on checks_df\n",
    "    carriers = []\n",
    "    carriers = checks_df.carrier.unique()\n",
    "    letter_cols = ['claim_nbr', 'loss_date', 'alt_name', 'contact', 'address1', 'city', 'state', 'zip', 'first', 'last', 'acct_number', 'payment_amount', 'StateDesc', 'StateCode', 'f_lang' ]\n",
    "\n",
    "    # Loop through carriers list\n",
    "    for carrier in carriers:\n",
    "\n",
    "        # Variable Defaults\n",
    "        sql_where_cal = ''\n",
    "        g_letters = True\n",
    "        s_letters = True\n",
    "        cals = []\n",
    "\n",
    "        # check for payment_type_id for GAP letters by carrier\n",
    "        g_letters_df = checks_df.loc[(checks_df['pymt_type_id'] == 1) & (checks_df['carrier'] == carrier)].copy()\n",
    "\n",
    "        # GAP Letter - create WHERE statement\n",
    "        if len(g_letters_df.index) > 0:\n",
    "            g_letters = True\n",
    "        else:\n",
    "            # No Letters\n",
    "            g_letters = False\n",
    "\n",
    "        # check for payment_type_id for Supplemental letters\n",
    "        s_letters_df = checks_df.loc[(checks_df['pymt_type_id'] == 2) & (checks_df['carrier'] == carrier)].copy()\n",
    "\n",
    "        # Supplemental Letter\n",
    "        if len(s_letters_df.index) > 0:\n",
    "            s_letters = True\n",
    "        else:\n",
    "            # No Letters\n",
    "            s_letters = False\n",
    "\n",
    "        # Calculations\n",
    "        # check for calculations for carrier\n",
    "        calculations_df = checks_df.loc[checks_df['carrier'] == carrier].copy()\n",
    "\n",
    "        if len(calculations_df.index) > 0:\n",
    "            # Multiple Calculation\n",
    "            for index, row in calculations_df.iterrows():\n",
    "                cals.append(row['claim_id'])\n",
    "\n",
    "        # GAP - Create letters\n",
    "        if g_letters == True:\n",
    "            # create df for template\n",
    "            g_template_df = g_letters_df[letter_cols]\n",
    "\n",
    "            # Create GAP Letters\n",
    "            g_pdf_template = \"letters/pdf_templates/GAP_letter_template.pdf\"\n",
    "            position = 1\n",
    "            gap_letter(g_template_df, g_pdf_template, position)\n",
    "        \n",
    "        # Supplemental - Create letters\n",
    "        if s_letters == True:\n",
    "            # create df for template\n",
    "            s_template_df = s_letters_df[letter_cols]\n",
    "            \n",
    "            # Create Supplement Letters\n",
    "            s_pdf_template = \"letters/pdf_templates/Supp_letter_template.pdf\"\n",
    "            position = 2\n",
    "            gap_letter(s_template_df, s_pdf_template, position)\n",
    "\n",
    "        # Calculations - Create SQL query, run query, create calculation sheets\n",
    "        if g_letters == True or s_letters == True:\n",
    "\n",
    "            # create calculation dataframe\n",
    "            temp_cols = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]                             \n",
    "            c_consolidate_df = pd.DataFrame(columns = temp_cols)\n",
    "\n",
    "            for cal in cals:\n",
    "                # create sql for calculation\n",
    "                c_sql_query = '''\n",
    "                        SELECT c.claim_nbr, c.loss_date, l.alt_name, l.contact, b.first, b.last, cc.gap_payable,\n",
    "                            cl.incp_date, cl.last_payment, cl.interest_rate, cl.amount AS Amt_Fin, cc.balance_last_pay,\n",
    "                            cc.nbr_of_days, cc.per_day, ROUND(cc.payoff,2) AS payoff, (cc.ltv * 100) AS ltv, cc.covered_fin_amount,\n",
    "                            (cc.percent_uncovered * 100) AS percent_uncovered, (cc.ltv_limit * 100) AS ltv_limit, v.nada_value, CONCAT(v.year, ' ', v.make, ' ', v.model) AS vehicle,\n",
    "                            v.deductible, cls.description AS loss_type, cc26.description AS primary_carrier,\n",
    "                            cc14.amount AS past_due, cc15.amount AS late_fees, cc16.amount AS skip_pymts,\n",
    "                            cc17.amount AS skip_fees, cc5.amount AS primary_pymt, cc7.amount AS excess_deductible,\n",
    "                            cc8.amount AS scr, cc9.amount AS clr, cc10.amount AS cdr, cc11.amount AS oref,\n",
    "                            cc18.amount AS salvage, cc19.amount AS prior_dmg, cc20.amount AS over_ltv,\n",
    "                            cc21.amount AS other1_amt, cc21.description AS other1_description,\n",
    "                            cc22.amount AS other2_amt, cc22.description AS other2_description, ca.description AS carrier,\n",
    "                            (cc22.amount + cc21.amount + cc20.amount + cc19.amount + cc18.amount + cc11.amount + cc10.amount + cc9.amount + \n",
    "                            cc8.amount + cc7.amount + cc5.amount + cc17.amount + cc16.amount + cc15.amount + cc14.amount) as subtotal\n",
    "                        FROM claims c\n",
    "                        INNER JOIN claim_lender l\n",
    "                            USING (claim_id)\n",
    "                        INNER JOIN claim_borrower b\n",
    "                            USING (claim_id)\n",
    "                        INNER JOIN claim_loan cl\n",
    "                            USING (claim_id)\n",
    "                        INNER JOIN claim_calculations cc\n",
    "                            USING (claim_id)\n",
    "                        INNER JOIN claim_vehicle v\n",
    "                            USING (claim_id)\n",
    "                        INNER JOIN claims_loss_type cls\n",
    "                            USING (loss_type_id)\n",
    "                        INNER JOIN carriers ca\n",
    "                            ON c.carrier_id = ca.carrier_id                        \n",
    "                        INNER JOIN claim_checklist AS cc26\n",
    "                            ON c.claim_id = cc26.claim_id\n",
    "                            AND cc26.checklist_item_id = 26    \n",
    "                        INNER JOIN claim_checklist AS cc14\n",
    "                            ON c.claim_id = cc14.claim_id\n",
    "                            AND cc14.checklist_item_id = 14    \n",
    "                        INNER JOIN claim_checklist AS cc15\n",
    "                            ON c.claim_id = cc15.claim_id\n",
    "                            AND cc15.checklist_item_id = 15    \n",
    "                        INNER JOIN claim_checklist AS cc16\n",
    "                            ON c.claim_id = cc16.claim_id\n",
    "                            AND cc16.checklist_item_id = 16       \n",
    "                        INNER JOIN claim_checklist AS cc17\n",
    "                            ON c.claim_id = cc17.claim_id\n",
    "                            AND cc17.checklist_item_id = 17       \n",
    "                        INNER JOIN claim_checklist AS cc5\n",
    "                            ON c.claim_id = cc5.claim_id\n",
    "                            AND cc5.checklist_item_id = 5\n",
    "                        INNER JOIN claim_checklist AS cc7\n",
    "                            ON c.claim_id = cc7.claim_id\n",
    "                            AND cc7.checklist_item_id = 7\n",
    "                        INNER JOIN claim_checklist AS cc8\n",
    "                            ON c.claim_id = cc8.claim_id\n",
    "                            AND cc8.checklist_item_id = 8\n",
    "                        INNER JOIN claim_checklist AS cc9\n",
    "                            ON c.claim_id = cc9.claim_id\n",
    "                            AND cc9.checklist_item_id = 9\n",
    "                        INNER JOIN claim_checklist AS cc10\n",
    "                            ON c.claim_id = cc10.claim_id\n",
    "                            AND cc10.checklist_item_id = 10\n",
    "                        INNER JOIN claim_checklist AS cc11\n",
    "                            ON c.claim_id = cc11.claim_id\n",
    "                            AND cc11.checklist_item_id = 11      \n",
    "                        INNER JOIN claim_checklist AS cc18\n",
    "                            ON c.claim_id = cc18.claim_id\n",
    "                            AND cc18.checklist_item_id = 18\n",
    "                        INNER JOIN claim_checklist AS cc19\n",
    "                            ON c.claim_id = cc19.claim_id\n",
    "                            AND cc19.checklist_item_id = 19\n",
    "                        INNER JOIN claim_checklist AS cc20\n",
    "                            ON c.claim_id = cc20.claim_id\n",
    "                            AND cc20.checklist_item_id = 20     \n",
    "                        INNER JOIN claim_checklist AS cc21\n",
    "                            ON c.claim_id = cc21.claim_id\n",
    "                            AND cc21.checklist_item_id = 21\n",
    "                        INNER JOIN claim_checklist AS cc22\n",
    "                            ON c.claim_id = cc22.claim_id\n",
    "                            AND cc22.checklist_item_id = 22  \n",
    "                        WHERE c.claim_id = {claimID};'''.format(claimID = cal)\n",
    "\n",
    "                # save query results as DF\n",
    "                c_temp_df = pd.DataFrame(mysql_q(vgc_u, vgc_pw, vgc_host, 'visualgap_claims', c_sql_query, 0, 0))\n",
    "\n",
    "                # append query result to c_template_df\n",
    "                c_consolidate_df = c_consolidate_df.append(c_temp_df)\n",
    "\n",
    "            cal_cols = {0:'claim_nbr', 1:'loss_date', 2:'alt_name', 3:'contact', 4:'first', 5:'last', 6:'gap_payable', 7:'incp_date', 8:'last_payment', 9:'interest_rate', 10:'Amt_Fin',\n",
    "                11:'balance_last_pay', 12:'nbr_of_days', 13:'per_day', 14:'payoff', 15:'ltv', 16:'covered_fin_amount', 17:'percent_uncovered', 18:'ltv_limit', 19:'nada_value',\n",
    "                20:'vehicle', 21:'deductible', 22:'loss_type', 23:'primary_carrier', 24:'past_due', 25:'late_fees', 26:'skip_pymts', 27:'skip_fees', 28:'primary_pymt',\n",
    "                29:'excess_deductible', 30:'scr', 31:'clr', 32:'cdr', 33:'oref', 34:'salvage', 35:'prior_dmg', 36:'over_ltv', 37:'other1_amt', 38:'other1_description',\n",
    "                39:'other2_amt', 40:'other2_description', 41:'carrier', 42:'subtotal'}\n",
    "\n",
    "            c_template_df = c_consolidate_df.rename(columns = cal_cols)\n",
    "            c_template_df = c_template_df.reset_index(drop=True)\n",
    "\n",
    "            # Create Calculations\n",
    "            c_pdf_template = \"letters/pdf_templates/GAP_calculation_template.pdf\"\n",
    "            position = 3\n",
    "            calculations(c_template_df, c_pdf_template, position)\n",
    "                \n",
    "        # Close SQL Connection\n",
    "\n",
    "        # create a list of file to concatenate\n",
    "        file_list = fileList(file_staging_dir, file_ext)\n",
    "\n",
    "        # Concatenated output file\n",
    "        outfn = f'S:/claims/letters/{carrier}_{now.strftime(\"%Y-%m-%d\")}_GAP'\n",
    "\n",
    "        ConCat_pdf(file_list, outfn)\n",
    "\n",
    "        # Remove files from staging directory\n",
    "        clear_dir(file_staging_dir, file_ext)\n",
    "\n",
    "else: print('No amount greater then 0.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update toVGC to 1\n",
    "update_tovgc_1(checks_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Plus Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect GAP Plus\n",
    "plus_pymts_df = pymts_df.loc[pymts_df['payment_category_id'] == 2].copy()\n",
    "plus_letters_df = plus_pymts_df.loc[plus_pymts_df['payment_amount'] > 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['payment_amount'] = template_df['payment_amount'].map('${:,.2f}'.format)\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['loss_date'] = pd.to_datetime(template_df['loss_date']).dt.strftime('%B %d, %Y')\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['StateDesc'] = template_df['StateDesc'].astype(str).replace({'None':''})\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['StateCode'] = template_df['StateCode'].astype(str).replace({'None':''})\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['f_lang'] = template_df['f_lang'].astype(str).replace({'None':''})\n"
     ]
    }
   ],
   "source": [
    "# Filter plus_letters_df for pymt_method 'Check'\n",
    "plus_df = plus_letters_df.loc[plus_letters_df['pymt_method'] == 'Check'].copy()\n",
    "\n",
    "if len(plus_df.index) > 0:\n",
    "\n",
    "    # Create list of Carriers on checks_df\n",
    "    p_carriers = []\n",
    "    p_carriers = plus_df.carrier.unique()\n",
    "\n",
    "    # Loop through carriers list\n",
    "    for p_carrier in p_carriers:\n",
    "\n",
    "        # Variable Defaults\n",
    "        p_letters = True\n",
    "\n",
    "        # check for payment_type_id for GAP letters by carrier\n",
    "        p_letters_df = plus_df.loc[plus_df['carrier'] == p_carrier].copy()\n",
    "\n",
    "        # check for records\n",
    "        if len(p_letters_df.index) > 0:\n",
    "            p_letters = True\n",
    "        else:\n",
    "            # No Letters\n",
    "            p_letters = False\n",
    "\n",
    "        # PLUS - Create letters\n",
    "        if p_letters == True:\n",
    "            # create df for template\n",
    "            p_template_df = p_letters_df[letter_cols]\n",
    "\n",
    "            # Create GAP Letters\n",
    "            p_pdf_template = \"letters/pdf_templates/PLUS_letter_template.pdf\"\n",
    "            position = 1\n",
    "            gap_letter(p_template_df, p_pdf_template, position)\n",
    "        \n",
    "            file_list = fileList(file_staging_dir, file_ext)\n",
    "\n",
    "            # Concatenated output file\n",
    "            outfn = f'S:/claims/letters/{p_carrier}_{now.strftime(\"%Y-%m-%d\")}_PLUS'\n",
    "\n",
    "            ConCat_pdf(file_list, outfn)\n",
    "\n",
    "        # Remove files from staging directory\n",
    "        clear_dir(file_staging_dir, file_ext)\n",
    "\n",
    "else: print('No Plus.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update toVGC to 1\n",
    "update_tovgc_1(plus_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create TotalRestart Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect GAP Plus\n",
    "tr_pymts_df = pymts_df.loc[pymts_df['payment_category_id'] == 3].copy()\n",
    "tr_letters_df = tr_pymts_df.loc[tr_pymts_df['payment_amount'] > 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['payment_amount'] = template_df['payment_amount'].map('${:,.2f}'.format)\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['loss_date'] = pd.to_datetime(template_df['loss_date']).dt.strftime('%B %d, %Y')\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['StateDesc'] = template_df['StateDesc'].astype(str).replace({'None':''})\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['StateCode'] = template_df['StateCode'].astype(str).replace({'None':''})\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['f_lang'] = template_df['f_lang'].astype(str).replace({'None':''})\n"
     ]
    }
   ],
   "source": [
    "# Filter tr_letters_df for pymt_method 'Check'\n",
    "tr_df = tr_letters_df.loc[tr_letters_df['pymt_method'] == 'Check'].copy()\n",
    "\n",
    "if len(tr_df.index) > 0:\n",
    "    # Create list of Carriers on checks_df\n",
    "    tr_carriers = []\n",
    "    tr_carriers = tr_df.carrier.unique()\n",
    "\n",
    "    # Loop through carriers list\n",
    "    for tr_carrier in tr_carriers:\n",
    "\n",
    "        # Variable Defaults\n",
    "        tr_sql_where_cal = ''\n",
    "        tr_letters = True\n",
    "        tr_s_letters = True\n",
    "        tr_cals = []\n",
    "\n",
    "        # check for payment_type_id for TR letters by carrier\n",
    "        tr_letters_df = tr_df.loc[(tr_df['pymt_type_id'] == 1) & (tr_df['carrier'] == tr_carrier)].copy()\n",
    "\n",
    "        # TR Letter - create WHERE statement\n",
    "        if len(tr_letters_df.index) > 0:\n",
    "            tr_letters = True\n",
    "        else:\n",
    "            # No Letters\n",
    "            tr_letters = False\n",
    "\n",
    "        # check for payment_type_id for Supplemental letters\n",
    "        tr_s_letters_df = tr_df.loc[(tr_df['pymt_type_id'] == 2) & (tr_df['carrier'] == tr_carrier)].copy()\n",
    "\n",
    "        # Supplemental Letter\n",
    "        if len(tr_s_letters_df.index) > 0:\n",
    "            tr_s_letters = True\n",
    "        else:\n",
    "            # No Letters\n",
    "            tr_s_letters = False\n",
    "\n",
    "        # Calculations\n",
    "        # check for calculations for carrier\n",
    "        tr_calculations_df = tr_df.loc[tr_df['carrier'] == tr_carrier].copy()\n",
    "\n",
    "        if len(tr_calculations_df.index) > 0:\n",
    "            # Multiple Calculation\n",
    "            for index, row in tr_calculations_df.iterrows():\n",
    "                tr_cals.append(row['claim_id'])\n",
    "\n",
    "        # TR - Create letters\n",
    "        if tr_letters == True:\n",
    "            # create df for template\n",
    "            tr_template_df = tr_letters_df[letter_cols]\n",
    "\n",
    "            # Create TR Letters\n",
    "            tr_pdf_template = \"letters/pdf_templates/TR_letter_template.pdf\"\n",
    "            position = 1\n",
    "            gap_letter(tr_template_df, tr_pdf_template, position)\n",
    "        \n",
    "        # Supplemental - Create letters\n",
    "        if tr_s_letters == True:\n",
    "            # create df for template\n",
    "            tr_s_template_df = tr_s_letters_df[letter_cols]\n",
    "            \n",
    "            # Create Supplement Letters\n",
    "            tr_s_pdf_template = \"letters/pdf_templates/TR_letter_template.pdf\"\n",
    "            position = 2\n",
    "            gap_letter(tr_s_template_df, tr_s_pdf_template, position)\n",
    "\n",
    "        # Calculations - Create SQL query, run query, create calculation sheets\n",
    "        if tr_letters == True or tr_s_letters == True:\n",
    "\n",
    "            # create calculation dataframe\n",
    "            temp_cols = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]                             \n",
    "            tr_consolidate_df = pd.DataFrame(columns = temp_cols)\n",
    "\n",
    "            for tr_cal in tr_cals:\n",
    "                # create sql for calculation\n",
    "                tr_sql_query = '''\n",
    "                        SELECT c.claim_nbr, c.loss_date, l.alt_name, l.contact, b.first, b.last, cc.gap_payable,\n",
    "                            cl.incp_date, v.nada_value, CONCAT(v.year, ' ', v.make, ' ', v.model) AS vehicle,\n",
    "                            cls.description AS loss_type, cc26.description AS primary_carrier, tr.max_benefit,\n",
    "                            cc5.amount AS primary_pymt, cc7.amount AS excess_deductible, tr.term, tr.totalrestart_payable,\n",
    "                            cc8.amount AS scr, cc9.amount AS clr, cc10.amount AS cdr, cc11.amount AS oref,\n",
    "                            cc18.amount AS salvage, cc19.amount AS prior_dmg, \n",
    "                            cc21.amount AS other1_amt, cc21.description AS other1_description,\n",
    "                            cc22.amount AS other2_amt, cc22.description AS other2_description, \n",
    "                            cc35.amount AS other3_amt, cc35.description AS other3_description, ca.description AS carrier,\n",
    "                            (cc5.amount + cc7.amount + cc18.amount + cc19.amount + cc.gap_payable + cc8.amount + cc8.amount + \n",
    "                            cc9.amount + cc10.amount + cc11.amount + cc21.amount + cc22.amount + cc35.amount) as subtotal\n",
    "                        FROM claims c\n",
    "                        INNER JOIN claim_lender l\n",
    "                            USING (claim_id)\n",
    "                        INNER JOIN claim_borrower b\n",
    "                            USING (claim_id)\n",
    "                        INNER JOIN claim_loan cl\n",
    "                            USING (claim_id)\n",
    "                        INNER JOIN claim_calculations cc\n",
    "                            USING (claim_id)\n",
    "                        INNER JOIN claim_vehicle v\n",
    "                            USING (claim_id)\n",
    "                        INNER JOIN claims_loss_type cls\n",
    "                            USING (loss_type_id)\n",
    "                        INNER JOIN carriers ca\n",
    "                            ON c.carrier_id = ca.carrier_id                        \n",
    "                        INNER JOIN claim_checklist AS cc26\n",
    "                            ON c.claim_id = cc26.claim_id\n",
    "                            AND cc26.checklist_item_id = 26                  \n",
    "                        INNER JOIN claim_checklist AS cc5\n",
    "                            ON c.claim_id = cc5.claim_id\n",
    "                            AND cc5.checklist_item_id = 5\n",
    "                        INNER JOIN claim_checklist AS cc7\n",
    "                            ON c.claim_id = cc7.claim_id\n",
    "                            AND cc7.checklist_item_id = 7\n",
    "                        INNER JOIN claim_checklist AS cc8\n",
    "                            ON c.claim_id = cc8.claim_id\n",
    "                            AND cc8.checklist_item_id = 8\n",
    "                        INNER JOIN claim_checklist AS cc9\n",
    "                            ON c.claim_id = cc9.claim_id\n",
    "                            AND cc9.checklist_item_id = 9\n",
    "                        INNER JOIN claim_checklist AS cc10\n",
    "                            ON c.claim_id = cc10.claim_id\n",
    "                            AND cc10.checklist_item_id = 10\n",
    "                        INNER JOIN claim_checklist AS cc11\n",
    "                            ON c.claim_id = cc11.claim_id\n",
    "                            AND cc11.checklist_item_id = 11      \n",
    "                        INNER JOIN claim_checklist AS cc18\n",
    "                            ON c.claim_id = cc18.claim_id\n",
    "                            AND cc18.checklist_item_id = 18\n",
    "                        INNER JOIN claim_checklist AS cc19\n",
    "                            ON c.claim_id = cc19.claim_id\n",
    "                            AND cc19.checklist_item_id = 19     \n",
    "                        INNER JOIN claim_checklist AS cc21\n",
    "                            ON c.claim_id = cc21.claim_id\n",
    "                            AND cc21.checklist_item_id = 21\n",
    "                        INNER JOIN claim_checklist AS cc22\n",
    "                            ON c.claim_id = cc22.claim_id\n",
    "                            AND cc22.checklist_item_id = 22\n",
    "                        INNER JOIN claim_checklist AS cc35\n",
    "                            ON c.claim_id = cc35.claim_id\n",
    "                            AND cc35.checklist_item_id = 35\n",
    "                        INNER JOIN claim_totalrestart AS tr\n",
    "                            ON c.claim_id = tr.claim_id\n",
    "                        WHERE c.claim_id = {claimID};'''.format(claimID = tr_cal)\n",
    "\n",
    "                # save query results as DF\n",
    "                tr_temp_df = pd.DataFrame(mysql_q(vgc_u, vgc_pw, vgc_host, 'visualgap_claims', tr_sql_query, 0, 0))\n",
    "\n",
    "                # append query result to c_template_df\n",
    "                tr_consolidate_df = tr_consolidate_df.append(tr_temp_df)\n",
    "\n",
    "            cal_cols = {0:'claim_nbr', 1:'loss_date', 2:'alt_name', 3:'contact', 4:'first', 5:'last', 6:'gap_payable', 7:'incp_date', 8:'nada_value',\n",
    "                9:'vehicle', 10:'loss_type', 11:'primary_carrier', 12:'max_benefit', 13:'primary_pymt', 14:'excess_deductible', 15:'term', 16:'totalrestart_payable', \n",
    "                17:'scr', 18:'clr', 19:'cdr', 20:'oref', 21:'salvage', 22:'prior_dmg', 23:'other1_amt', 24:'other1_description',\n",
    "                25:'other2_amt', 26:'other2_description', 27:'other3_amt', 28:'other3_description', 29:'carrier', 30:'subtotal'}\n",
    "\n",
    "            tr_template_df = tr_consolidate_df.rename(columns = cal_cols)\n",
    "            tr_template_df = tr_template_df.reset_index(drop=True)\n",
    "\n",
    "            # Create Calculations\n",
    "            tr_pdf_template = \"letters/pdf_templates/TR_calculation_template.pdf\"\n",
    "            position = 3\n",
    "            tr_calculations(tr_template_df, tr_pdf_template, position)\n",
    "\n",
    "        # create \n",
    "        file_list = fileList(file_staging_dir, file_ext)\n",
    "\n",
    "        # Concatenated output file\n",
    "        outfn = f'S:/claims/letters/TOTALRESTART_{now.strftime(\"%Y-%m-%d\")}'\n",
    "\n",
    "        ConCat_pdf(file_list, outfn)\n",
    "\n",
    "        # Remove files from staging directory\n",
    "        clear_dir(file_staging_dir, file_ext)\n",
    "\n",
    "else: print('No amount greater then 0.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update toVGC to 1\n",
    "update_tovgc_1(tr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAP Claims paid via ACH and $0 claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAP Claims paid via ACH and $0 Claims\n",
    "ach_0_df = gap_pymts_df.loc[(gap_pymts_df['payment_amount'] <= 0) | (gap_pymts_df['pymt_method'].str.contains('ACH'))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['payment_amount'] = template_df['payment_amount'].map('${:,.2f}'.format)\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['loss_date'] = pd.to_datetime(template_df['loss_date']).dt.strftime('%B %d, %Y')\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['StateDesc'] = template_df['StateDesc'].astype(str).replace({'None':''})\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['StateCode'] = template_df['StateCode'].astype(str).replace({'None':''})\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['f_lang'] = template_df['f_lang'].astype(str).replace({'None':''})\n"
     ]
    }
   ],
   "source": [
    "# attachment directory\n",
    "attachment_dir = 'S:/claims/letters/attachment/'\n",
    "\n",
    "# Remove files from staging directory\n",
    "clear_dir(file_staging_dir, file_ext) \n",
    "clear_dir(attachment_dir, file_ext)\n",
    "\n",
    "if len(ach_0_df.index) > 0:\n",
    "    letter_cols = ['claim_nbr', 'loss_date', 'alt_name', 'contact', 'address1', 'city', 'state', 'zip', 'first', 'last', 'acct_number', 'payment_amount', 'StateDesc', 'StateCode', 'f_lang' ]\n",
    "    temp_cols = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]\n",
    "\n",
    "    for index, row in ach_0_df.iterrows():\n",
    "        # Check for GAP or Supplemental\n",
    "        if row['pymt_type_id'] == 1:\n",
    "            # GAP\n",
    "            letter_df = ach_0_df.loc[ach_0_df['claim_id'] == row['claim_id']].copy()       \n",
    "            template_df = letter_df[letter_cols]\n",
    "            pdf_template = \"letters/pdf_templates/GAP_letter_template.pdf\"\n",
    "            position = 1\n",
    "            claim_type = 'GAP Claim'\n",
    "        else:\n",
    "            # Supplemental\n",
    "            letter_df = ach_0_df.loc[ach_0_df['claim_id'] == row['claim_id']].copy()       \n",
    "            template_df = letter_df[letter_cols]\n",
    "            pdf_template = \"letters/pdf_templates/Supp_letter_template.pdf\"\n",
    "            position = 2\n",
    "            claim_type = 'Supplemental GAP Claim'\n",
    "\n",
    "        gap_letter(template_df, pdf_template, position)\n",
    "\n",
    "        # Calculation\n",
    "        c_consolidate_df = pd.DataFrame(columns = temp_cols)\n",
    "\n",
    "        c_sql_query = '''\n",
    "        SELECT c.claim_nbr, c.loss_date, l.alt_name, l.contact, b.first, b.last, cc.gap_payable,\n",
    "            cl.incp_date, cl.last_payment, cl.interest_rate, cl.amount AS Amt_Fin, cc.balance_last_pay,\n",
    "            cc.nbr_of_days, cc.per_day, ROUND(cc.payoff,2) AS payoff, (cc.ltv * 100) AS ltv, cc.covered_fin_amount,\n",
    "            (cc.percent_uncovered * 100) AS percent_uncovered, (cc.ltv_limit * 100) AS ltv_limit, v.nada_value, CONCAT(v.year, ' ', v.make, ' ', v.model) AS vehicle,\n",
    "            v.deductible, cls.description AS loss_type, cc26.description AS primary_carrier,\n",
    "            cc14.amount AS past_due, cc15.amount AS late_fees, cc16.amount AS skip_pymts,\n",
    "            cc17.amount AS skip_fees, cc5.amount AS primary_pymt, cc7.amount AS excess_deductible,\n",
    "            cc8.amount AS scr, cc9.amount AS clr, cc10.amount AS cdr, cc11.amount AS oref,\n",
    "            cc18.amount AS salvage, cc19.amount AS prior_dmg, cc20.amount AS over_ltv,\n",
    "            cc21.amount AS other1_amt, cc21.description AS other1_description,\n",
    "            cc22.amount AS other2_amt, cc22.description AS other2_description, ca.description AS carrier,\n",
    "            (cc22.amount + cc21.amount + cc20.amount + cc19.amount + cc18.amount + cc11.amount + cc10.amount + cc9.amount + \n",
    "            cc8.amount + cc7.amount + cc5.amount + cc17.amount + cc16.amount + cc15.amount + cc14.amount) as subtotal\n",
    "        FROM claims c\n",
    "        INNER JOIN claim_lender l\n",
    "            USING (claim_id)\n",
    "        INNER JOIN claim_borrower b\n",
    "            USING (claim_id)\n",
    "        INNER JOIN claim_loan cl\n",
    "            USING (claim_id)\n",
    "        INNER JOIN claim_calculations cc\n",
    "            USING (claim_id)\n",
    "        INNER JOIN claim_vehicle v\n",
    "            USING (claim_id)\n",
    "        INNER JOIN claims_loss_type cls\n",
    "            USING (loss_type_id)\n",
    "        INNER JOIN carriers ca\n",
    "            ON c.carrier_id = ca.carrier_id                        \n",
    "        INNER JOIN claim_checklist AS cc26\n",
    "            ON c.claim_id = cc26.claim_id\n",
    "            AND cc26.checklist_item_id = 26    \n",
    "        INNER JOIN claim_checklist AS cc14\n",
    "            ON c.claim_id = cc14.claim_id\n",
    "            AND cc14.checklist_item_id = 14    \n",
    "        INNER JOIN claim_checklist AS cc15\n",
    "            ON c.claim_id = cc15.claim_id\n",
    "            AND cc15.checklist_item_id = 15    \n",
    "        INNER JOIN claim_checklist AS cc16\n",
    "            ON c.claim_id = cc16.claim_id\n",
    "            AND cc16.checklist_item_id = 16       \n",
    "        INNER JOIN claim_checklist AS cc17\n",
    "            ON c.claim_id = cc17.claim_id\n",
    "            AND cc17.checklist_item_id = 17       \n",
    "        INNER JOIN claim_checklist AS cc5\n",
    "            ON c.claim_id = cc5.claim_id\n",
    "            AND cc5.checklist_item_id = 5\n",
    "        INNER JOIN claim_checklist AS cc7\n",
    "            ON c.claim_id = cc7.claim_id\n",
    "            AND cc7.checklist_item_id = 7\n",
    "        INNER JOIN claim_checklist AS cc8\n",
    "            ON c.claim_id = cc8.claim_id\n",
    "            AND cc8.checklist_item_id = 8\n",
    "        INNER JOIN claim_checklist AS cc9\n",
    "            ON c.claim_id = cc9.claim_id\n",
    "            AND cc9.checklist_item_id = 9\n",
    "        INNER JOIN claim_checklist AS cc10\n",
    "            ON c.claim_id = cc10.claim_id\n",
    "            AND cc10.checklist_item_id = 10\n",
    "        INNER JOIN claim_checklist AS cc11\n",
    "            ON c.claim_id = cc11.claim_id\n",
    "            AND cc11.checklist_item_id = 11      \n",
    "        INNER JOIN claim_checklist AS cc18\n",
    "            ON c.claim_id = cc18.claim_id\n",
    "            AND cc18.checklist_item_id = 18\n",
    "        INNER JOIN claim_checklist AS cc19\n",
    "            ON c.claim_id = cc19.claim_id\n",
    "            AND cc19.checklist_item_id = 19\n",
    "        INNER JOIN claim_checklist AS cc20\n",
    "            ON c.claim_id = cc20.claim_id\n",
    "            AND cc20.checklist_item_id = 20     \n",
    "        INNER JOIN claim_checklist AS cc21\n",
    "            ON c.claim_id = cc21.claim_id\n",
    "            AND cc21.checklist_item_id = 21\n",
    "        INNER JOIN claim_checklist AS cc22\n",
    "            ON c.claim_id = cc22.claim_id\n",
    "            AND cc22.checklist_item_id = 22  \n",
    "        WHERE c.claim_id = {claimID};'''.format(claimID = row['claim_id'])\n",
    "\n",
    "        # # save query results as DF\n",
    "        c_temp_df = pd.DataFrame(mysql_q(vgc_u, vgc_pw, vgc_host, 'visualgap_claims', c_sql_query, 0, 0))\n",
    "\n",
    "        # append query result to c_template_df\n",
    "        c_consolidate_df = c_consolidate_df.append(c_temp_df)\n",
    "\n",
    "        cal_cols = {0:'claim_nbr', 1:'loss_date', 2:'alt_name', 3:'contact', 4:'first', 5:'last', 6:'gap_payable', 7:'incp_date', 8:'last_payment', 9:'interest_rate', 10:'Amt_Fin',\n",
    "            11:'balance_last_pay', 12:'nbr_of_days', 13:'per_day', 14:'payoff', 15:'ltv', 16:'covered_fin_amount', 17:'percent_uncovered', 18:'ltv_limit', 19:'nada_value',\n",
    "            20:'vehicle', 21:'deductible', 22:'loss_type', 23:'primary_carrier', 24:'past_due', 25:'late_fees', 26:'skip_pymts', 27:'skip_fees', 28:'primary_pymt',\n",
    "            29:'excess_deductible', 30:'scr', 31:'clr', 32:'cdr', 33:'oref', 34:'salvage', 35:'prior_dmg', 36:'over_ltv', 37:'other1_amt', 38:'other1_description',\n",
    "            39:'other2_amt', 40:'other2_description', 41:'carrier', 42:'subtotal'}\n",
    "\n",
    "        c_template_df = c_consolidate_df.rename(columns = cal_cols)\n",
    "        c_template_df = c_template_df.reset_index(drop=True)\n",
    "\n",
    "        # Create Calculations\n",
    "        c_pdf_template = \"letters/pdf_templates/GAP_calculation_template.pdf\"\n",
    "        position = 3\n",
    "        calculations(c_template_df, c_pdf_template, position)\n",
    "\n",
    "        # create a list of file to concatenate\n",
    "        file_list = fileList(file_staging_dir, file_ext)\n",
    "\n",
    "        # Concatenated output file\n",
    "        outfn = attachment_dir + row['claim_nbr'] + '-' + row['last']\n",
    "        \n",
    "        ConCat_pdf(file_list, outfn)\n",
    "\n",
    "        # send Email\n",
    "        attach_name = row['claim_nbr'] + '-' + row['last'] + \".pdf\"\n",
    "        missingEmail = 0\n",
    "        toEmail = []\n",
    "\n",
    "        if row['email'] == '' and row['email2'] == '':\n",
    "            toEmail.append('claims@visualgap.com')\n",
    "            missingEmail = 1\n",
    "        elif row['email'] == '':\n",
    "            toEmail.append(row['email2'])\n",
    "        elif row['email2'] == '':\n",
    "            toEmail.append(row['email'])\n",
    "        else:\n",
    "            toEmail.append(row['email'])\n",
    "            toEmail.append(row['email2'])\n",
    "\n",
    "        if missingEmail == 0:\n",
    "            msg_html = \"\"\"\n",
    "            <html>\n",
    "            <body>\n",
    "                <p>Hello {}:<br>\n",
    "                <br>\n",
    "                We completed the {} for {}'s loan.  Attached is claim letter and calculation. <br>\n",
    "                <br>\n",
    "                Thank you, <br>\n",
    "                Claims Department <br>\n",
    "                <br>\n",
    "                <b>Frost Financial Services, Inc. | VisualGAP <br>\n",
    "                Claims Department <br>\n",
    "                Phone: 888-753-7678 Option 3</b>\n",
    "                </p>\n",
    "            </body>\n",
    "            </html>\n",
    "            \"\"\".format(row['contact'], claim_type, (row['first'] + ' ' + row['last']))\n",
    "        else:\n",
    "            msg_html = \"\"\"\n",
    "            <html>\n",
    "            <body>\n",
    "                <p>There is not an email address on claim {}:<br>\n",
    "                <br>\n",
    "                Please fax or mail the attached to {}. <br>\n",
    "                <br>\n",
    "                Thank you, <br>\n",
    "                Claims Department <br>\n",
    "                <br>\n",
    "                <b>Frost Financial Services, Inc. | VisualGAP <br>\n",
    "                Claims Department <br>\n",
    "                Phone: 888-753-7678 Option 3</b>\n",
    "                </p>\n",
    "            </body>\n",
    "            </html>\n",
    "            \"\"\".format(row['claim_nbr'], row['alt_name'])\n",
    "\n",
    "        send_email(toEmail, claim_type, msg_html, attachment_dir, attach_name)\n",
    "\n",
    "        # Remove files from staging directory\n",
    "        clear_dir(attachment_dir, file_ext)\n",
    "        clear_dir(file_staging_dir, file_ext)     \n",
    "\n",
    "else: print('No ACH and/or $0 claims.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update toVGC to 1\n",
    "update_tovgc_1(ach_0_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plus Claims paid via ACH and $0 Claims\n",
    "ach_plus_df = pymts_df.loc[(pymts_df['payment_category_id'] == 2) & ((pymts_df['payment_amount'] <= 0) | (pymts_df['pymt_method'].str.contains('ACH')))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['payment_amount'] = template_df['payment_amount'].map('${:,.2f}'.format)\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['loss_date'] = pd.to_datetime(template_df['loss_date']).dt.strftime('%B %d, %Y')\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['StateDesc'] = template_df['StateDesc'].astype(str).replace({'None':''})\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['StateCode'] = template_df['StateCode'].astype(str).replace({'None':''})\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['f_lang'] = template_df['f_lang'].astype(str).replace({'None':''})\n"
     ]
    }
   ],
   "source": [
    "# Remove files from staging directory\n",
    "clear_dir(file_staging_dir, file_ext) \n",
    "clear_dir(attachment_dir, file_ext)\n",
    "\n",
    "letter_cols = ['claim_nbr', 'loss_date', 'alt_name', 'contact', 'address1', 'city', 'state', 'zip', 'first', 'last', 'acct_number', 'payment_amount', 'StateDesc', 'StateCode', 'f_lang' ]\n",
    "\n",
    "if len(ach_plus_df.index) > 0:\n",
    "\n",
    "    for index, row in ach_plus_df.iterrows():\n",
    "\n",
    "        # create df for template\n",
    "        letter_df = ach_plus_df.loc[ach_plus_df['claim_id'] == row['claim_id']].copy()\n",
    "        template_df = letter_df[letter_cols]\n",
    "        pdf_template = \"letters/pdf_templates/PLUS_letter_template.pdf\"\n",
    "        position = 1\n",
    "        claim_type = \"GAP Plus Claim\"\n",
    "\n",
    "        # Create Plus Letter\n",
    "        gap_letter(template_df, pdf_template, position)\n",
    "\n",
    "        s_dir = f\"{file_staging_dir}{row['claim_nbr']}-{position}.pdf\"\n",
    "        a_dir = f\"{attachment_dir}{row['claim_nbr']}-{row['last']}.pdf\"\n",
    "        # Move and rename file to Attachment directory   \n",
    "        shutil.move(s_dir, a_dir)\n",
    "\n",
    "        # send Email\n",
    "        attach_name = f\"{row['claim_nbr']}-{row['last']}.pdf\"\n",
    "        missingEmail = 0\n",
    "        toEmail = []\n",
    "\n",
    "        if row['email'] == '' and row['email2'] == '':\n",
    "            toEmail.append('claims@visualgap.com')\n",
    "            missingEmail = 1\n",
    "        elif row['email'] == '':\n",
    "            toEmail.append(row['email2'])\n",
    "        elif row['email2'] == '':\n",
    "            toEmail.append(row['email'])\n",
    "        else:\n",
    "            toEmail.append(row['email'])\n",
    "            toEmail.append(row['email2'])\n",
    "\n",
    "        if missingEmail == 0:\n",
    "            msg_html = \"\"\"\n",
    "            <html>\n",
    "            <body>\n",
    "                <p>Hello {}:<br>\n",
    "                <br>\n",
    "                We completed the {} for {}'s loan.  Attached is claim letter and calculation. <br>\n",
    "                <br>\n",
    "                Thank you, <br>\n",
    "                Claims Department <br>\n",
    "                <br>\n",
    "                <b>Frost Financial Services, Inc. | VisualGAP <br>\n",
    "                Claims Department <br>\n",
    "                Phone: 888-753-7678 Option 3</b>\n",
    "                </p>\n",
    "            </body>\n",
    "            </html>\n",
    "            \"\"\".format(row['contact'], claim_type, (row['first'] + ' ' + row['last']))\n",
    "        else:\n",
    "            msg_html = \"\"\"\n",
    "            <html>\n",
    "            <body>\n",
    "                <p>There is not an email address on claim {}:<br>\n",
    "                <br>\n",
    "                Please fax or mail the attached to {}. <br>\n",
    "                <br>\n",
    "                Thank you, <br>\n",
    "                Claims Department <br>\n",
    "                <br>\n",
    "                <b>Frost Financial Services, Inc. | VisualGAP <br>\n",
    "                Claims Department <br>\n",
    "                Phone: 888-753-7678 Option 3</b>\n",
    "                </p>\n",
    "            </body>\n",
    "            </html>\n",
    "            \"\"\".format(row['claim_nbr'], row['alt_name'])\n",
    "\n",
    "        send_email(toEmail, claim_type, msg_html, attachment_dir, attach_name)\n",
    "\n",
    "        # Remove files from staging directory\n",
    "        clear_dir(attachment_dir, file_ext)\n",
    "\n",
    "else: print('No ACH Plus.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update toVGC to 1\n",
    "update_tovgc_1(ach_plus_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plus Claims paid via ACH and $0 Claims\n",
    "tr_ach_0_df = pymts_df.loc[(pymts_df['payment_category_id'] == 3) & ((pymts_df['payment_amount'] <= 0) | (pymts_df['pymt_method'].str.contains('ACH')))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['payment_amount'] = template_df['payment_amount'].map('${:,.2f}'.format)\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['loss_date'] = pd.to_datetime(template_df['loss_date']).dt.strftime('%B %d, %Y')\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['StateDesc'] = template_df['StateDesc'].astype(str).replace({'None':''})\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['StateCode'] = template_df['StateCode'].astype(str).replace({'None':''})\n",
      "C:\\Users\\jbehler\\AppData\\Local\\Temp/ipykernel_9680/1898802128.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_df['f_lang'] = template_df['f_lang'].astype(str).replace({'None':''})\n"
     ]
    }
   ],
   "source": [
    "# Remove files from staging directory\n",
    "clear_dir(file_staging_dir, file_ext) \n",
    "clear_dir(attachment_dir, file_ext)\n",
    "\n",
    "if len(tr_ach_0_df.index) > 0:\n",
    "    letter_cols = ['claim_nbr', 'loss_date', 'alt_name', 'contact', 'address1', 'city', 'state', 'zip', 'first', 'last', 'acct_number', 'payment_amount', 'StateDesc', 'StateCode', 'f_lang' ]\n",
    "    temp_cols = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]\n",
    "\n",
    "    for index, row in tr_ach_0_df.iterrows():\n",
    "        # Check for GAP or Supplemental\n",
    "        if row['pymt_type_id'] == 1:\n",
    "            # GAP\n",
    "            letter_df = tr_ach_0_df.loc[tr_ach_0_df['claim_id'] == row['claim_id']].copy()       \n",
    "            template_df = letter_df[letter_cols]\n",
    "            pdf_template = \"letters/pdf_templates/TR_letter_template.pdf\"\n",
    "            position = 1\n",
    "            claim_type = 'TotalRestart Claim'\n",
    "        else:\n",
    "            # Supplemental\n",
    "            letter_df = ach_0_df.loc[ach_0_df['claim_id'] == row['claim_id']].copy()       \n",
    "            template_df = letter_df[letter_cols]\n",
    "            pdf_template = \"letters/pdf_templates/TR_letter_template.pdf\"\n",
    "            position = 2\n",
    "            claim_type = 'Supplemental TotalRestart Claim'\n",
    "\n",
    "        gap_letter(template_df, pdf_template, position)\n",
    "\n",
    "        # Calculation\n",
    "        c_consolidate_df = pd.DataFrame(columns = temp_cols)\n",
    "\n",
    "        c_sql_query = '''\n",
    "                SELECT c.claim_nbr, c.loss_date, l.alt_name, l.contact, b.first, b.last, cc.gap_payable,\n",
    "                    cl.incp_date, v.nada_value, CONCAT(v.year, ' ', v.make, ' ', v.model) AS vehicle,\n",
    "                    cls.description AS loss_type, cc26.description AS primary_carrier, tr.max_benefit,\n",
    "                    cc5.amount AS primary_pymt, cc7.amount AS excess_deductible, tr.term, tr.totalrestart_payable,\n",
    "                    cc8.amount AS scr, cc9.amount AS clr, cc10.amount AS cdr, cc11.amount AS oref,\n",
    "                    cc18.amount AS salvage, cc19.amount AS prior_dmg, \n",
    "                    cc21.amount AS other1_amt, cc21.description AS other1_description,\n",
    "                    cc22.amount AS other2_amt, cc22.description AS other2_description, \n",
    "                    cc35.amount AS other3_amt, cc35.description AS other3_description, ca.description AS carrier,\n",
    "                    (cc5.amount + cc7.amount + cc18.amount + cc19.amount + cc.gap_payable + cc8.amount + cc8.amount + \n",
    "                    cc9.amount + cc10.amount + cc11.amount + cc21.amount + cc22.amount + cc35.amount) as subtotal\n",
    "                FROM claims c\n",
    "                INNER JOIN claim_lender l\n",
    "                    USING (claim_id)\n",
    "                INNER JOIN claim_borrower b\n",
    "                    USING (claim_id)\n",
    "                INNER JOIN claim_loan cl\n",
    "                    USING (claim_id)\n",
    "                INNER JOIN claim_calculations cc\n",
    "                    USING (claim_id)\n",
    "                INNER JOIN claim_vehicle v\n",
    "                    USING (claim_id)\n",
    "                INNER JOIN claims_loss_type cls\n",
    "                    USING (loss_type_id)\n",
    "                INNER JOIN carriers ca\n",
    "                    ON c.carrier_id = ca.carrier_id                        \n",
    "                INNER JOIN claim_checklist AS cc26\n",
    "                    ON c.claim_id = cc26.claim_id\n",
    "                    AND cc26.checklist_item_id = 26                  \n",
    "                INNER JOIN claim_checklist AS cc5\n",
    "                    ON c.claim_id = cc5.claim_id\n",
    "                    AND cc5.checklist_item_id = 5\n",
    "                INNER JOIN claim_checklist AS cc7\n",
    "                    ON c.claim_id = cc7.claim_id\n",
    "                    AND cc7.checklist_item_id = 7\n",
    "                INNER JOIN claim_checklist AS cc8\n",
    "                    ON c.claim_id = cc8.claim_id\n",
    "                    AND cc8.checklist_item_id = 8\n",
    "                INNER JOIN claim_checklist AS cc9\n",
    "                    ON c.claim_id = cc9.claim_id\n",
    "                    AND cc9.checklist_item_id = 9\n",
    "                INNER JOIN claim_checklist AS cc10\n",
    "                    ON c.claim_id = cc10.claim_id\n",
    "                    AND cc10.checklist_item_id = 10\n",
    "                INNER JOIN claim_checklist AS cc11\n",
    "                    ON c.claim_id = cc11.claim_id\n",
    "                    AND cc11.checklist_item_id = 11      \n",
    "                INNER JOIN claim_checklist AS cc18\n",
    "                    ON c.claim_id = cc18.claim_id\n",
    "                    AND cc18.checklist_item_id = 18\n",
    "                INNER JOIN claim_checklist AS cc19\n",
    "                    ON c.claim_id = cc19.claim_id\n",
    "                    AND cc19.checklist_item_id = 19     \n",
    "                INNER JOIN claim_checklist AS cc21\n",
    "                    ON c.claim_id = cc21.claim_id\n",
    "                    AND cc21.checklist_item_id = 21\n",
    "                INNER JOIN claim_checklist AS cc22\n",
    "                    ON c.claim_id = cc22.claim_id\n",
    "                    AND cc22.checklist_item_id = 22\n",
    "                INNER JOIN claim_checklist AS cc35\n",
    "                    ON c.claim_id = cc35.claim_id\n",
    "                    AND cc35.checklist_item_id = 35\n",
    "                INNER JOIN claim_totalrestart AS tr\n",
    "                    ON c.claim_id = tr.claim_id\n",
    "                WHERE c.claim_id = {claimID};'''.format(claimID = row['claim_id'])\n",
    "\n",
    "        # # save query results as DF\n",
    "        c_temp_df = pd.DataFrame(mysql_q(vgc_u, vgc_pw, vgc_host, 'visualgap_claims', c_sql_query, 0, 0))\n",
    "\n",
    "        # append query result to c_template_df\n",
    "        c_consolidate_df = c_consolidate_df.append(c_temp_df)\n",
    "\n",
    "        cal_cols = {0:'claim_nbr', 1:'loss_date', 2:'alt_name', 3:'contact', 4:'first', 5:'last', 6:'gap_payable', 7:'incp_date', 8:'nada_value',\n",
    "            9:'vehicle', 10:'loss_type', 11:'primary_carrier', 12:'max_benefit', 13:'primary_pymt', 14:'excess_deductible', 15:'term', 16:'totalrestart_payable', \n",
    "            17:'scr', 18:'clr', 19:'cdr', 20:'oref', 21:'salvage', 22:'prior_dmg', 23:'other1_amt', 24:'other1_description',\n",
    "            25:'other2_amt', 26:'other2_description', 27:'other3_amt', 28:'other3_description', 29:'carrier', 30:'subtotal'}\n",
    "\n",
    "        c_template_df = c_consolidate_df.rename(columns = cal_cols)\n",
    "        c_template_df = c_template_df.reset_index(drop=True)\n",
    "\n",
    "        # Create Calculations\n",
    "        c_pdf_template = \"letters/pdf_templates/TR_calculation_template.pdf\"\n",
    "        position = 3\n",
    "        tr_calculations(c_template_df, c_pdf_template, position)\n",
    "\n",
    "        # create a list of file to concatenate\n",
    "        file_list = fileList(file_staging_dir, file_ext)\n",
    "\n",
    "        # Concatenated output file\n",
    "        outfn = attachment_dir + row['claim_nbr'] + '-' + row['last']\n",
    "        \n",
    "        ConCat_pdf(file_list, outfn)\n",
    "\n",
    "        # send Email\n",
    "        attach_name = row['claim_nbr'] + '-' + row['last'] + \".pdf\"\n",
    "        missingEmail = 0\n",
    "        toEmail = []\n",
    "\n",
    "        if row['email'] == '' and row['email2'] == '':\n",
    "            toEmail.append('claims@visualgap.com')\n",
    "            missingEmail = 1\n",
    "        elif row['email'] == '':\n",
    "            toEmail.append(row['email2'])\n",
    "        elif row['email2'] == '':\n",
    "            toEmail.append(row['email'])\n",
    "        else:\n",
    "            toEmail.append(row['email'])\n",
    "            toEmail.append(row['email2'])\n",
    "\n",
    "        if missingEmail == 0:\n",
    "            msg_html = \"\"\"\n",
    "            <html>\n",
    "            <body>\n",
    "                <p>Hello {}:<br>\n",
    "                <br>\n",
    "                We completed the {} for {}'s loan.  Attached is claim letter and calculation. <br>\n",
    "                <br>\n",
    "                Thank you, <br>\n",
    "                Claims Department <br>\n",
    "                <br>\n",
    "                <b>Frost Financial Services, Inc. | VisualGAP <br>\n",
    "                Claims Department <br>\n",
    "                Phone: 888-753-7678 Option 3</b>\n",
    "                </p>\n",
    "            </body>\n",
    "            </html>\n",
    "            \"\"\".format(row['contact'], claim_type, (row['first'] + ' ' + row['last']))\n",
    "        else:\n",
    "            msg_html = \"\"\"\n",
    "            <html>\n",
    "            <body>\n",
    "                <p>There is not an email address on claim {}:<br>\n",
    "                <br>\n",
    "                Please fax or mail the attached to {}. <br>\n",
    "                <br>\n",
    "                Thank you, <br>\n",
    "                Claims Department <br>\n",
    "                <br>\n",
    "                <b>Frost Financial Services, Inc. | VisualGAP <br>\n",
    "                Claims Department <br>\n",
    "                Phone: 888-753-7678 Option 3</b>\n",
    "                </p>\n",
    "            </body>\n",
    "            </html>\n",
    "            \"\"\".format(row['claim_nbr'], row['alt_name'])\n",
    "\n",
    "        send_email(toEmail, claim_type, msg_html, attachment_dir, attach_name)\n",
    "\n",
    "        # Remove files from staging directory\n",
    "        clear_dir(attachment_dir, file_ext)\n",
    "        clear_dir(file_staging_dir, file_ext)\n",
    "\n",
    "else: print('No ACH and/or $0 claims.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update toVGC to 1\n",
    "update_tovgc_1(tr_ach_0_df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25bbdcee2c7b2635f2af9203b4c23ac3a2ece7544aac2f4db010efbeb60ffe95"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 32-bit ('pyfrost32-dev': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
